<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Lolipop's Studio | RSS Feed]]></title><description><![CDATA[Homepage and blog of Lolipop, share knowledge about software / frontend development.]]></description><link>https://blog.towind.fun</link><generator>GatsbyJS</generator><lastBuildDate>Thu, 12 Jun 2025 07:16:01 GMT</lastBuildDate><item><title><![CDATA[通过 Grafana 管理日志信息]]></title><description><![CDATA[新买了服务器果然能为自己创造一个又一个接连不断的需求。例如，笔者想到公网服务器上跑的定时脚本会把日志重定向到指定的文件里，但是并不会自动清理，意味着它们会在几十年后占满笔者的硬盘空间，这是不可接受不可妥协的！ 因此在本篇博客里，笔者计划部署统一的日志管理服务，存储来自各个地方的日志信息，既方便管理，也方便快速发现并修复问题。

在 Github 上以  为关键词，按星标数排序…]]></description><link>https://blog.towind.fun/posts/grafana-log-system</link><guid isPermaLink="false">grafana-log-system</guid><category><![CDATA[技术琐事]]></category><pubDate>Tue, 03 Jun 2025 00:00:00 GMT</pubDate><content:original-text>
新买了服务器果然能为自己创造一个又一个接连不断的需求。例如，笔者想到公网服务器上跑的定时脚本会把日志重定向到指定的文件里，但是并不会自动清理，意味着它们会在几十年后占满笔者的硬盘空间，这是不可接受不可妥协的！

因此在本篇博客里，笔者计划部署统一的日志管理服务，存储来自各个地方的日志信息，既方便管理，也方便快速发现并修复问题。

在 Github 上以 `log` 为关键词，按星标数排序，目前排名第一的开源软件是 [`grafana/grafana`](https://github.com/grafana/grafana)。浏览使用文档后觉得它远远超出笔者的需要，但就笔者就爱用大炮打蚊子，遂决定在 Docker 上部署之，作为笔者的日志管理服务。

## 部署 Grafana 和 Loki 容器

[Grafana](https://grafana.org.cn/docs/grafana/latest/) 的定位是整合数据查询和展示的客户端，在它背后实际负责日志收集和存储的是 [Loki](https://grafana.org.cn/docs/loki/latest/)。因此，首先我们需要部署 Grafana 和 Loki 这两个容器。

&gt; 笔者在撰写本文时，使用的 Grafana 的版本为 `12.0`，Loki 的版本为 `3.5.x`。

参考 Loki 官方快速开始文档，精简示例里的 [`docker-compose.yaml` 文件](https://github.com/grafana/loki/blob/main/examples/getting-started/docker-compose.yaml)，只保留用于启动 Grafana 和 Loki 这两个服务和其他一些必要服务的容器的配置。简化后的 `docker-compose.yaml` 配置文件如下：

```yaml
---
networks:
  loki:

services:
  # Loki 日志读服务
  read:
    image: grafana/loki:latest
    command: &quot;-config.file=/etc/loki/config.yaml -target=read&quot;
    ports:
      - 3101:3100
      - 7946
      - 9095
    volumes:
      - ./loki-config.yaml:/etc/loki/config.yaml
    depends_on:
      - minio
    healthcheck:
      test:
        [
          &quot;CMD-SHELL&quot;,
          &quot;wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1&quot;,
        ]
      interval: 10s
      timeout: 5s
      retries: 5
    networks: &amp;loki-dns
      loki:
        aliases:
          - loki

  # Loki 日志写服务
  write:
    image: grafana/loki:latest
    command: &quot;-config.file=/etc/loki/config.yaml -target=write&quot;
    ports:
      - 3102:3100
      - 7946
      - 9095
    volumes:
      - ./loki-config.yaml:/etc/loki/config.yaml
    healthcheck:
      test:
        [
          &quot;CMD-SHELL&quot;,
          &quot;wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1&quot;,
        ]
      interval: 10s
      timeout: 5s
      retries: 5
    depends_on:
      - minio
    networks:
      &lt;&lt;: *loki-dns

  # MinIO 对象存储服务；负责持久化存储 Loki 的日志数据
  minio:
    image: minio/minio
    entrypoint:
      - sh
      - -euc
      - |
        mkdir -p /data/loki-data &amp;&amp; \
        mkdir -p /data/loki-ruler &amp;&amp; \
        minio server /data --console-address &quot;:9001&quot;
    environment:
      - MINIO_ROOT_USER=loki # 数据库管理员用户名
      - MINIO_ROOT_PASSWORD=supersecret # 数据库管理员用户登录密码
      - MINIO_PROMETHEUS_AUTH_TYPE=public
      - MINIO_UPDATE=off
    ports:
      - 9000
      - 9001 # 监听 Web UI 端口
    volumes:
      - ./.data/minio:/data
    healthcheck:
      test: [&quot;CMD&quot;, &quot;curl&quot;, &quot;-f&quot;, &quot;http://localhost:9000/minio/health/live&quot;]
      interval: 15s
      timeout: 20s
      retries: 5
    networks:
      - loki

  # Grafana 日志可视化服务
  grafana:
    image: grafana/grafana:latest
    environment:
      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning
      - GF_AUTH_ANONYMOUS_ENABLED=true # 允许匿名访问
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin # 匿名访问用户角色为 Admin，可以使用所有功能；如果控制台可以通过公网访问，应当对权限进行相应控制
    depends_on:
      - gateway
    entrypoint:
      - sh
      - -euc
      - |
        mkdir -p /etc/grafana/provisioning/datasources
        cat &lt;&lt;EOF &gt; /etc/grafana/provisioning/datasources/ds.yaml
        apiVersion: 1
        datasources:
          - name: Loki
            type: loki
            access: proxy
            url: http://gateway:3100
            jsonData:
              httpHeaderName1: &quot;X-Scope-OrgID&quot;
            secureJsonData:
              httpHeaderValue1: &quot;YOUR_TENANT_ID&quot;
        EOF
        /run.sh
    ports:
      - &quot;3000:3000&quot;
    healthcheck:
      test:
        [
          &quot;CMD-SHELL&quot;,
          &quot;wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1&quot;,
        ]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - loki

  # Loki 其它后端服务
  backend:
    image: grafana/loki:latest
    volumes:
      - ./loki-config.yaml:/etc/loki/config.yaml
    ports:
      - &quot;3100&quot;
      - &quot;7946&quot;
    command: &quot;-config.file=/etc/loki/config.yaml -target=backend -legacy-read-mode=false&quot;
    depends_on:
      - gateway
    networks:
      - loki

  # Nginx 网关；负责转发请求到对应的 Loki 服务
  gateway:
    image: nginx:latest
    depends_on:
      - read
      - write
    entrypoint:
      - sh
      - -euc
      - |
        cat &lt;&lt;EOF &gt; /etc/nginx/nginx.conf
        user  nginx;
        worker_processes  5;  ## Default: 1

        events {
          worker_connections   1000;
        }

        http {
          resolver 127.0.0.11;

          server {
            listen             3100;

            location = / {
              return 200 &apos;OK&apos;;
              auth_basic off;
            }

            location = /api/prom/push {
              proxy_pass       http://write:3100\$$request_uri;
            }

            location = /api/prom/tail {
              proxy_pass       http://read:3100\$$request_uri;
              proxy_set_header Upgrade \$$http_upgrade;
              proxy_set_header Connection &quot;upgrade&quot;;
            }

            location ~ /api/prom/.* {
              proxy_pass       http://read:3100\$$request_uri;
            }

            location = /loki/api/v1/push {
              proxy_pass       http://write:3100\$$request_uri;
            }

            location = /loki/api/v1/tail {
              proxy_pass       http://read:3100\$$request_uri;
              proxy_set_header Upgrade \$$http_upgrade;
              proxy_set_header Connection &quot;upgrade&quot;;
            }

            location ~ /loki/api/.* {
              proxy_pass       http://read:3100\$$request_uri;
            }
          }
        }
        EOF
        /docker-entrypoint.sh nginx -g &quot;daemon off;&quot;
    ports:
      - &quot;3100:3100&quot;
    healthcheck:
      test: [&quot;CMD&quot;, &quot;service&quot;, &quot;nginx&quot;, &quot;status&quot;]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - loki
```

在上面的配置里，Grafana 采用了租户控制，您需要手动替换 `YOUR_TENANT_ID` 为您想要的租户名。如果想要添加其他租户，请按照格式添加新的 `datasources` 数组项。如果你打算将 Grafana 暴露到公网访问，请务必修改权限控制的相关配置，阻止匿名用户访问或避免匿名用户权限过高。

启动 Loki 容器时还用到了对应的配置文件，直接使用示例里给出的 [`loki-config.yaml` 文件](https://github.com/grafana/loki/blob/main/examples/getting-started/loki-config.yaml)即可：

```yaml
---
server:
  http_listen_address: 0.0.0.0
  http_listen_port: 3100

memberlist:
  join_members: [&quot;read&quot;, &quot;write&quot;, &quot;backend&quot;]
  dead_node_reclaim_time: 30s
  gossip_to_dead_nodes_time: 15s
  left_ingesters_timeout: 30s
  bind_addr: [&quot;0.0.0.0&quot;]
  bind_port: 7946
  gossip_interval: 2s

schema_config:
  configs:
    - from: 2025-06-03
      store: tsdb
      object_store: s3
      schema: v13
      index:
        prefix: index_
        period: 24h

common:
  path_prefix: /loki
  replication_factor: 1
  compactor_address: http://backend:3100
  storage:
    s3:
      endpoint: minio:9000
      insecure: true
      bucketnames: loki-data
      access_key_id: loki # MinIO 数据库管理员用户名，需要与 MinIO 容器环境变量里的配置保持一致
      secret_access_key: supersecret # MinIO 数据库管理员用户登录密码，需要与 MinIO 容器环境变量里的配置保持一致
      s3forcepathstyle: true
  ring:
    kvstore:
      store: memberlist

ruler:
  storage:
    s3:
      bucketnames: loki-ruler

compactor:
  working_directory: /tmp/compactor
```

对于笔者的实际使用需求来说，上面每个配置的具体含义并不需要清楚了解，让我们直接启动 Grafana 和 Loki 容器吧！

```bash
% docker-compose up -d
[+] Running 18/18
 ✔ gateway Pulled                                                                                                                                         15.0s
 ✔ minio Pulled                                                                                                                                            9.9s
[+] Running 7/7
 ✔ Network grafana_loki         Created                                                                                                                    0.0s
 ✔ Container grafana-minio-1    Started                                                                                                                    0.5s
 ✔ Container grafana-write-1    Started                                                                                                                    0.6s
 ✔ Container grafana-read-1     Started                                                                                                                    0.6s
 ✔ Container grafana-gateway-1  Started                                                                                                                    0.7s
 ✔ Container grafana-backend-1  Started                                                                                                                    0.9s
 ✔ Container grafana-grafana-1  Started                                                                                                                    0.9s
```

验证 Grafana 容器是否启动成功，可以访问 Grafana 容器映射在机器上的端口（配置文件里默认为 `3000`）。如果成功的话，将正常显示控制台页面如下：

![grafana-panel](./grafana-log-system/grafana-panel.png)

另外，可以分别访问 `http://localhost:3101/ready` 和 `http://localhost:3102/ready`，来验证 Loki 的读写服务是否启动成功。如果成功的话，将返回 `ready` 字符串。

至此，前置工作就完成了。

## 重定向日志信息到 Loki

### 使用 Alloy 重定向常规日志信息

现在，我们拥有了 Loki 来收集日志，那么该拿什么负责在客户端转发日志呢？Loki 的官方文档里推荐使用 [Alloy](https://grafana.org.cn/docs/alloy/latest/)，这里笔者也就沿用官方文档提供的方案了。

&gt; 笔者在撰写本文时，使用的 Alloy 的版本为 `1.8`。

对于笔者的 CentOS 服务器，依次执行下面的命令来安装 Alloy：

```bash
wget -q -O gpg.key https://rpm.grafana.com/gpg.key
sudo rpm --import gpg.key
echo -e &apos;[grafana]\nname=grafana\nbaseurl=https://rpm.grafana.com\nrepo_gpgcheck=1\nenabled=1\ngpgcheck=1\ngpgkey=https://rpm.grafana.com/gpg.key\nsslverify=1\nsslcacert=/etc/pki/tls/certs/ca-bundle.crt&apos; | sudo tee /etc/yum.repos.d/grafana.repo

yum update

sudo yum install alloy
```

配置 Alloy 服务开机自启：

```bash
sudo systemctl enable alloy.service
```

配置 Alloy 服务采集日志，编辑 `/etc/alloy/config.alloy` 文件如下：

```alloy
// 添加本地的日志文件源
local.file_match &quot;local_files&quot; {
  path_targets = [{&quot;__path__&quot; = &quot;/path/to/*.log&quot;}]
  sync_period = &quot;5s&quot;
}

// 读取日志文件源并转发到处理器
loki.source.file &quot;log_scrape&quot; {
  targets = local.file_match.local_files.targets
  forward_to = [loki.process.filter_logs.receiver]
  tail_from_end = true
}

// 过滤日志数据并转发到接收器
loki.process &quot;filter_logs&quot; {
  // ...其它的过滤日志数据的配置
  forward_to = [loki.write.grafana_loki.receiver]
}

// 将日志信息发送到 Loki 服务
loki.write &quot;grafana_loki&quot; {
  // 添加日志标签，方便在 Grafana 上过滤查看
  external_labels = {
    service_name = &quot;YOUR_SERVICE_NAME&quot;,
  }
  endpoint {
    url = &quot;http://localhost:3100/loki/api/v1/push&quot;
    // 必须配置租户 tenant_id 为 Grafana 容器配置中包含的租户 ID
    tenant_id = &quot;YOUR_TENANT_ID&quot;
  }
}
```

启动 Alloy 服务：

```bash
sudo systemctl start alloy
```

验证 Alloy 服务的运行状态：

```bash
$ sudo systemctl status alloy
● alloy.service - Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines
   Loaded: loaded (/usr/lib/systemd/system/alloy.service; enabled; vendor preset: disabled)
   Active: active (running) since Tue 2025-06-03 21:43:30 CST; 21s ago
     Docs: https://grafana.com/docs/alloy
 Main PID: 25555 (alloy)
   CGroup: /system.slice/alloy.service
           └─25555 /usr/bin/alloy run --storage.path=/var/lib/alloy/data /etc/alloy/config.alloy

Jun 03 21:43:30 iZ2vc17uca9zl48iicx7ojZ alloy[25555]: ts=2025-06-03T13:43:30.861446161Z level=info msg=&quot;scheduling loaded com...ices&quot;
Jun 03 21:43:30 iZ2vc17uca9zl48iicx7ojZ alloy[25555]: ts=2025-06-03T13:43:30.861894177Z level=info msg=&quot;starting cluster node...ut=0s
Jun 03 21:43:30 iZ2vc17uca9zl48iicx7ojZ alloy[25555]: ts=2025-06-03T13:43:30.862038357Z level=info msg=&quot;peers changed&quot; servic...x7ojZ
Jun 03 21:43:30 iZ2vc17uca9zl48iicx7ojZ alloy[25555]: ts=2025-06-03T13:43:30.862410662Z level=info msg=&quot;tail routine: started...e.log
Jun 03 21:43:30 iZ2vc17uca9zl48iicx7ojZ alloy[25555]: ts=2025-06-03T13:43:30.862469331Z level=info msg=&quot;tail routine: started...g.log
Jun 03 21:43:30 iZ2vc17uca9zl48iicx7ojZ alloy[25555]: ts=2025-06-03T13:43:30.862606748Z level=info msg=&quot;Seeked /var/www/logs/...ailer
Jun 03 21:43:30 iZ2vc17uca9zl48iicx7ojZ alloy[25555]: ts=2025-06-03T13:43:30.862646605Z level=info msg=&quot;Seeked /var/www/logs/...ailer
Jun 03 21:43:30 iZ2vc17uca9zl48iicx7ojZ alloy[25555]: ts=2025-06-03T13:43:30.862631467Z level=info msg=&quot;now listening for htt...12345
Jun 03 21:43:30 iZ2vc17uca9zl48iicx7ojZ alloy[25555]: ts=2025-06-03T13:43:30.862724788Z level=info msg=&quot;tail routine: started...e.log
Jun 03 21:43:30 iZ2vc17uca9zl48iicx7ojZ alloy[25555]: ts=2025-06-03T13:43:30.862774924Z level=info msg=&quot;Seeked /var/www/logs/...aile
```

在 Grafana 查看收集到的日志信息：

![grafana-query-logs](./grafana-log-system/grafana-query-logs.png)

这样，远程服务器上的日志信息就顺利上传到 Loki 里，并可以通过 Grafana 查看了！之后，只需要定时清除本地的日志文件就可以了。

### 使用 Docker 插件重定向容器日志信息

为了重定向 Docker 容器里的日志到 Loki 上，只需要安装 Loki 的 Docker 插件并为容器添加相应的 `logging` 配置。

笔者的 Docker 服务部署在使用 ARM64 架构的机器上，因此执行如下命令安装 [Loki 的 Docker 插件](https://hub.docker.com/r/grafana/loki-docker-driver/tags)：

```bash
docker plugin install grafana/loki-docker-driver:3.5.1-arm64 --alias loki --grant-all-permissions
```

在配置容器前，假如您忘记了最初用 `docker run` 命令启动容器时后面的一串参数怎么办？用 `docker inspect` 去找灵感也太麻烦了！这个问题也困扰了笔者一阵，幸运的是以前有人遇到这样的烦恼后，开发了 [`runlike`](https://github.com/lavie/runlike) 工具，它可以帮助打印出运行指定容器时可能使用到的参数。

拉取 `assaflavie/runlike` 镜像：

```bash
docker pull assaflavie/runlike
```

编辑 `~/.bashrc` 或 `~/.zshrc`，添加命令别名 `runlike`：

```bash
alias runlike=&quot;docker run --rm -v /var/run/docker.sock:/var/run/docker.sock assaflavie/runlike&quot;
```

重启终端，现在，执行 `runlike &lt;container-name&gt;` 就可以查看启动容器 `&lt;container-name&gt;` 时可能使用到的命令了。

以笔者部署的 AList 容器为例，查看它的启动命令：

```bash
# 添加 -p 以多行显示
% runlike -p alist
docker run --name=alist \
        --hostname=xxxxxx \
        --mac-address=xx:xx:xx:xx:xx:xx \
        --volume /path/to/data:/opt/alist/data \
        --network=bridge \
        --workdir=/opt/alist/ \
        -p 5244:5244 \
        --expose=5245 \
        --restart=unless-stopped \
        --runtime=runc \
        --detach=true \
        xhofe/alist \
        /entrypoint.sh
```

摘取其中笔者确实使用到的参数，简洁的启动命令就是：`docker run --name=alist --volume /path/to/data:/opt/alist/data -p 5244:5244 --restart=unless-stopped xhofe/alist`。

随着容器化和自动化技术的发展，`docker run` 也该被扫进历史的垃圾堆，用 `docker-compose` 来替代了。趁着这次机会，笔者也将服务器上所有容器的启动方式替换为了 `docker-compose`，以后再遇到需要修改参数或更新镜像等的情况，就可以省时省力地一键启动新容器了。

对于笔者的 AList 容器，可以编写 `docker-compose.yaml` 文件如下：

```yml
services:
  alist:
    image: xhofe/alist:v3.40.0 # 由于 AList 已被出售给商业公司，这里笔者使用了普遍认为较安全的版本；如果出现云盘兼容性问题，请考虑使用最新版本或切换其他云盘管理工具
    container_name: alist-server
    volumes:
      - /path/to/data:/opt/alist/data
    ports:
      - 5244:5244
    restart: unless-stopped
```

&gt; 更进一步，您还可以选用 [Dockge](https://github.com/louislam/dockge) 等服务来统一管理系统上的所有 `docker-compose.yaml` 文件，不再在此博客里赘述。

重定向 AList 容器的日志信息到 Loki，完整的 `docker-compose.yaml` 配置文件如下：

```yml
services:
  alist:
    image: xhofe/alist:v3.40.0
    container_name: alist-server
    volumes:
      - /path/to/data:/opt/alist/data
    ports:
      - 5244:5244
    restart: unless-stopped
    logging:
      driver: loki
      options:
        loki-url: http://localhost:3100/loki/api/v1/push
        loki-tenant-id: YOUR_TENANT_ID
```

同样，您需要替换配置里的 `YOUR_TENANT_ID` 为您指定的租户 ID。

最后，删除旧的使用 `docker run` 命令启动的容器，执行命令启动新容器：`docker-compose up -d`。

在 Grafana 查看收集到的日志信息：

![grafana-query-docker-logs](./grafana-log-system/grafana-query-docker-logs.png)

这样，Docker 容器的日志信息也顺利上传到了 Loki 里，并可以通过 Grafana 查看了！

## ？

还有高手……！请看 VCR：[《2999,入手 16+256G 内存的 macmini4，如何通过部署 50 个服务实现最大价值化》](https://www.v2ex.com/t/1123367)。
</content:original-text><content:updated-at>2025-06-12T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[部署自己的 WebDAV 服务，同步应用数据与软件配置]]></title><description><![CDATA[今天在使用 Next Chat 时，突然想到聊天数据应该是可以备份的，在设置里面发现它支持 WebDAV 和没听说过的 UpStash 两种文件协议，而笔者使用的其它软件又广泛支持前者进行备份同步。于是今天便来捣腾 WebDAV 并实现数据备份。 Docker 部署 WebDAV 服务

此章节里笔者选用了纯净的 WebDAV 服务镜像。如果想要更友好的交互体验，可以选用支持 WebDAV…]]></description><link>https://blog.towind.fun/posts/private-webdav</link><guid isPermaLink="false">private-webdav</guid><category><![CDATA[技术琐事]]></category><pubDate>Thu, 03 Apr 2025 00:00:00 GMT</pubDate><content:original-text>
今天在使用 Next Chat 时，突然想到聊天数据应该是可以备份的，在设置里面发现它支持 WebDAV 和没听说过的 UpStash 两种文件协议，而笔者使用的其它软件又广泛支持前者进行备份同步。于是今天便来捣腾 WebDAV 并实现数据备份。

## Docker 部署 WebDAV 服务

&gt; 此章节里笔者选用了纯净的 WebDAV 服务镜像。如果想要更友好的交互体验，可以选用支持 WebDAV 协议的文件服务器镜像，例如 [`drakkan/sftpgo`](https://github.com/drakkan/sftpgo) 以及 [`sigoden/dufs`](https://github.com/sigoden/dufs) 等等。

一旦使用上了 Docker，就再也不想回去那种将服务运行在机器本体上的时代了。笔者首选使用 Docker 部署 WebDAV 服务，在 Docker Hub 以 `webdav` 为关键词搜索，有如下结果：

![WebDAV 镜像列表](./private-webdav/docker-hub-webdav.png)

笔者尝试了下载量和收藏数最高的 `bytemark/webdav`，但发现设置了挂载目录后无法正常上传文件，仔细阅读说明文档才看到，必须要将宿主机的挂载目录设置为写死的 `/srv/dav`，意味着如果想要用其它目录来存放 WebDAV 里的文件的话，还需要额外添加一个软链接。这样的限制感觉笨笨的，很不优雅，于是找了找它的 [Github 仓库](https://github.com/BytemarkHosting/docker-webdav)，发现作者在写完这个镜像，发了一篇[部署文档](https://docs.bytemark.co.uk/article/run-your-own-webdav-server-with-docker/)后就再没有更新了，代码也就停留在了 2018 年。也许 WebDAV 协议已经相当稳定了，但是笔者还是想要找一个不这么别扭的 WebDAV 镜像。

接着，笔者尝试在 Github 上以 `webdav` 为关键词搜索，发现了关联度和星标均为最高，并且活跃更新的 [`hacdias/webdav`](https://github.com/hacdias/webdav)。它针对不同用户组的权限管理能力非常强，笔者很喜欢。但笔者遇到的一个问题是：将本地的 `local/path/to/data` 目录关联给容器的 `/data` 目录后，同步的数据仍然被放置在了容器的根目录里，导致无法持久化备份文件。这也许是当前版本的 Bug，也有可能是没配置对的原因，总之笔者又着眼于找下一个 WebDAV 镜像去了。

最后，笔者尝试了 Docker Hub 上目前下载量和收藏数均为第二的 [`ugeek/webdav`](https://github.com/uGeek/docker-webdav)，没有上述的问题，满足了笔者的需要。笔者在 MacOS 系统上部署 WebDAV 服务，因此要拉取它的 `arm64` 版本：

```bash
docker pull ugeek/webdav:arm64
```

运行 WebDAV 容器：

```bash
docker run --name webdav \
  --restart=unless-stopped \
  -p 3180:80 \
  -v $HOME/WebDAV/data:/media \
  -e USERNAME=YOUR_USERNAME \
  -e PASSWORD=YOUR_PASSWORD \
  -e TZ=Asia/Shanghai  \
  -e UDI=1000 \
  -e GID=1000 \
  -d  ugeek/webdav:arm64
```

上面的配置表明，WebDAV 服务暴露在宿主机的 3180 端口，数据持久化存储在 `$HOME/WebDAV/data` 目录下。访问 `127.0.0.1:3180`，输入用户名和密码，即可看见当前 WebDAV 中包含的文件列表（当然此时为空）。

通过 frp 将内网的服务暴露给公网服务器上，再通过配置公网服务器的 Nginx 配置，现在 WebDAV 就可以通过域名 `https://webdav.towind.fun` 访问了！

## 图形界面管理 WebDAV

用 Postman 来对 WebDAV 里的文件做 CRUD 显然不符合实际需要。在这一章节，笔者将分别通过 AList 和 Windows 来连接 WebDAV，方便以后实际的管理与使用。

### AList 挂载 WebDAV

笔者之前在 Docker 上部署了 AList 服务，它也支持连接并管理 WebDAV。这一切尤为简单，只需要在“管理 - 存储”页面添加 WebDAV 驱动，输入账号和密码即可连接：

![AList 连接到 WebDAV](./private-webdav/alist-webdav-connect.png)

回到主页，WebDAV 就顺利地挂载到了指定的路由上。这样，就可以随时随地通过浏览器访问 AList，管理 WebDAV 里的文件了！

### Windows 挂载 WebDAV

&gt; 如果遇到报错：`The network path was not found`，可以尝试多连接几次，直到顺利访问。笔者暂时没有找到能够稳定连接的方案，Drive me crazy，更推荐使用其它方案访问 WebDAV。

笔者的 WebDAV 由 SSL 加密，因此需要首先配置 Windows Web Client 的身份验证级别，即在“运行”里输入 `regedit` 打开注册表编辑器，找到 `Computer\HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\WebClient\Parameters`，修改其中键 `BasicAuthLevel` 的值为 `2`。

![修改 Windows Web Client 身份验证级别](./private-webdav/windows-webdav-auth-level.png)

笔者翻阅有关 Windows 系统连接 WebDAV 的[官方文档](https://learn.microsoft.com/zh-cn/iis/publish/using-webdav/using-the-webdav-redirector#webdav-redirector-registry-settings)发现，`BasicAuthLevel` 默认值为 `1` 的时候就已经对 SSL 加密的连接启用身份验证了，但笔者仅在其值为 `2` 的时候能顺利连接上。暂且蒙在鼓里。

右键“我的电脑”，选择“选择映射网络驱动器”，输入 WebDAV 的地址，再输入用户名和密码即可连接到服务：

![Windows 连接到 WebDAV](./private-webdav/windows-webdav-connect.png)

现在就可以在 Windows 系统上，像管理其它本地文件一样管理 WebDAV 里的文件了！

## 备份应用数据到 WebDAV

### 备份 Obsidian 笔记到 WebDAV

笔者使用 Obsidian 同步多端的笔记，过去使用 OneDrive 来同步笔记的数据，现在就可以迁移到 WebDAV 上了。只需要安装插件市场里的三方插件 Remotely Save，配置 WebDAV 即可：

![配置 Obsidian 插件 Remotely Save](./private-webdav/webdav-sync-obsidian.png)

点击同步按钮，笔记数据就光速保存到 WebDAV 上啦！相比 OneDrive，使用 WebDAV 同步的速度要显著更快，但是也失去了 OneDrive 提供的历史版本管理能力，得失兼而有之。

尽管 OneDrive 的能力更加强大，但笔者还是倾心于玩弄自己的小玩具，暂且不与 OneDrive 同步了 XD。

### 备份 Next Chat 数据到 WebDAV

笔者撰写此文时使用的 Next Chat 版本为 `v2.15.8`。

尽管在设置中它提供了同步配置和聊天记录到 WebDAV 的能力，但无论怎么配置也无法成功同步数据，翻看好多 Issues 和相关的帖子才终于理出来要如何使用。

Next Chat 设计为 Web 端使用，部署时需要配置环境变量 `WHITE_WEBDAV_ENDPOINTS` 以添加 WebDAV 终端白名单。官方构建版本仅对一些常见 WebDAV 服务域名开了白名单，但像笔者这样的自建 WebDAV 服务自然是不可能在名单内。因此需要手动部署一个属于自己的 Next Chat 的 Web 版本，在环境变量中为 WebDAV 开放白名单。这样，在同步数据时，使用自建站点的域名作为代理服务器就可以了。

参考部署 Next Chat 到 Vercel 的[官方文档](https://github.com/ChatGPTNextWeb/NextChat/blob/48469bd8ca4b29d40db0ade61b57f9be6f601e01/docs/vercel-cn.md)，部署自己的 Next Chat 站点，将 `WHITE_WEBDAV_ENDPOINTS=https://webdav.towind.fun` 作为构建环境变量：

![构建 Next Chat 环境变量](./private-webdav/next-chat-vercel-env.png)

现在，笔者自建的 Next Chat 站点便允许被作为请求 `https://webdav.towind.fun` 的代理服务器了。

配置云同步，检查可用性，成功在 WebDAV 的根目录下创建了文件夹 `chatgpt-next-chat`：

![Next Chat 云同步检查 WebDAV 可用性](./private-webdav/next-chat-check-connection.png)

尝试初次同步 Next Chat 的数据：首先发出请求检查 WebDAV 里是否包含云端数据，响应值为 404 Not Found，符合预期；接着将备份的数据上传到 WebDAV，服务端响应 502 Bad Gateway：

![Next Chat 同步失败 502 Error](./private-webdav/webdav-sync-next-chat-failed.png)

查看 Nginx 错误日志：

```bash
tail -n 10 /var/log/nginx/error.log
...
[error] 2107#2107: *36701359 client intended to send too large body: 1663429 bytes, client: xxx.xx.xxx.xxx, server: webdav.towind.fun, request: &quot;PUT /chatgpt-next-web/backup.json HTTP/2.0&quot;, host: &quot;webdav.towind.fun&quot;
```

可知 Nginx 拒绝了包含超过设定大小数据的请求，那么只需要将设定的大小放开一点就好。编辑对应的 Nginx 配置，例如设置允许的最大请求体为 `50M`：

```nginx
server {
  server_name webdav.towind.fun;
  client_max_body_size 50M;
}
```

再次尝试同步 Next Chat 的数据，成功上传到 WebDAV：

![Next Chat 同步成功](./private-webdav/webdav-backup-next-chat.png)

未来无论在什么客户端使用 Next Chat 时，都可以同步配置数据和聊天记录了！

## 参考文章

在“Windows 挂载 WebDAV”章节里，主要参考了[《Windows 挂载 WebDAV》](https://www.expoli.tech/articles/2020/12/30/1609327097930#00ecc29c1a7b4cb783cfa61ba925073e)这篇博客，解决了挂载异常的问题。

在“备份应用数据到 WebDAV”章节里，主要参考了[《终于拿下了NextChat的WebDAV云同步，我感觉我又行了》](https://linux.do/t/topic/373723)和[《【拉跨，但能用了】NextChat 的 WebDAV 云同步。。》](https://linux.do/t/topic/328163)这两篇帖子。
</content:original-text><content:updated-at>2025-04-05T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[拿刚买的 Mac Mini 做我的应用服务器]]></title><description><![CDATA[去年笔者拿 N100 小型主机安装 OpenWRT 系统作主路由服务器，又塞进去一些笔者自己部署的应用服务。后来 Mac Mini 4 发售了，评测得到的功耗与性能令人惊叹，最重要的是定价很不苹果，让我这个苹果小黑子也想要入手一台玩玩。 当然，笔者在逛 V2EX 的时候也看了大量劝退购入 Mac Mini 4 的帖子，尤其是像笔者这样只是拿来做轻量级应用服务器的用途 —— 大炮打蚊子。但是你知道的…]]></description><link>https://blog.towind.fun/posts/mac-mini-server</link><guid isPermaLink="false">mac-mini-server</guid><category><![CDATA[技术琐事]]></category><pubDate>Sun, 30 Mar 2025 00:00:00 GMT</pubDate><content:original-text>
去年笔者拿 N100 小型主机安装 OpenWRT 系统作主路由服务器，又塞进去一些笔者自己部署的应用服务。后来 Mac Mini 4 发售了，评测得到的功耗与性能令人惊叹，最重要的是定价很不苹果，让我这个苹果小黑子也想要入手一台玩玩。

当然，笔者在逛 V2EX 的时候也看了大量劝退购入 Mac Mini 4 的帖子，尤其是像笔者这样只是拿来做轻量级应用服务器的用途 —— 大炮打蚊子。但是你知道的，即使并不需要，但是笔者可以拿它为自己的灵魂制造全新的刺激来充实生活。

时间来到 2025 年 03 月，Mac Mini 4 已经有大量现货而无需蹲点抢购了，国家补贴叠加学生补贴，在京东上花 2999 元下单入手了最低配置（但最高性价比）的版本。

这篇博客将记录笔者从零开始折腾 Mac Mini 4 做轻量级应用服务器的全过程，包括从原来的服务器上迁移已有 Docker 容器和应用服务的操作步骤。

## 首次启动 Mac Mini

这是笔者第一次购买 Mac Mini，在首次启动主机的时候，会需要连接鼠标与键盘。由于 Mac Mini 4 上只有 Type-C 接口，所以可以提前准备拓展坞或者 Type-C 连接线。

如果有蓝牙鼠标和蓝牙键盘，就不需要用到这些硬件了：只需要把蓝牙鼠标和蓝牙键盘置于配对模式下，Mac Mini 就会自动配对并连接。

## 面向服务器的 Mac Mini 配置

参考网上其它教程指出的细节，在“设置”里对 Mac Mini 进行配置：

1. **保持服务器运行状态**：在“能源”里设置“显示器关闭时，防止自动进入睡眠”和“断电后自动启动”；在“锁定屏幕”里禁用屏幕保护程序。
2. **保持用户的登录状态**：在“用户与群组”里设置“自动以此身份登录”，保障各种软件在主机启动后正常运行。
3. **避免系统更新自动重启**：在“通用 - 软件更新”里关闭自动检查、下载和安装更新的功能。

这样，Mac Mini 就满足作为常时运转的服务器的基本配置需要了。

## 基于 Rust Desk 的远程桌面连接

笔者目前只有局域网访问主机的需求，而且不想用包含中继服务器等增值服务在内的各类远程桌面软件，因此选用了 Github 上星标最多的开源远程桌面客户端 [Rust Desk](https://github.com/rustdesk/rustdesk)。

不想付一分钱就想体验完整的 Rust Desk 能力的话，可以选择在本地部署 Rust Desk 的中继服务器。

Docker 是管理本地服务的无二之选，首先在 Mac Mini 上下载并安装 Docker Desktop。设置国内镜像仓库，在 “Settings - Docker Engine” 里添加：

```json
{
  &quot;registry-mirrors&quot;: [
    &quot;https://docker-0.unsee.tech&quot;,
    &quot;https://hub.fast360.xyz&quot;,
    &quot;https://dockerpull.cn&quot;
  ]
}
```

不保证上述镜像仓库始终可用，如失效，可以在网上找到最新有效的地址。

拉取 Rust Desk 的服务端镜像 `rustdesk/rustdesk-server`，运行 hbbs 和 hbbr 容器服务：

&gt; 官方文档使用“Host”网络模式来免除一个一个手动设置端口号的麻烦，但是 MacOS 系统并不支持这种网络模式。

```bash
# RustDesk 的中介服务器，用于管理和协调客户端连接
docker run --name hbbs -p 21115:21115 -p 21116:21116 -p 21116:21116/udp -p 21118:21118 -v path/to/rustdesk/data:/root -td rustdesk/rustdesk-server hbbs
# RustDesk 的中继服务器，用于在两台客户端之间进行连接中继
docker run --name hbbr -p 21117:21117 -p 21119:21119 -v path/to/rustdesk/data:/root -td rustdesk/rustdesk-server hbbr
```

获取 Rust Desk 中介服务器公钥，打印 `path/to/rustdesk/data` 目录下自动生成的 `id_ed25519.pub` 文件内容：

```bash
cat path/to/rustdesk/data/id_ed25519.pub
```

分别在主控端（笔者为 Windows）和被控端（即 Mac Mini）上安装 Rust Desk 客户端，在客户端的“设置 - 网络 - ID/中继服务器”处配置中继服务器相关内容。这里填写主路由自动为 Mac Mini 的分配的 IP 地址 `192.168.100.239`，如果在启动中继服务器容器时没有自行修改暴露的端口，那么 Rust Desk 会自动访问到对应的端口，无需单独指定端口号，同时将上一步得到公钥填写到 “Key” 里：

![配置 Rust Desk 客户端的中继服务器](./mac-mini-server/rustdesk-public-key.png)

在主控端输入被控端的 ID 与连接密码，即可连接到远程桌面：

![连接到 Mac Mini 远程桌面](./mac-mini-server/connect-to-mac-mini.png)

在默认情况下，远程桌面将使用不可变更的 1920 \* 1080 分辨率，可以使用 Better Dummy 软件为 Mac Mini 虚拟一个显示屏，就可以自定义分辨率与启用 HiDPI 功能了。

![使用 Better Dummy 虚拟显示屏](./mac-mini-server/with-better-dummy.png)

最后，在 OpenWRT 的控制面板里将 `192.168.100.239` 绑定为分配给 Mac Mini 的固定 IPv4 地址，以后就无需在主控端重新配置中继服务器的相关内容啦。

![为 Mac Mini 绑定静态 IPv4 地址](./mac-mini-server/dhcp-static-ipv4.png)

如上图所示，笔者同时将 Mac Mini 无线网卡的 MAC 地址和以太网卡的 MAC 地址绑定给 `192.168.100.239`，为之后断开 WiFi 使用网线做好了铺垫。

## 迁移 Docker 里的容器

### 迁移数据库

数据库是笔者其它应用服务的基石，首先应当迁移这些容器服务。

这里以 PostgreSQL 数据库为例，首先应当调用 PostgreSQL 官方提供的备份工具，备份数据库里的所有数据表：

```bash
# 进入名为 postgres 的 PostgreSQL 数据库容器
docker exec -it postgres bash
# 以 postgres 的身份备份数据库里的所有数据表
pg_dumpall -U postgres &gt; /path/to/all_databases.sql
# 退出容器
exit
```

接着，将数据表备份文件 `all_databases.sql` 复制到主机上：

```bash
docker cp postgres:/path/to/all_databases.sql /local/path/to/all_databases.sql
```

然后，将主机上的数据表备份文件传输到 Mac Mini 上：

```bash
scp /local/path/to/all_databases.sql username@192.168.100.239:/Users/username/PostgreSQL/all_databases.sql
```

在 Mac Mini 上拉取最新版本的 `postgres` 镜像，启动新的 PostgreSQL 容器：

```bash
docker run --name postgres --restart unless-stopped -e POSTGRES_PASSWORD=SECRET_PASSWORD -v /Users/username/PostgreSQL/data:/var/lib/postgresql/data -d -p 5432:5432 postgres:latest
```

将数据表备份文件复制到 PostgreSQL 容器里：

```bash
docker cp /Users/username/PostgreSQL/all_databases.sql postgres:/path/to/all_databases.sql
```

最后，进入到新的 PostgreSQL 容器里，还原数据表：

```bash
docker exec -it postgres bash
psql -U postgres -f /path/to/all_databases.sql
```

这样，对 PostgreSQL 数据库容器的迁移就完成了！

笔者基于 frpc 实现内网穿透，因此还需要在 OpenWRT 控制面板里配置 frpc 客户端，修改 PostgreSQL 数据库的内网地址，从原来主路由服务器所在的 `192.168.100.1`，改到 Mac Mini 所在的 `192.168.100.239`。这样，外网也能正常访问新的 PostgreSQL 数据库了。

### 迁移 Minecraft 游戏服务器到 Docker 容器

去年笔者在主路由上部署了 Minecraft 服务器，这次决定一并迁移到 Mac Mini 里来，作为 Docker 容器继续对外提供服务。

打包现有的 Minecraft 数据文件，包括模组生成的文件等。对于笔者，执行了如下命令：

```bash
tar -czf mc-server.tar.gz banned-ips.json banned-players.json config dynmap journeymap mods ops.json server-icon.png server.properties usercache.json whitelist.json world
```

将打包后的文件传输到 Mac Mini 上：

```bash
scp mc-server.tar.gz username@192.168.100.239:/path/to/mc-server.tar.gz
```

解压到数据文件夹中：

```bash
tar -zxf /path/to/mc-server.tar.gz -C /path/to/mc-server-data
```

拉取最新版本的 `itzg/minecraft-server` 镜像，基于它运行 Minecraft 容器：

```bash
docker run --name mc-server-fabric --restart unless-stopped -v /path/to/mc-server-data:/data -e TYPE=FABRIC -e VERSION=1.20.4 -e FABRIC_LOADER_VERSION=0.15.10 -e FABRIC_LAUNCHER_VERSION=1.0.1 -e EULA=TRUE -d -p 25565:25565 -p 8123:8123 itzg/minecraft-server:latest
```

上面的环境变量表示，启用一个 Fabric 模组服务器，Minecraft 版本为 1.20.4，Fabric loader 版本为 0.15.10，Fabric launcher 版本为 1.0.1。笔者的 Minecraft 服务器对外提供了 Dynmap 在线地图服务，因此除了 Minecraft 主服务端口 25565 外，还对外暴露了 8123 地图服务端口。

这样，容器会在初始化时自动下载对应版本的 Fabric 加载器即 `fabric-server-mc.1.20.4-loader.0.15.10-launcher.1.0.1.jar`，最后像原来一样运行起来 Minecraft 服务器，完成对 Minecraft 服务器的迁移！

![初始化 Minecraft 服务器](./mac-mini-server/docker-minecraft-initialize.png)

从刀耕火种的 `screen` 后台应用，到如今的 Docker 容器服务，也算是一次小型的 Minecraft 服务端架构升级吧\~

## 开始漫长的工作

最初对 Mac Mini 的配置与调教依赖于 WiFi 和显示屏，在远程桌面就绪后，就可以将它移动到无线 AP 旁边，作为一台沉默的服务器开始漫长的工作啦！

关闭 Mac Mini 的 WiFi 功能，关闭主机，将主机移动到无线 AP 旁边，使用网线将两者连接。

启动 Mac Mini，主路由服务器的 DHCP 服务将会自动为 Mac Mini 分配前面笔者固定好的 IPv4 地址。

通过主控端 Rust Desk 访问 Mac Mini，顺利连接到远程桌面，一切搞定！

## 写在最后

兜兜转转折折腾腾两天时间，终于把所有应用服务从主路由服务器上迁移到了 Mac Mini 上，可喜可贺可喜可贺！

![Docker 容器服务列表](./mac-mini-server/docker-containers.png)

过程总体来说是相当顺利的，没有遇到什么疑难杂症或不兼容的问题，迁移到 Docker 里的应用服务也都一次正常跑通，不由让我对 Docker 依赖愈陷愈深。

其实笔者购买 Mac Mini 4 的另一个初衷是拿它做旁路由，替换目前的 OpenWRT 主路由，但是又觉得过于繁琐（源于过去心血的积累），最终搁浅了这个计划。

现在在笔者的家里，就由 N100 主路由服务器与 Mac Mini 4 轻量级应用服务器相辅相成，共同承载笔者捣腾的心理需要。
</content:original-text><content:updated-at>2025-03-31T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[对油猴脚本 Pixiv Previewer 进行二次开发]]></title><description><![CDATA[笔者的业余爱好之一就是逛 Pixiv，收藏好看的插画等作品并下载到本地。油猴脚本 Pixiv Previewer 做了两个笔者很喜欢的便利性工作： 作品预览：鼠标悬浮在作品缩略图上时自动在当前页面显示大图，快速决定要不要点进作品页。 作品排序：作品搜索页可以按“排序”按钮，自动获取若干页的作品并按照收藏数排序，快速筛选优质作品。

出于爱好，笔者去脚本仓库翻阅了一下源码，觉得有一些可以改进的地方…]]></description><link>https://blog.towind.fun/posts/pixiv-previewer</link><guid isPermaLink="false">pixiv-previewer</guid><category><![CDATA[前端开发]]></category><pubDate>Mon, 17 Mar 2025 00:00:00 GMT</pubDate><content:original-text>
笔者的业余爱好之一就是逛 Pixiv，收藏好看的插画等作品并下载到本地。油猴脚本 Pixiv Previewer 做了两个笔者很喜欢的便利性工作：

1. **作品预览**：鼠标悬浮在作品缩略图上时自动在当前页面显示大图，快速决定要不要点进作品页。
2. **作品排序**：作品搜索页可以按“排序”按钮，自动获取若干页的作品并按照收藏数排序，快速筛选优质作品。

出于爱好，笔者去[脚本仓库](https://github.com/Ocrosoft/PixivPreviewer)翻阅了一下源码，觉得有一些可以改进的地方，遂 Fork 之，进行二次开发。

## 工程化改造

首当其冲的自然是前端项目工程化改造。大多数油猴脚本仓库都是孤零零的一个 JavaScript 文件，Pixiv Previewer 也不例外。直接手搓 JavaScript 虽然很便利，但在项目长期的维护开发中多半会掩埋掉许多未曾留意的 Bug，同时由于缺少关联提示，开发效率也会大打折扣。作为前端工程师，这样可怕的事情自然是不能接受的。

### 通过 tsup 构建项目

初始化 `package.json` 等过程跳过不表，添加 TypeScript 支持自然是对项目进行二次开发的前置步骤。由于我们最后构建的产物将作为油猴脚本运行，即打包为库，因此笔者选择了开箱即用的 [tsup](https://github.com/egoist/tsup)：

```bash
yarn add -D tsup
```

添加 tsup 的配置文件 `tsup.config.ts`：

```ts
import svg from &quot;esbuild-plugin-svg&quot;;
import { defineConfig } from &quot;tsup&quot;;

import packageJson from &quot;./package.json&quot;;

export default defineConfig({
  entry: [&quot;src/index.ts&quot;],
  target: [&quot;chrome107&quot;],
  minify: false,
  splitting: false,
  clean: true,
  platform: &quot;browser&quot;,
  esbuildPlugins: [svg()], // 为了正确处理项目中用到的 SVG 文件，添加了此插件
  env: {
    VERSION: packageJson.version,
  },
  banner: {
    js: `// ==UserScript==
// @name                Pixiv Previewer L
// @namespace           ${packageJson.homepage}
// @version             ${packageJson.version}-${new Date().toLocaleDateString()}
// @description         ${packageJson.description}
// @author              ${packageJson.author}
// @license             ${packageJson.license}
// @supportURL          ${packageJson.homepage}
// @match               *://www.pixiv.net/*
// @grant               GM_getValue
// @grant               GM_setValue
// @grant               GM_registerMenuCommand
// @grant               GM_unregisterMenuCommand
// @grant               GM.xmlHttpRequest
// @icon                https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;fallback_opts=TYPE,SIZE,URL&amp;size=32&amp;url=https://www.pixiv.net
// @icon64              https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;fallback_opts=TYPE,SIZE,URL&amp;size=64&amp;url=https://www.pixiv.net
// @require             https://update.greasyfork.org/scripts/515994/1478507/gh_2215_make_GM_xhr_more_parallel_again.js
// @require             http://code.jquery.com/jquery-3.7.1.min.js
// @run-at              document-end
// ==/UserScript==`,
  },
});
```

上面的配置表示，tsup 将打包构建源文件 `src/index.ts` 至默认产物文件 `dist/index.js`，保证兼容 `Chrome &gt;= 107`。其中，笔者复用了 `package.json` 里定义的若干字段，用于添加项目环境变量（脚本通过版本号判断是否显示更新日志框），以及生成油猴脚本顶部的配置项。

这样，当执行 `tsup` 命令时，构建的产物形如：

```js
// ==UserScript==
// @name                Pixiv Previewer L
// @namespace           https://github.com/LolipopJ/PixivPreviewer
// @version             1.1.1-2025/4/25
// @description         Original project: https://github.com/Ocrosoft/PixivPreviewer.
// @author              Ocrosoft, LolipopJ
// @license             GPL-3.0
// @supportURL          https://github.com/LolipopJ/PixivPreviewer
// @match               *://www.pixiv.net/*
// @grant               GM_getValue
// @grant               GM_setValue
// @grant               GM_registerMenuCommand
// @grant               GM_unregisterMenuCommand
// @grant               GM.xmlHttpRequest
// @icon                https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;fallback_opts=TYPE,SIZE,URL&amp;size=32&amp;url=https://www.pixiv.net
// @icon64              https://t0.gstatic.com/faviconV2?client=SOCIAL&amp;type=FAVICON&amp;fallback_opts=TYPE,SIZE,URL&amp;size=64&amp;url=https://www.pixiv.net
// @require             https://update.greasyfork.org/scripts/515994/1478507/gh_2215_make_GM_xhr_more_parallel_again.js
// @require             http://code.jquery.com/jquery-3.7.1.min.js
// @run-at              document-end
// ==/UserScript==

// ... 脚本构建后的代码
```

在默认情况下，tsup 会使用预设的 `tsconfig.json`。但笔者在开发过程中，发现 VSCode 有时会遗漏某些类型的定义，于是手动添加了 `tsconfig.json` 在根目录：

```json
{
  &quot;compilerOptions&quot;: {
    &quot;target&quot;: &quot;ES2022&quot;,
    &quot;module&quot;: &quot;esnext&quot;,
    &quot;esModuleInterop&quot;: true,
    &quot;moduleResolution&quot;: &quot;bundler&quot;,
    &quot;skipLibCheck&quot;: true,
    &quot;forceConsistentCasingInFileNames&quot;: true,
    &quot;lib&quot;: [&quot;esnext&quot;, &quot;dom&quot;]
  },
  &quot;include&quot;: [&quot;src/&quot;],
  &quot;exclude&quot;: [&quot;node_modules/&quot;, &quot;dist/&quot;]
}
```

### 代码质量检查与格式化工具

TypeScript 自身已包含类型检查等能力，但还可以引入更多质量检查的通用规范，提升项目的健壮性。对于 TypeScript 项目，目前来看最好的选择应当是 [`typescript-eslint`](https://github.com/typescript-eslint/typescript-eslint)，参考官方文档添加依赖并配置 `eslint.config.mjs`：

```bash
yarn add --dev eslint @eslint/js typescript typescript-eslint
```

```js
// @ts-check

import eslint from &quot;@eslint/js&quot;;
import tseslint from &quot;typescript-eslint&quot;;

export default tseslint.config(
  eslint.configs.recommended,
  tseslint.configs.recommended,
);
```

现在执行 `eslint src --fix` 即可查找并尝试修复 `src/` 目录下所有文件的质量问题。

接着为项目添加自动格式化能力，Prettier 是笔者最推荐的选项。此外，笔者喜欢将 Prettier 与 ESLint 结合起来一起使用，即在执行 `eslint src --fix` 修复质量问题的同时，自动格式化代码。ESLint 的插件 [`eslint-plugin-prettier`](https://github.com/prettier/eslint-plugin-prettier) 可以实现如上能力，添加此插件作为依赖并进一步配置 `eslint.config.mjs`：

```bash
yarn add -D prettier eslint-plugin-prettier eslint-config-prettier
```

```js
// @ts-check

import eslint from &quot;@eslint/js&quot;;
import tseslint from &quot;typescript-eslint&quot;;
import eslintPluginPrettierRecommended from &quot;eslint-plugin-prettier/recommended&quot;;

export default tseslint.config(
  eslint.configs.recommended,
  tseslint.configs.recommended,
  eslintPluginPrettierRecommended,
);
```

此外，Pixiv Previewer 项目依赖于 JQuery 来执行查找与修改 DOM 的操作，笔者添加了相应的类型提示：

```bash
yarn add -D @types/jquery globals
```

```js
// @ts-check

import eslint from &quot;@eslint/js&quot;;
import tseslint from &quot;typescript-eslint&quot;;
import eslintPluginPrettierRecommended from &quot;eslint-plugin-prettier/recommended&quot;;
import globals from &quot;globals&quot;;

export default tseslint.config(
  eslint.configs.recommended,
  tseslint.configs.recommended,
  {
    languageOptions: {
      globals: {
        ...globals.browser,
        ...globals.jquery,
      },
    },
  },
  eslintPluginPrettierRecommended,
);
```

至此，基本的项目工程化改造告一段落。项目可以基于 TypeScript 开发，并且拥有了质量检查和格式化能力。将原来的 JavaScript 代码放到 `src/index.ts` 文件里，开始正式的二次开发吧！

## 预览功能优化

笔者在阅读 Pixiv Previewer 源码时发现，原作者根据当前页面的不同，实现了不同的通过 DOM 节点寻找可预览作品的 `&lt;img&gt;` 标签的方法，并为它添加监听事件，当鼠标悬浮在上面的时候显示预览。为了应对作品动态加载的情况，脚本设置了循环定时器，每隔一小段时间重新执行上述方法。

这种力大砖飞的实现方案存在一个不可忽视的缺陷：要考虑到每个页面里不同的作品 `&lt;img&gt;` 标签的排布情况，为之实现绑定事件的方法。对于没有考虑到的作品 `&lt;img&gt;` 标签，自然无法显示预览。例如，在当前的 `Pixiv Previewer@3.7.32` 版本，艺术家弹窗里最近的作品就无法显示预览：

![艺术家最近作品暂未实现预览功能](./pixiv-previewer/unimplemented-preview.png)

另外，如果 Pixiv 前端页面某一天迭代更新，寻找作品 `&lt;img&gt;` 标签的方法也可能需要大刀阔斧地修改。

&gt; 发布这篇博客的第二天，2025 年 03 月 18 日，Pixiv 就上线了新的首页，“推荐作品”模块的下面变成了无限滚动的推文模式。由于推文存在高度上限，因此部分尺寸的图片还是会出现显示不全的情况，可以添加预览功能。

笔者想到的改进方案是：把找作品 `&lt;img&gt;` 标签的工作，从脚本手中移交到用户手中。即监听鼠标移动事件，当悬浮到可预览作品上时，显示预览。

```ts
const onMouseMove = (mouseMoveEvent: JQuery.MouseMoveEvent) =&gt; {
  onMouseOverIllust(mouseMoveEvent.target);
};

$(document).on(&quot;mousemove&quot;, onMouseMove);
```

在上面的代码里，笔者为 Document 全局绑定了 `mousemove` 监听事件，当鼠标移动时，调用 `onMouseOverIllust(target)` 方法，其中 `target` 是当前鼠标所悬浮的元素。如果监听的是 `mouseover` 事件，脚本可以有更好的性能表现，但是笔者在测试过程中，发现 `mouseover` 事件回调对象里的 `ctrlKey` 等值有时不能正确反映实际状态，所以这里选择了监听 `mousemove` 事件。

接着，需要判断鼠标当前悬浮的元素是否为可预览的作品，如果是则显示预览。观察 DOM 节点：

![作品 DOM 节点](./pixiv-previewer/hovered-illust.png)

如果当前鼠标悬浮在可预览的作品 `&lt;img&gt;` 元素上，则本身包含了访问链接的 `src` 属性，通过简单的正则表达式处理即可获取作品的 Pixiv ID 和大图链接。同时，外层还包裹着一层 `&lt;a&gt;` 元素，包含跳转链接。

考虑到部分 `&lt;img&gt;` 元素虽然包含作品缩略图的 `src` 属性，但跳转的页面并不一定是作品页本身（`https://www.pixiv.net/artworks/artwork-id`），即可能只是作为封面打开别的页面，鼠标悬浮在这些 `&lt;img&gt;` 元素上时不应当显示预览。因此笔者实现的判断方法是：统一取当前悬浮元素的第一个父 `&lt;a&gt;` 节点，当其 `href` 值满足 `/\/artworks\/(\d+)/` 时，显示预览：

```ts
const getIllustMetadata = (target: JQuery&lt;HTMLElement&gt;) =&gt; {
  let imgLink = target;
  while (!imgLink.is(&quot;A&quot;)) {
    imgLink = imgLink.parent();

    if (!imgLink.length) {
      return null;
    }
  }

  const illustHref = imgLink.attr(&quot;href&quot;);
  const illustHrefMatch = illustHref?.match(/\/artworks\/(\d+)/);
  if (!illustHrefMatch) {
    return null;
  }
  const illustId = illustHrefMatch[1];

  const ugoiraSvg = imgLink.children(&quot;div:first&quot;).find(&quot;svg:first&quot;);
  const illustType =
    ugoiraSvg.length || imgLink.hasClass(&quot;ugoku-illust&quot;)
      ? IllustType.UGOIRA // 动图作品
      : IllustType.ILLUST; // 插画或漫画作品

  return {
    /** 作品 ID */
    illustId,
    /** 作品类型 */
    illustType,
  };
};

const onMouseOverIllust = (target: JQuery&lt;HTMLElement&gt;) =&gt; {
  const { illustId, illustType } = getIllustMetadata(target) || {};
  previewIllust({ target, illustId, illustType });
};
```

插画或漫画作品可能存在多页，处理时可以始终调用 Pixiv 官方的接口 `/ajax/illust/${artwork-id}/pages`，获取指定作品包含的所有链接。

再修修补补，为事件加上节流函数，响应鼠标滚动切换页数等，实现了和原版并无二异的预览能力：

![Pixiv 预览](./pixiv-previewer/previewer.png)

可喜可贺，可喜可贺！

## 排序功能重构与优化

原脚本的预览功能已经能够非常好地实现笔者需要的能力了，笔者想要在此基础上加入一些额外的能力或做一些优化。

其实最开始的原动力来自于原脚本在某次更新以后，排序功能突然变得很慢，过去笔者一次性喜欢排序 20 页作品再挑选喜欢的，现在却要先等个二十分钟，还可能“中道崩殂”。后面看公告才了解到，Pixiv 从某天开始对来自于同 IP 的请求进行了限制，如果短时间内超过了请求的上限，则会暂时屏蔽请求。这点对于 Pixiv 来说无可厚非，能大大缓解像笔者这样的服务器蛀虫的“攻击”。

于是便开始了本以为无限期的咕咕咕，但最近代码写得手感火热，想到了这个坑，就花了几天填一填。同时也多接触接触原生 DOM 操作等小知识，百利无一害。

### 请求作品分页

排序功能的本质就是同时请求多个分页，再获取每个作品的详细信息，最后在前端显示出来。下面以标签搜索结果的页面为例，实现排序功能。

对于不同的可排序页面，需要获取的要素有三：

- 请求分页的接口。
- 请求分页的参数。
- 作品的列表 DOM 节点。

使用观察法，在网络请求里找到分页的接口与默认的搜索参数：

```ts
function getSortOptionsFromPathname(pathname: string) {
  let type: IllustSortType;
  let api: string;
  let defaultSearchParams: string;

  let match: RegExpMatchArray;
  if (
    (match = pathname.match(/\/tags\/(.+)\/(artworks|illustrations|manga)$/))
  ) {
    const tagName = match[1];
    const filterType = match[2];

    switch (filterType) {
      case &quot;artworks&quot;:
        type = IllustSortType.TAG_ARTWORK;
        api = `/ajax/search/artworks/${tagName}`;
        defaultSearchParams = `word=${tagName}&amp;order=date_d&amp;mode=all&amp;p=1&amp;csw=0&amp;s_mode=s_tag_full&amp;type=all&amp;lang=zh`;
        break;
      case &quot;illustrations&quot;:
        type = IllustSortType.TAG_ILLUST;
        api = `/ajax/search/illustrations/${tagName}`;
        defaultSearchParams = `word=${tagName}&amp;order=date_d&amp;mode=all&amp;p=1&amp;csw=0&amp;s_mode=s_tag_full&amp;type=illust_and_ugoira&amp;lang=zh`;
        break;
      case &quot;manga&quot;:
        type = IllustSortType.TAG_MANGA;
        api = `/ajax/search/manga/${tagName}`;
        defaultSearchParams = `word=${tagName}&amp;order=date_d&amp;mode=all&amp;p=1&amp;csw=0&amp;s_mode=s_tag_full&amp;type=manga&amp;lang=zh`;
        break;
    }
  }

  return {
    type,
    api,
    searchParams: new URLSearchParams(defaultSearchParams),
  };
}
```

当用户在前端设置搜索条件的时候，搜索条件会自动追加到当前的 Search Params 上。脚本在处理的时候，也应该读取 Search Params，再与方法 `getSortOptionsFromPathname()` 得到的默认 `searchParams` 进行合并，让接口请求符合用户预期：

```ts
const url = new URL(location.href);
const { pathname, searchParams } = url;

const {
  type,
  api,
  searchParams: defaultSearchParams,
} = getSortOptionsFromPathname(pathname);

const mergedSearchParams = new URLSearchParams(defaultSearchParams);
searchParams.forEach((value, key) =&gt; {
  // 相同参数覆盖，不同参数追加
  mergedSearchParams.set(key, value);
});
```

接着，获取陈列作品的列表 DOM 节点：

```ts
function getIllustrationsListDom(type: IllustSortType) {
  let dom: JQuery&lt;HTMLUListElement&gt;;
  if (
    [
      IllustSortType.TAG_ARTWORK,
      IllustSortType.TAG_ILLUST,
      IllustSortType.TAG_MANGA,
    ].includes(type)
  ) {
    dom = $(&quot;ul.sc-ad8346e6-1.iwHaa-d&quot;);
  }

  if (dom) {
    return dom;
  } else {
    throw new Error(`Illustrations list DOM not found.`);
  }
}
```

Pixiv 的前端使用了 CSS Module 技术来生成样式名，能够有效避免同名样式冲突的问题。但是对于脚本开发者来说会增加一定的维护成本 —— 每次 Pixiv 的样式改变而重新构建样式文件时，需要手动更新脚本里对应的样式名，以正确找到目标节点。

因此在实现里，当脚本寻找节点失败时，会抛出一个错误，阻止脚本的继续运行。作为开发者，在碰到这样的问题时，就得赶紧更新脚本，或者想一想有没有更好的方式能定位此节点（例如搜索包含 60 个子元素的 `ul` 节点 🤔）。

### 获取作品详细信息

为了获取作品的收藏数据以排序，需要通过另外的接口获取作品的详细信息（如果请求分页的时候，响应值就把作品的收藏数带上就好了……）。

这里直接沿用了原作者使用的接口 `/touch/ajax/illust/details?illust_id=${id}`：

```ts
async function getIllustrationDetailsWithCache(id: string) {
  let illustDetails: IllustrationDetails =
    await getCachedIllustrationDetails(id);

  if (illustDetails) {
    iLog.d(`Use cached details for illustration ${id}`, illustDetails);
  } else {
    const requestUrl = `/touch/ajax/illust/details?illust_id=${id}`;
    const getIllustDetailsRes = await requestWithRetry({
      url: requestUrl,
      onRetry: (response, retryTimes) =&gt; {
        iLog.w(
          `Get illustration details through \`${requestUrl}\` failed:`,
          response,
          `${retryTimes} times retrying...`,
        );
      },
    });
    illustDetails = (
      getIllustDetailsRes.response as PixivStandardResponse&lt;{
        illust_details: IllustrationDetails;
      }&gt;
    ).body.illust_details;
  }

  cacheIllustrationDetails(illustDetails);

  return illustDetails;
}
```

在此基础上做了两个额外的工作。其一是缓存作品详细信息：对于发布时间超过 6 小时的作品，我们认为作品的收藏数变化率会趋于平稳，则将作品的详细信息缓存 12 小时。12 小时内再获取此作品的详细信息时，直接从 Indexed DB 里拿。省略 Indexed DB 初始化和错误处理等逻辑，核心实现如下：

```ts
/** 缓存数据表名称 */
const ILLUSTRATION_DETAILS_CACHE_TABLE_KEY = &quot;illustrationDetailsCache&quot;;
/** 缓存过期时间 */
const ILLUSTRATION_DETAILS_CACHE_TIME = 1000 * 60 * 60 * 12; // 12 小时
/** 不添加缓存的新作品发布时间 */
const NEW_ILLUSTRATION_NOT_CACHE_TIME = 1000 * 60 * 60 * 6; // 6 小时

/** 缓存作品详细信息 */
const cacheIllustrationDetails = (
  illustration: IllustrationDetails,
  now: Date = new Date(),
) =&gt; {
  return new Promise&lt;void&gt;(() =&gt; {
    const cachedIllustrationDetailsObjectStore = db
      .transaction(ILLUSTRATION_DETAILS_CACHE_TABLE_KEY, &quot;readwrite&quot;)
      .objectStore(ILLUSTRATION_DETAILS_CACHE_TABLE_KEY);

    const createDate = new Date(illustration.createDate);

    if (
      now.getTime() - createDate.getTime() &gt;
      NEW_ILLUSTRATION_NOT_CACHE_TIME
    ) {
      // 作品发布超过一定时间，添加缓存
      const illustrationDetails: IllustrationDetailsCache = {
        ...illustration,
        cacheDate: now,
      };
      // 这里简单处理，如果数据库里已经包含缓存数据，则重新设置缓存过期时间为 12 小时
      // 当然，更合理的方式是存在时跳过，不重新设置缓存过期时间
      const addCachedIllustrationDetailsRequest =
        cachedIllustrationDetailsObjectStore.put(illustrationDetails);
    }
  });
};

/** 获取作品详细信息缓存 */
const getCachedIllustrationDetails = (id: string, now: Date = new Date()) =&gt; {
  return new Promise&lt;IllustrationDetailsCache | undefined&gt;((resolve) =&gt; {
    const cachedIllustrationDetailsObjectStore = db
      .transaction(ILLUSTRATION_DETAILS_CACHE_TABLE_KEY, &quot;readwrite&quot;)
      .objectStore(ILLUSTRATION_DETAILS_CACHE_TABLE_KEY);

    const getCachedIllustrationDetailsRequest =
      cachedIllustrationDetailsObjectStore.get(id);

    getCachedIllustrationDetailsRequest.onsuccess = (event) =&gt; {
      const illustrationDetails = (
        event.target as IDBRequest&lt;IllustrationDetailsCache | undefined&gt;
      ).result;
      if (illustrationDetails) {
        const { cacheDate } = illustrationDetails;
        if (
          now.getTime() - cacheDate.getTime() &lt;=
          ILLUSTRATION_DETAILS_CACHE_TIME
        ) {
          // 缓存未过期，返回缓存结果
          resolve(illustrationDetails);
        }
      }
      resolve(undefined);
    };
  });
};
```

设置缓存的主要目的是减少被 Pixiv 限制请求的可能性，也便于开发测试。

第二个额外的工作是：为获取作品详细信息的请求添加错误重试机制，避免因 Pixiv 限制引起的排序功能失效。这一点通过封装请求方法实现：

```ts
const xmlHttpRequest = window.GM.xmlHttpRequest;

/** 标准的请求方法 */
export const request = &lt;TContext = unknown&gt;(
  options: Tampermonkey.Request&lt;TContext&gt;,
) =&gt; {
  const { headers, ...restOptions } = options;
  return xmlHttpRequest&lt;TContext&gt;({
    responseType: &quot;json&quot;,
    ...restOptions,
    headers: {
      referer: &quot;https://www.pixiv.net/&quot;,
      ...headers,
    },
  });
};

/** 封装了错误重试机制的请求方法 */
export const requestWithRetry = async &lt;TContext = unknown&gt;(
  options: Tampermonkey.Request&lt;TContext&gt; &amp; {
    /** 重试间隔时间（ms） */
    retryDelay?: number;
    /** 最大重试次数 */
    maxRetryTimes?: number;
    /** 重试回调 */
    onRetry?: (
      response: Tampermonkey.Response&lt;TContext&gt;,
      retryTimes: number,
    ) =&gt; void;
  },
) =&gt; {
  const {
    retryDelay = 10000,
    maxRetryTimes = Infinity,
    onRetry,
    ...restOptions
  } = options;

  let response: Tampermonkey.Response&lt;TContext&gt;;
  let retryTimes = 0;
  while (retryTimes &lt; maxRetryTimes) {
    response = await request&lt;TContext&gt;(restOptions);

    if (response.status === 200) {
      const responseData = response.response as PixivStandardResponse&lt;unknown&gt;;
      if (!responseData.error) {
        return response;
      }
    }

    retryTimes += 1;
    onRetry?.(response, retryTimes);
    await pause(retryDelay);
  }
  throw new Error(
    `Request for ${restOptions.url} failed: ${response.responseText}`,
  );
};
```

在默认情况下，如果请求失败，那么会间隔 10s 重新发起相同请求，重试的次数无上限。直到请求获得成功响应值时，返回响应的结果。

最后，实现并发请求的能力：

```ts
export const execLimitConcurrentPromises = async &lt;T&gt;(
  promises: (() =&gt; Promise&lt;T&gt;)[],
  limit = 48,
) =&gt; {
  const results: T[] = [];
  let index = 0;

  const executeNext = async () =&gt; {
    if (index &gt;= promises.length) return Promise.resolve();

    const currentIndex = index++;
    const result = await promises[currentIndex]();
    results[currentIndex] = result;
    return await executeNext();
  };

  const initialPromises = Array.from(
    { length: Math.min(limit, promises.length) },
    () =&gt; executeNext(),
  );

  await Promise.all(initialPromises);
  return results;
};

const getDetailedIllustrationPromises: (() =&gt; Promise&lt;IllustrationDetails&gt;)[] =
  [];
for (let i = 0; i &lt; illustrations.length; i += 1) {
  getDetailedIllustrationPromises.push(async () =&gt; {
    const illustration = illustrations[i];
    const illustrationId = illustration.id;
    const illustrationDetails =
      await getIllustrationDetailsWithCache(illustrationId);
    return {
      ...illustration,
      bookmark_user_total: illustrationDetails.bookmark_user_total,
    } as IllustrationDetails;
  });
}
const detailedIllustrations = await execLimitConcurrentPromises(
  getDetailedIllustrationPromises,
);
```

在默认情况下，脚本会发起至多 48 个获取作品详细信息的并发请求，提升排序功能的处理效率。而且经测试，并发能在一定程度上“绕过”风控机制，至少在首次完成排序时，不会被限制请求。只是结束后，Pixiv 的服务器“反应过来”了，就会被关几分钟小黑屋了 😿。

关于并发请求功能，值得补充的一点是，由于 [Chrome MV3 issue](https://github.com/w3c/webextensions/issues/694)，笔者参考原作者的处理，引入了修复脚本 `https://update.greasyfork.org/scripts/515994/1478507/gh_2215_make_GM_xhr_more_parallel_again.js` 使得并行请求功能生效。

### 前端展示作品

根据条件过滤作品和排序作品的过程跳过不表，直接来到最后展示作品的环节！

```ts
class IllustrationsSorter {
  type: IllustSortType;
  illustrations: IllustrationDetails[];
  listElement: JQuery&lt;HTMLUListElement&gt;;

  constructor() {
    // ...
    this.showIllustrations();
  }

  showIllustrations() {
    const fragment = document.createDocumentFragment();
    for (const {
      aiType,
      alt,
      bookmarkData,
      bookmark_user_total,
      id,
      illustType,
      pageCount,
      profileImageUrl,
      tags,
      title,
      url,
      userId,
      userName,
    } of this.illustrations) {
      const isUgoira = illustType === IllustType.UGOIRA;

      const listItem = document.createElement(&quot;li&quot;);

      const container = document.createElement(&quot;div&quot;);
      container.style = &quot;width: 184px;&quot;;

      const illustrationAnchor = document.createElement(&quot;a&quot;);
      illustrationAnchor.setAttribute(&quot;data-gtm-value&quot;, id);
      illustrationAnchor.setAttribute(&quot;data-gtm-user-id&quot;, userId);
      illustrationAnchor.href = `/artworks/${id}`;
      illustrationAnchor.target = &quot;_blank&quot;;
      illustrationAnchor.rel = &quot;external&quot;;
      illustrationAnchor.style =
        &quot;display: block; position: relative; width: 184px;&quot;;

      const illustrationImageWrapper = document.createElement(&quot;div&quot;);
      illustrationImageWrapper.style =
        &quot;position: relative; width: 100%; height: 100%; display: flex; align-items: center; justify-content: center;&quot;;

      const illustrationImage = document.createElement(&quot;img&quot;);
      illustrationImage.src = url;
      illustrationImage.alt = alt;
      illustrationImage.style =
        &quot;object-fit: cover; object-position: center center; width: 100%; height: 100%; border-radius: 4px; background-color: rgb(31, 31, 31);&quot;;

      const ugoriaSvg = document.createElement(&quot;div&quot;);
      ugoriaSvg.style = &quot;position: absolute;&quot;;
      ugoriaSvg.innerHTML = playIcon;

      const illustrationMeta = document.createElement(&quot;div&quot;);
      illustrationMeta.style =
        &quot;position: absolute; top: 0px; left: 0px; right: 0px; display: flex; align-items: flex-start; padding: 4px 4px 0; pointer-events: none; font-size: 10px;&quot;;
      illustrationMeta.innerHTML = `
          ${
            pageCount &gt; 1
              ? `
                &lt;div style=&quot;margin-left: auto;&quot;&gt;
                  &lt;div style=&quot;display: flex; justify-content: center; align-items: center; height: 20px; min-width: 20px; color: rgb(245, 245, 245); font-weight: bold; padding: 0px 6px; background: rgba(0, 0, 0, 0.32); border-radius: 10px; line-height: 10px;&quot;&gt;
                    ${pageIcon}
                    &lt;span&gt;${pageCount}&lt;/span&gt;
                  &lt;/div&gt;
                &lt;/div&gt;`
              : &quot;&quot;
          }
        `;

      const illustrationToolbar = document.createElement(&quot;div&quot;);
      illustrationToolbar.style =
        &quot;position: absolute; top: 154px; left: 0px; right: 0px; display: flex; align-items: center; padding: 0 4px 4px; pointer-events: none; font-size: 12px;&quot;;
      // Mock: 暂未实现点击爱心收藏 / 取消收藏的功能
      illustrationToolbar.innerHTML = `
          &lt;div style=&quot;padding: 0px 4px; border-radius: 4px; color: rgb(245, 245, 245); background: ${bookmark_user_total &gt; 50000 ? &quot;#9f1239&quot; : bookmark_user_total &gt; 10000 ? &quot;#dc2626&quot; : bookmark_user_total &gt; 5000 ? &quot;#1d4ed8&quot; : bookmark_user_total &gt; 1000 ? &quot;#15803d&quot; : &quot;#475569&quot;}; font-weight: bold; line-height: 16px; user-select: none;&quot;&gt;❤ ${bookmark_user_total}&lt;/div&gt;
          &lt;div style=&quot;margin-left: auto;&quot;&gt;${bookmarkData ? heartFilledIcon : heartIcon}&lt;/div&gt;
        `;

      const illustrationTitle = document.createElement(&quot;div&quot;);
      illustrationTitle.innerHTML = title;
      illustrationTitle.style =
        &quot;margin-top: 4px; max-width: 100%; overflow: hidden; text-decoration: none; text-overflow: ellipsis; white-space: nowrap; line-height: 22px; font-size: 14px; font-weight: bold; color: rgb(245, 245, 245); transition: color 0.2s;&quot;;

      const illustrationAuthor = document.createElement(&quot;a&quot;);
      illustrationAuthor.setAttribute(&quot;data-gtm-value&quot;, userId);
      illustrationAuthor.href = `/users/${userId}`;
      illustrationAuthor.target = &quot;_blank&quot;;
      illustrationAuthor.rel = &quot;external&quot;;
      illustrationAuthor.style =
        &quot;display: flex; align-items: center; margin-top: 4px;&quot;;
      illustrationAuthor.innerHTML = `
          &lt;img src=&quot;${profileImageUrl}&quot; alt=&quot;${userName}&quot; style=&quot;object-fit: cover; object-position: center top; width: 24px; height: 24px; border-radius: 50%; margin-right: 4px;&quot;&gt;
          &lt;span style=&quot;min-width: 0px; line-height: 22px; font-size: 14px; color: rgb(214, 214, 214); text-decoration: none; text-overflow: ellipsis; white-space: nowrap; overflow: hidden;&quot;&gt;${userName}&lt;/span&gt;
        `;

      illustrationImageWrapper.appendChild(illustrationImage);
      if (isUgoira) illustrationImageWrapper.appendChild(ugoriaSvg);
      illustrationAnchor.appendChild(illustrationImageWrapper);
      illustrationAnchor.appendChild(illustrationMeta);
      illustrationAnchor.appendChild(illustrationToolbar);
      illustrationAnchor.appendChild(illustrationTitle);
      container.appendChild(illustrationAnchor);
      container.appendChild(illustrationAuthor);
      listItem.appendChild(container);
      fragment.appendChild(listItem);
    }

    this.listElement.find(&quot;li&quot;).remove();
    this.listElement.append(fragment);
  }
}
```

在上面的代码里，笔者参考 Pixiv 作品节点的 DOM，手动构造了大抵一致的作品节点，尽管部分功能缺失，但能够满足排序作品的需要。笔者也考虑过克隆原生节点再修改 DOM 节点里的属性会否更好，但是忧心前端版本更新导致 DOM 节点结构更新，以及担心不同页面的 DOM 节点结构不同，所以还是人工捏出来了通用节点。

最后，在想要排序的页面点击按钮，看看大家都爱什么作品吧！

![排序包含“天童爱丽丝”标签的作品](./pixiv-previewer/sorting-result.png)

## 最后

附上 [Github 仓库地址](https://github.com/LolipopJ/PixivPreviewerL)与 [Tampermonkey 脚本地址](https://greasyfork.org/zh-CN/scripts/533844)，希望对有类似需求的人带来一些帮助。
</content:original-text><content:updated-at>2025-04-26T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[基于 Gatsby 打造我的个人博客系统]]></title><description><![CDATA[碎碎念 搭建一个自己的博客是无数跻身于 IT 行业开发者的心中最质朴的愿望！

回想笔者的历程，刚上大学的时候就摩拳擦掌想要实现一个博客系统，但在 Github 上创建了仓库以后就开始了无限期的拖延：自己没有任何的开发经验，不知道框架也不知道 UI 组件库，只知道要写 HTML + CSS + JavaScript 代码，面对眼前想要实现的完整系统不知该如何下手。

从零开始实现博客系统的计划虽然搁浅…]]></description><link>https://blog.towind.fun/posts/build-my-gatsby-blog</link><guid isPermaLink="false">build-my-gatsby-blog</guid><category><![CDATA[前端开发]]></category><pubDate>Wed, 12 Mar 2025 00:00:00 GMT</pubDate><content:original-text>
## 碎碎念

搭建一个自己的博客是无数跻身于 IT 行业开发者的心中最质朴的愿望！

回想笔者的历程，刚上大学的时候就摩拳擦掌想要实现一个博客系统，但在 Github 上创建了仓库以后就开始了无限期的拖延：自己没有任何的开发经验，不知道框架也不知道 UI 组件库，只知道要写 HTML + CSS + JavaScript 代码，面对眼前想要实现的完整系统不知该如何下手。

从零开始实现博客系统的计划虽然搁浅，但是拥有一个自己的博客的愿望没有改变。如果自己还没有能力去开发，那末至少基于现有的博客平台去搭建一个博客吧，毕竟对于博客来说，最重要的还是内容本身。终于在大二上学期的时候收集资料，横向对比，考量不同博客平台的优势与短板，最终选择了基于 Hexo 搭建自己的静态博客系统，能够部署在 Github Pages 上而不需要自己去维护服务器。还根据自己搭建的经验，模仿别人写了一篇搭建 Hexo 博客的&lt;Link to=&quot;/posts/hello-hexo-world&quot;&gt;简易教程&lt;/Link&gt;，这篇教程也顺理成章成为了自己的第一篇技术博客。

Hexo 由目录结构驱动，简单好用，作为博主可以全身心投入到博客的撰写中去。Hexo 的生态里已经有着非常多的惹眼且实用的博客主题，下载到到对应目录下再修改全局配置项即可一键启用。笔者在大学生涯相当慢热，直到认为自己应该开始做某件事情时，才不留余力地去 push 自己，可谓是充分享受了青春。因此在拥有了博客系统之后，反而失去了很多原初的动力，又因为自己并没有沉浸于对技术的学习中去，也就没有什么好写下来记成博客的。

直到大三下学期，笔者选定了成为前端开发工程师的道路，在众多面经中看到了博客的重要性，于是开始慢慢写起来。将实习遇到的困难，竞赛项目里的开发经验，以及自学过程中的收获整理为一篇篇博客，上传到自己的仓库里。望着终于能翻页的个人博客，一种纯粹的满足感油然而生。

后来又觉得自己的大学生涯有些太平淡了，于是便挑战向 Github 别人的项目提交 PR，其中提的最多的 PR 是给自己在用的博客主题 —— [hexo-theme-archer](https://github.com/fi3ework/hexo-theme-archer) 的，另外还成功申请成为了它的维护者，自发地帮忙去解决堆积的 Issues。现在想来那时候的自己还非常的稚嫩，Issues 里提什么就做什么，没有认真分析过它是否真的是一个问题。例如有个 Issue 提到希望提供图片左右浮动的功能，自己就写了两个样式类塞到主题里并贴上如何使用，但实际上这样的功能只需要博主在撰写博客的时候使用 `&lt;img&gt;` 标签的形式再添加行内样式就可以了。不过当然也有自认为做得不错的工作，例如看到有人提了一个 PR 为主题添加了暗色模式支持，一下子激发起了我的兴趣，于是在这个 PR 的基础上花了几天的功夫实现了完全的暗色模式适配，想必为大家的眼睛起到了一些保护的作用。

时间流转，随着前端开发成为了自己的工作，站点开发已然成为烹小鲜般的事情，不再神秘。某天笔者偶然看到了 [Thesis Theme Vision](https://thesis.priority.vision/)，很有笔记软件的感觉，十分符合自己对博客系统的审美喜好，终于耐不住想要去复刻实现的愿望，于是又创建了一个 Github 仓库。这一次，确实从零开始了自己的博客系统搭建工程。

## 技术选型

笔者最初考虑的是为 Hexo 开发一个主题，但是我实在很难投入到学习其支持的 EJS 或 Pug 这些模板语言中去，思来想去还是继续投资到我擅长的且更有未来的 React 中去吧，遂决定另起炉灶。

接下来无论是选择 Next.js 或 Umi 作为开发框架都没有问题，甚至直接基于 React 手搓也没有问题，但笔者还是对 Gatsby 和 Docusaurus 这类所谓的“静态站点生成器”非常感兴趣。考虑到笔者自身对博客系统高可自定义性的需要，最终选择了 Gatsby 作为框架。核心使用到的库版本如下：

- `gatsby@^5.0.0`
- `react@^18.0.0`

## 解析并渲染博客

### 解析 `.mdx` 文件为博客

不同于 Hexo 的将文件塞到 `source/_posts/` 目录下即识别为一篇博客，Gatsby 要求开发者实现“解析数据源到数据层”和“组件从数据层查询”两个阶段的工作。援引官方文档的数据层介绍图片如下：

![data layer](./build-my-gatsby-blog/data-layer.png)

解析数据源到数据层可以通过**源插件**实现，官方约定源插件以 `gatsby-source-` 开头。笔者现在想要将本地的 `.mdx` 文件解析到数据层，可以通过插件 [`gatsby-source-filesystem`](https://www.gatsbyjs.com/plugins/gatsby-source-filesystem/) 实现。编辑 `gatsby-config.ts`，添加此插件并配置欲解析的本地目录：

```ts
import type { GatsbyConfig } from &quot;gatsby&quot;;

const config: GatsbyConfig = {
  plugins: [
    {
      resolve: &quot;gatsby-source-filesystem&quot;, // 尽管官方文档在使用字符串时都使用的是反引号 `，但笔者更习惯使用单双引号
      options: {
        name: &quot;posts&quot;,
        path: &quot;path/to/posts/&quot;,
      },
      __key: &quot;posts&quot;,
    },
  ],
};

export default config;
```

上面的配置项表示插件将自动读取 `path/to/posts/` 目录下的文件信息。

但这还不够，源插件 `gatsby-source-filesystem` 仅允许查询与文件相关的元数据，但是却无法读取文件包含的具体内容。为了读取到文件内容，就要用到**转换器插件**。对于 `.mdx` 格式的 File 节点，可以通过插件 [`gatsby-plugin-mdx`](https://www.gatsbyjs.com/plugins/gatsby-plugin-mdx/) 转换为 MDX 节点。官方文档里的这张图能直观描述这个过程：

![data layer with nodes](./build-my-gatsby-blog/data-layer-with-nodes.png)

继续配置 `gatsby-config.ts` 如下：

```ts
import remarkGfm from &quot;remark-gfm&quot;;

const config: GatsbyConfig = {
  plugins: [
    {
      resolve: &quot;gatsby-source-filesystem&quot;,
      // ...
    },
    {
      resolve: &quot;gatsby-plugin-mdx&quot;,
      options: {
        gatsbyRemarkPlugins: [
          // 添加代码块高亮
          &quot;gatsby-remark-prismjs&quot;,
        ],
        mdxOptions: {
          remarkPlugins: [
            // 添加对 GitHub flavored markdown (GFM) 格式的支持
            remarkGfm,
          ],
        },
      },
    },
  ],
};
```

值得一提的是，由于 MDX 的语法与 Markdown 不同，它默认仅支持 CommonMark 格式，而某些非标准的 Markdown 功能如表格，需要单独配置插件启用。在上面的配置项里，笔者就引入了 `remark-gfm` 插件，它使得 `gatsby-plugin-mdx` 能够解析 GFM 格式的语法。但是由于[兼容性问题](https://github.com/gatsbyjs/gatsby/discussions/36406)，经测试，目前的 `gatsby-plugin-mdx@5.14.0` 只能搭配旧版本的 `remark-gfm@^1.0.0` 使用，不过也足够了。

官方教程在博客的 Frontmatter 中添加了 `slug` 属性来方便后续生成路由，但笔者更喜好 Hexo 那种文件路径驱动路由生成的模式。为此，可以在 `gatsby-node.ts` 中导出 `onCreateNode()` 方法，对于每一个 MDX 节点，根据文件路径生成 `slug` 属性：

```ts
import { type CreateNodeArgs } from &quot;gatsby&quot;;

export const onCreateNode = ({ node, actions }: CreateNodeArgs) =&gt; {
  const { createNodeField } = actions;
  if (node.internal.type === &quot;Mdx&quot;) {
    createNodeField({
      node,
      name: &quot;slug&quot;,
      value: parseFilePathToPostSlug(String(node.internal.contentFilePath)),
    });
  }
};
```

假设撰写了如下的博客文章：

```md
---
title: 从同步 QQ 空间说说到前端呈现，我都做了些啥
date: 2024-10-17
updated: 2024-10-17
timeliness: false
categories:
  - 全栈开发
tags:
  - React
  - TypeScript
  - Node
  - ffmpeg
---

最近在捣腾我的 Timeline 时间线项目，希望将我在不同平台上的发言和活跃记录同步过来，在独立的站点上按照创建时间倒序呈现。

...
```

访问本地的 GraphQL IDE 界面，查询到的 MDX 节点结果如下：

![query mdx nodes](./build-my-gatsby-blog/query-mdx-nodes.png)

截止目前，笔者完成了“解析数据源到数据层”这一阶段的工作，接下来就需要在组件读取这些 MDX 节点数据并展示了。

### 渲染博客

Gatsby 的 GraphQL 数据层是它区别于其它前端框架的重要特性，尽管最终构建的产物仍是静态的前端页面，但在开发的过程中引入了这样的类似数据库的概念，体验仿似自己与自己握手，自己同自己进行数据传递。

如果想在 React 组件里消费数据层里准备好的数据，就需要编写对应的 GraphQL 查询代码：

```ts
// src/hooks/useAllMdx.ts
import { graphql, useStaticQuery } from &quot;gatsby&quot;;

interface MdxNode {
  excerpt: string;
  fields: {
    slug: string;
  };
  frontmatter: {
    categories: string[];
    tags: string[];
    title: string;
    date: string;
    updated: string;
  };
}

export const useAllMdx = () =&gt; {
  const {
    allMdx: { nodes: posts },
  } = useStaticQuery&lt;{
    allMdx: { nodes: MdxNode[] };
  }&gt;(graphql`
    query {
      allMdx(sort: { frontmatter: { date: DESC } }) {
        nodes {
          excerpt
          fields {
            slug
          }
          frontmatter {
            categories
            tags
            title
            date
            updated
          }
        }
      }
    }
  `);

  return posts;
};

export default useAllMdx;
```

一个 React 组件里最多只能有 `useStaticQuery()` 方法，如果您想要查询多条不同数据，要么把查询代码写到一起，要么就新建一个钩子方法（推荐）。对于全局都会使用到的数据，可以分门别类放到不同的钩子方法里，例如网站的 Metadata，就可以把查询代码放在 `src/hooks/useSiteMetadata.ts` 里。

现在，笔者需要根据博客的文件路径，自动生成访问路由，例如对于博客文件 `path/to/this-is-a-blog.mdx`，可以通过路由 `/posts/this-is-a-blog` 访问到，该怎么实现呢？官方提供了多种方案，这里笔者选择了更符合自己喜好的，在 `gatsby-node.ts` 文件里导出 `createPages()` 方法的方案，它能够以编程的方式自定义生成路由，拥有相当大的自由度：

```ts
import { type CreatePagesArgs } from &quot;gatsby&quot;;
import path from &quot;path&quot;;

const postTemplate = path.resolve(&quot;./src/templates/post.tsx&quot;);

export const createPages = async function ({
  actions,
  graphql,
}: CreatePagesArgs) {
  const { data } = await graphql&lt;{
    allMdx: {
      nodes: MdxNode[];
    };
  }&gt;(`
    query {
      allMdx(
        sort: { frontmatter: { date: DESC } }
        filter: {
          internal: {
            contentFilePath: { regex: &quot;//blog/posts/|/blog/about-me.mdx/&quot; }
          }
        }
      ) {
        nodes {
          fields {
            slug
          }
          id
          internal {
            contentFilePath
          }
        }
      }
    }
  `);

  data?.allMdx.nodes.forEach((node) =&gt; {
    const slug = node.fields.slug;
    const path = slug === &quot;about-me&quot; ? &quot;/about-me&quot; : `/posts/${slug}`;

    actions.createPage({
      path,
      component: `${postTemplate}?__contentFilePath=${node.internal.contentFilePath}`,
      context: {
        id: node.id,
      },
    });
  });
};
```

笔者就根据博客的路径，单独配置了“关于我”页面的路由。此外，笔者读取了 `src/templates/post.tsx` 文件作为生成路由的页面模板，并按照 `gatsby-plugin-mdx` 推荐的方式，将 MDX 节点的文件路径作为了查询参数 `__contentFilePath` 的值，这样模板文件的 `props.children` 将自动被替换为 MDX 节点包含内容的解析结果。另外，笔者向组件传递了 MDX 节点 `id` 作为上下文，为模板文件查询 MDX 节点详细信息提供了参数。

下面，编写模板文件 `src/templates/post.tsx` 即可实现博客页面的展示了：

```tsx
import { MDXProvider } from &quot;@mdx-js/react&quot;;
import { graphql, type PageProps } from &quot;gatsby&quot;;
import * as React from &quot;react&quot;;

type PostPageData = {
  mdx: Pick&lt;MdxNode, &quot;frontmatter&quot;&gt;;
};

type PostPageContext = Pick&lt;MdxNode, &quot;id&quot;&gt;;

const PostTemplate: React.FC&lt;PageProps&lt;PostPageData, PostPageContext&gt;&gt; = ({
  children,
  data,
}) =&gt; {
  const {
    mdx: {
      frontmatter: { title, date, updated, categories, tags },
    },
  } = data;

  return (
    &lt;div&gt;
      &lt;h1&gt;{title}&lt;/h1&gt;
      &lt;article&gt;
        &lt;MDXProvider&gt;{children}&lt;/MDXProvider&gt;
      &lt;/article&gt;
    &lt;/div&gt;
  );
};

export const query = graphql`
  query ($id: String!) {
    mdx(id: { eq: $id }) {
      frontmatter {
        categories
        tags
        title
        date
        updated
      }
    }
  }
`;

export default PostTemplate;
```

通过上面导出的 `query` 静态查询字符串，Gatsby 将查询数据层里 `mdx.id === props.pageContext.id` 的 MDX 节点的指定属性，并把结果传递给 React 组件的 `props.data`。

这样，“组件从数据层查询”这一阶段的工作也完成了，剩下的就是慢慢优化页面逻辑和展示效果了！

### 添加自定义组件

MDX 格式的博客相比 MD 格式的最大优势在于能够使用自定义的 React 组件：

```tsx
import { type MDXProps } from &quot;mdx/types&quot;;

import Card from &quot;path/to/components/card&quot;;

const components: MDXProps[&quot;components&quot;] = {
  Card,
};

const PostTemplate = ({ children }) =&gt; {
  return (
    &lt;article&gt;
      &lt;MDXProvider components={components}&gt;{children}&lt;/MDXProvider&gt;
    &lt;/article&gt;
  );
};
```

现在，在编写 MDX 博客文件的时候，就可以像使用 JSX 一样使用 `&lt;Card&gt;` 组件了！

![card](./build-my-gatsby-blog/card.png)

另外，还可以自定义现有标签渲染的结果。例如想要为图片文件添加 FancyBox 的支持，可以编写代码如下：

```tsx
import { Fancybox } from &quot;@fancyapps/ui&quot;;

const FancyBoxImage = (props: { alt?: string; src?: string }) =&gt; {
  const {
    alt = &quot;The author is too lazy to give an alt&quot;,
    src,
    ...restProps
  } = props;
  return (
    &lt;a href={src} data-fancybox=&quot;gallery&quot; data-caption={alt}&gt;
      &lt;img src={src} alt={alt} {...restProps} /&gt;
    &lt;/a&gt;
  );
};

const components: MDXProps[&quot;components&quot;] = {
  img: FancyBoxImage,
};

const PostTemplate = ({ children }) =&gt; {
  const articleRef = React.useRef&lt;HTMLElement&gt;(null);

  React.useEffect(() =&gt; {
    Fancybox.bind(&quot;[data-fancybox]&quot;);
    return () =&gt; Fancybox.unbind(&quot;[data-fancybox]&quot;);
  }, []);

  return (
    &lt;article ref={articleRef}&gt;
      &lt;MDXProvider components={components}&gt;{children}&lt;/MDXProvider&gt;
    &lt;/article&gt;
  );
};
```

MDX 渲染得到的 `&lt;img&gt;` 组件将自动被替换为 `&lt;FancyBoxImage&gt;` 组件，随后正确地被 FancyBox 绑定。

### 静态资源的访问与优化

插件 `gatsby-remark-images` 将自动压缩博客中使用到的本地图片的体积，减少流量压力；同时生成占位图片，避免博客高度突然变化，提升阅读体验。在 Gatsby 的最佳实践中，应当始终使用类似 `gatsby-remark-images` 这样的插件来优化图片类型的静态资源。

结合插件 `gatsby-plugin-mdx` 使用，可以配置如下：

```ts
const config: GatsbyConfig = {
  plugins: [
    &quot;gatsby-plugin-sharp&quot;,
    {
      resolve: &quot;gatsby-plugin-mdx&quot;,
      options: {
        gatsbyRemarkPlugins: [
          {
            resolve: &quot;gatsby-remark-images&quot;,
            options: {
              maxWidth: 624,
            },
          },
          &quot;gatsby-remark-copy-linked-files&quot;,
        ],
      },
    },
  ],
};
```

其中，`gatsby-plugin-sharp` 是插件 `gatsby-remark-images` 的必要依赖，需要添加到插件列表中。

需要留意的是，插件 `gatsby-remark-images` 仅支持压缩、处理部分的图片格式如 `.jpeg`。其它的格式如 `.gif` 则需要插件 `gatsby-remark-copy-linked-files` 帮助，原封不动地移动到对应的目录里。协同使用这两个插件即可达成最好的静态资源访问效果。

除此之外，由于 `gatsby-remark-images` 会相应地封装 MDX 渲染结果里的 `&lt;img&gt;` 标签，为了兼容 FancyBox 能力，可以更新 `useEffect()` 钩子方法如下：

```tsx
const PostTemplate = ({ children }) =&gt; {
  const articleRef = React.useRef&lt;HTMLElement&gt;(null);

  React.useEffect(() =&gt; {
    const optimizedImageLinks =
      articleRef.current?.querySelectorAll&lt;HTMLLinkElement&gt;(
        &quot;a.gatsby-resp-image-link&quot;,
      );
    optimizedImageLinks?.forEach((link) =&gt; {
      const image = link.children.item(1) as HTMLImageElement;
      link.setAttribute(&quot;data-fancybox&quot;, &quot;gallery&quot;);
      link.setAttribute(&quot;data-caption&quot;, image.alt);
    });

    Fancybox.bind(&quot;[data-fancybox]&quot;);
    return () =&gt; Fancybox.unbind(&quot;[data-fancybox]&quot;);
  }, []);

  return (
    &lt;article ref={articleRef}&gt;
      &lt;MDXProvider components={components}&gt;{children}&lt;/MDXProvider&gt;
    &lt;/article&gt;
  );
};
```

## 博客搜索

笔者基于 Algolia 实现博客搜索的能力，插件 [`gatsby-plugin-algolia`](https://www.gatsbyjs.com/plugins/gatsby-plugin-algolia/) 能帮助项目快速实现索引数据并上传的功能。

在 Algolia 控制台创建索引并获取密钥等的操作不再冗述，将这些值塞到环境变量里即可被 Gatsby 访问：

```bash
# 包含 GATSBY_ 前缀的环境变量将暴露到客户端环境中
# 后续将用到下面三个变量实现客户端检索功能
GATSBY_ALGOLIA_APP_ID=YOUR_ALGOLIA_APP_ID
GATSBY_ALGOLIA_API_PUBLIC_KEY=YOUR_ALGOLIA_API_PUBLIC_KEY
GATSBY_ALGOLIA_INDEX_NAME=YOUR_ALGOLIA_INDEX_NAME
# 用于上传数据等的 Algolia 管理秘钥切勿添加 GATSBY_ 前缀
ALGOLIA_API_KEY=YOUR_ALGOLIA_API_PRIVATE_KEY
```

为了上传用于检索的博客元数据到 Algolia，可以像这么配置 `gatsby-config.ts`：

```ts
import dotenv from &quot;dotenv&quot;;

dotenv.config({
  path: [&quot;.env&quot;, `.env.${process.env.NODE_ENV}`],
});

const config: GatsbyConfig = {
  plugins: [
    {
      // ... 其它的插件
    },
    {
      // 应当将 gatsby-plugin-algolia 插件放到列表的最后
      // 确保 GraphQL 查询到的结果是经过其它插件处理后的最终结果
      resolve: &quot;gatsby-plugin-algolia&quot;,
      options: {
        appId: process.env.GATSBY_ALGOLIA_APP_ID,
        apiKey: process.env.ALGOLIA_API_KEY,
        indexName: process.env.GATSBY_ALGOLIA_INDEX_NAME,
        queries: [
          {
            query: `
              query {
                allMdx(
                  filter: {
                    internal: {
                      contentFilePath: { regex: &quot;//blog/posts//&quot; }
                    }
                  }
                ) {
                  nodes {
                    excerpt(pruneLength: 200)
                    fields {
                      slug
                    }
                    frontmatter {
                      categories
                      tags
                      title
                      date
                      updated
                      timeliness
                    }
                    id
                    internal {
                      contentDigest
                    }
                  }
                }
              }`,
            transformer: ({
              data,
            }: {
              data: {
                allMdx: {
                  nodes: MdxNode[];
                };
              };
            }) =&gt; data.allMdx.nodes,
          },
        ],
      },
    },
  ],
};
```

插件 `gatsby-plugin-algolia` 需要一个包含博客元数据的数组以执行上传操作，而 GraphQL 查询的结果是一个形如 `{ addMdx: { nodes: MdxNode[] }}` 的对象，因此要使用插件提供的 `transformer()` 方法，返回里面的 `nodes` 数组。

为了确保博客在索引里的唯一性，并确保索引里的数据始终最新，在 GraphQL 查询的时候必须提供 `id` 和 `internal.contentDigest`。

这样，当执行构建操作时，插件将自动把查询得到的博客元数据上传到指定的 Algolia 索引中去。登录 Algolia 控制台，即可查看上传的结果啦：

![algolia index item](./build-my-gatsby-blog/algolia-index-item.png)

客户端搜索博客的功能可以基于 Algolia 提供的前端组件实现，笔者实现的一个简单的例子是：

```tsx
import { liteClient as algoliasearch } from &quot;algoliasearch/lite&quot;;
import {
  Configure,
  Hits,
  InstantSearch,
  Pagination,
  SearchBox,
} from &quot;react-instantsearch&quot;;

import Post from &quot;path/to/post&quot;;

const searchClient = algoliasearch(
  String(process.env.GATSBY_ALGOLIA_APP_ID),
  String(process.env.GATSBY_ALGOLIA_API_PUBLIC_KEY),
);

const Hit = ({ hit }) =&gt; {
  return &lt;Post post={hit} /&gt;;
};

const AlgoliaSearch = () =&gt; {
  return (
    &lt;InstantSearch
      insights
      searchClient={searchClient}
      indexName={process.env.GATSBY_ALGOLIA_INDEX_NAME}
    &gt;
      &lt;Configure hitsPerPage={10} /&gt;
      &lt;SearchBox /&gt;
      &lt;Hits&lt;AlgoliaPostItem&gt; hitComponent={Hit} /&gt;
      &lt;Pagination /&gt;
    &lt;/InstantSearch&gt;
  );
};

export default AlgoliaSearch;
```

这样，客户端的博客搜索能力就实现了：

![algolia search item](./build-my-gatsby-blog/algolia-search-item.png)

更多的功能例如自动生成 RSS 订阅也可以参考上面的处理方案，搜索合适的插件配置实现。

## 路由切换体验优化

有的时候切换路由可能需要几秒钟时间处理，用户看不到页面切换的结果可能会非常焦虑。为了缓解这种焦虑，在库 `nprogress` 的帮助下，笔者添加了可视化的进度条。

这一切非常简单，只需要在 `gatsby-browser.ts` 文件导出两个生命周期钩子方法即可：

```ts
import nProgress from &quot;nprogress&quot;;

export const onRouteUpdateDelayed = () =&gt; {
  nProgress.start();
};

export const onRouteUpdate = () =&gt; {
  nProgress.done();
};
```

上面的代码表示，当路由切换耗时大于 1 秒时，显示进度条。当路由切换完成时，隐藏进度条。

当然，别忘了引入 `nprogress` 的样式文件，如果希望，还可以修改一下进度条的颜色。

## 持续部署

笔者基于 Github Workflow 实现项目的持续集成与持续部署，可以编写工作流文件 `.github/workflows/deployment.yml` 如下：

```yml
name: Blog System CI &amp; CD

on:
  push:
    branches:
      - main

jobs:
  pages:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Use Node.js 20.x
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: &quot;yarn&quot;

      - name: Cache Yarn dependencies
        uses: actions/cache@v4
        id: yarn-cache
        with:
          path: node_modules
          key: ${{ runner.os }}-yarn-${{ hashFiles(&apos;**/yarn.lock&apos;) }}
          restore-keys: |
            ${{ runner.os }}-yarn-

      - name: Install dependencies
        run: yarn install --immutable

      - name: Build Gatsby
        run: yarn build

      - name: Deploy
        uses: peaceiris/actions-gh-pages@v4
        with:
          personal_token: ${{ secrets.PERSONAL_ACCESS_TOKEN }} # 笔者将产物推送到了外部仓库，因此这里使用到了 Github 私人密钥
          publish_dir: ./public
          external_repository: LolipopJ/LolipopJ.github.io
          publish_branch: main
```
</content:original-text><content:updated-at>2025-03-17T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[从同步 QQ 空间说说到前端呈现，我都做了些啥]]></title><description><![CDATA[最近在捣腾我的 Timeline 时间线项目，希望将我在不同平台上的发言和活跃记录同步过来，在独立的站点上按照创建时间倒序呈现。 过去，我尝试把这个想法放到 Telegram 上实现，把发言和记录同步到我的频道上。但是格式转换的繁杂以及自由度上的限制让我大费周章，加之增量开设的同步内容会以消息的方式一条一条添加到末尾，无法按时间排序，最终我放弃了这个方案。

言归正传，在项目开发的过程中…]]></description><link>https://blog.towind.fun/posts/sync-qzone-talks</link><guid isPermaLink="false">sync-qzone-talks</guid><category><![CDATA[全栈开发]]></category><pubDate>Thu, 17 Oct 2024 00:00:00 GMT</pubDate><content:original-text>
最近在捣腾我的 Timeline 时间线项目，希望将我在不同平台上的发言和活跃记录同步过来，在独立的站点上按照创建时间倒序呈现。

过去，我尝试把这个想法放到 Telegram 上实现，把发言和记录同步到我的频道上。但是格式转换的繁杂以及自由度上的限制让我大费周章，加之增量开设的同步内容会以消息的方式一条一条添加到末尾，无法按时间排序，最终我放弃了这个方案。

言归正传，在项目开发的过程中，我遇到的一个相对复杂的场景即 QQ 空间说说的同步。本文事无巨细地记录下我在处理 QQ 空间说说同步的过程中，做了哪些工作，希望为有相应需求的厚米们带来一些灵感。

## 同步 QQ 空间说说

### 同步方案的探索与确定

非常自然而然的，笔者设想使用 Puppeteer 模拟用户操作，打开 QQ 空间网页端，输入账号和密码，进入到个人主页，根据 DOM 结构爬取得到说说的信息。同样非常自然而然的，在切换登录模式（从二维码登录到账号密码登录）步骤就卡着了，模拟点击切换登录模式按钮无效。笔者并非爬虫专家，没有此类问题的对抗经验，在搜索无果后无奈放弃。再想到后续可能还要处理登录安全验证，或处理别的防爬手段，判断 Puppeteer 的方案其实并不合适 QQ 空间说说的同步。

于是想到接口的方案，模拟登录请求，并把返回的 Cookies 塞给获取说说信息的请求。查看网络请求可知，H5 端的 QQ 空间使用 `https://h5.qzone.qq.com/webapp/json/mqzone_feeds/getActiveFeeds` 接口拉取说说信息，然而同时也发现请求路径里包含了两个不存在于 Cookies 的参数：`qzonetoken` 和 `g_tk`，它是通过某种特殊算法在前端生成的！

既然是前端生成的，那么算法一定有迹可循，求助于搜索引擎果然找到了 `g_tk` 的生成算法。

在搜索的过程中，还发现大家一般通过 `https://user.qzone.qq.com/proxy/domain/taotao.qq.com/cgi-bin/emotion_cgi_msglist_v6` 接口拉取说说说信息；另外在 Github 上找到了一个高星标的 Python 实现 [GetQzonehistory](https://github.com/LibraHp/GetQzonehistory)，可以基于它改巴改巴实现 Node.js 版本。

至此，同步方案得到了确定（毕竟实现好的 GetQzonehistory 珠玉在前），我们将通过接口的方式拉取说说信息。

### 登录方案的确定与实现

现在开始实现 QQ 空间登录的功能，翻看 GetQzonehistory 源码，发现其采用了扫码登录的实现方式，这应当是有所考量的，可以绕过登录安全验证等棘手的问题，当然也存在无法自动化同步任务的问题。

考虑到发空间说说并不是一个高频行为，在想要同步时手动扫码是可接受的，遂沿用了**扫码登录**的实现。整体流程如下：

1. 用户端：访问获取登录二维码页面。
2. 服务端：请求 `https://ssl.ptlogin2.qq.com/ptqrshow` 接口获取登录二维码，从响应的 Cookies 里获取 `qrsig`。将登录二维码返回给用户。
3. 服务端：轮询请求 `https://ssl.ptlogin2.qq.com/ptqrlogin` 得到扫码结果，请求的路径参数 `ptqrtoken` 基于上一步得到的 `qrsig` 生成，生成算法如下：

   ```ts
   const getPtqrToken = (qrSig: string) =&gt; {
     let ptqrToken = 0;

     for (let i = 0; i &lt; qrSig.length; i += 1) {
       ptqrToken += (ptqrToken &lt;&lt; 5) + qrSig.charCodeAt(i);
     }

     return 2147483647 &amp; ptqrToken;
   };
   ```

4. 用户端：使用手机扫码并确认登录。
5. 服务端：轮询响应包含 `登录成功` 字段，获取响应体里 `ptsigx` 和响应 Cookies 里键 `uin` 的值。
6. 服务端：将上一步得到的 `ptsigx` 和 `uin` 作为请求 `https://ptlogin2.qzone.qq.com/check_sig` 的路径参数。请求成功，响应为 302 状态码，将响应的 Cookies 保存到本地，此即为所需的用户登录态 Cookies。

### 同步方案的实现

登录态 Cookies 已解决，同步的具体实现也就轻而易举了。流程如下：

1. 读取保存在本地的用户登录态 Cookies，获取 `p_skey` 键对应的值，基于它生成 `g_tk`，生成算法如下：

   ```ts
   const getGTk = (pSkey: string) =&gt; {
     let gTk = 5381;

     for (let i = 0; i &lt; pSkey.length; i += 1) {
       gTk += (gTk &lt;&lt; 5) + pSkey.charCodeAt(i);
     }

     return gTk &amp; 2147483647;
   };
   ```

2. 请求 `https://user.qzone.qq.com/proxy/domain/taotao.qq.com/cgi-bin/emotion_cgi_msglist_v6` 接口，将 `g_tk` 作为路径参数。此外由 `pos` 指定查询偏移量，`num` 指定查询条数。
3. 从响应里提取说说的具体信息，通过 `JSON.parse()` 方法转换为 JSON 格式做进一步处理。其格式定义简略表示如下：

   ```ts
   interface QZoneInfo {
     code: number;
     logininfo: {
       /** 用户名 */
       name: string;
       /** QQ 号 */
       uin: number;
     };
     /** 说说列表 */
     msglist: QZoneTalk[];
     /** 说说总数 */
     total: number;
   }

   interface QZoneTalk {
     /** 评论列表 */
     commentlist?: QZoneTalkComment[];
     /** 说说内容 */
     content: string;
     /** 创建时间(s) */
     created_time: number;
     /** 上次修改时间(s)。默认值为 0 */
     lastmodify: number;
     /** 定位信息 */
     lbs: {
       name: string;
       pos_x: string;
       pos_y: string;
     };
     name: string;
     /** 说说图片附件（仅有图片或同时包含图片和视频时，使用该字段） */
     pic?: QZoneTalkPic[];
     /** 是否私密 */
     secret: 0 | 1;
     /** 说说 ID */
     tid: string;
     uin: number;
     /** 说说视频附件（仅有视频时，使用该字段） */
     video?: QZoneTalkVideo[];
   }
   ```

4. 根据自身需要处理得到的说说结果，完成同步的需求。

## 说说视频播放优化

笔者心满意足地浏览着呈现在时间线项目里的 QQ 空间说说，兴趣盎然地点击了一个以前上传的视频，视频却在笔者焦虑地等待十多秒后才开始播放。这下子笔者知道又有新的问题要解决了 —— 怎么实现视频的边下载边播放？

求助于搜索引擎，得知 `.mp4` 格式的视频文件本来是可以边下载边播放的。如果不能边下载边播放，则说明描述它的**视频格式信息元数据**被放置到了视频文件的中间或末尾。一个简单且常用的处理方案是使用 [`qt-faststart`](https://github.com/danielgtaylor/qtfaststart) 工具，将这些元数据移动到视频文件的头部。

笔者在导出说说后，发现同时包含有 `.mp4` 和 `.m3u8` 两种格式的文件。其中 `.m3u8` 是播放列表文件，还需要把里面包含的视频片段文件下载到本地，不然无法正常播放。下面的代码片段供君参考：

```ts
if (videoFilename.endsWith(&quot;.m3u8&quot;)) {
  const videoSliceFilenameMatches = String(responseData).matchAll(
    new RegExp(`^${videoFilename.split(&quot;.&quot;)[0]}.+$`, &quot;gm&quot;),
  );
  const videoBaseUrl = path.dirname(videoUrl);

  Array.from(videoSliceFilenameMatches).forEach((videoSliceFilenameMatch) =&gt; {
    const sliceFilename = videoSliceFilenameMatch[0];
    const sliceDownloadUrl = `${videoBaseUrl}/${sliceFilename}`;
    const sliceSavePath = path.resolve(
      videoSaveDir,
      sliceFilename.split(&quot;?&quot;)[0],
    );

    axios
      .get(sliceDownloadUrl, {
        responseType: &quot;arraybuffer&quot;,
        maxContentLength: Infinity, // 避免文件过大异常跳出；在下载原始视频时也应当配置此项
        maxBodyLength: Infinity, // 同上
      })
      .then((getSliceResponse) =&gt; {
        fs.writeFileSync(sliceSavePath, getSliceResponse.data);
      });
  });
}
```

&gt; 与传统的视频格式不同，M3U8 视频格式将整个视频分成**多个小片段**进行传输，这些小片段可以根据网络情况自动调节其质量和大小。这种方式使得 M3U8 视频格式非常适合在网络环境不稳定或带宽不足的情况下播放视频。

经过复杂且折腾的思想斗争后，笔者决定使用 [`ffmpeg`](https://ffmpeg.org) 工具，将所有 `.mp4` 格式的视频文件转换为 `.m3u8` 格式，完成格式上的统一与边下载边播放的需求。

### 服务端转换视频文件为 `.m3u8` 格式

服务端安装使用 `ffmpeg` 所需的依赖：

```bash
yarn add fluent-ffmpeg ffmpeg-static
```

其中 `ffmpeg-static` 在安装时会自动下载一个编译好的 `ffmpeg` 二进制文件到本地，供 `fluent-ffmpeg` 使用：

```ts
import pathToFfmpeg from &quot;ffmpeg-static&quot;;
import Ffmpeg from &quot;fluent-ffmpeg&quot;;

Ffmpeg.setFfmpegPath(pathToFfmpeg);
```

将 `.mp4` 文件转换为 `.m3u8` 格式，可编写如下代码：

```ts
const convertVideoToM3u8 = (videoFilePath: string, outputFilePath: string) =&gt; {
  Ffmpeg(videoFilePath)
    .outputFormat(&quot;hls&quot;)
    .outputOptions([&quot;-hls_list_size 0&quot;, &quot;-hls_allow_cache 1&quot;])
    .output(outputFilePath)
    .run();
};
```

其中 `outputOptions()` 的完整参数列表定义可以[在这里](https://ffmpeg.org/ffmpeg-formats.html#hls-2)找到。`-hls_list_size` 配置保留的视频片段数量，此处的视频格式转换并非直播场景，因此需要设为 `0`；`-hls_allow_cache` 即是否允许客户端缓视频片段，设为允许；另外 `-hls_time` 可以配置每个视频片段的长度，默认值为 `2`（秒），此处保持默认。

转换后的结果如下图所示：

![m3u8-results](./sync-qzone-talks/m3u8-results.png)

特别的，如果机器的运行内存不足够批量处理多个视频文件，建议封装一个串行执行 Promise 任务的方法（最近一次面试遇到的题目，居然即刻在自己的项目里用上👍🏼），依次执行转换任务，避免内存溢出导致的程序异常跳出。可参考笔者的实现：

```ts
const createPromiseQueue = () =&gt; {
  const queue: (() =&gt; Promise&lt;void&gt;)[] = [];
  let isProcessing = false;

  const processQueue = async () =&gt; {
    if (isProcessing) return;
    isProcessing = true;

    while (queue.length &gt; 0) {
      const task = queue.shift();
      try {
        await task?.(); // 执行任务
      } catch (error) {
        console.error(&quot;Queue task failed:&quot;, error);
      }
    }

    isProcessing = false; // 处理完成
  };

  return (promiseFunction: () =&gt; Promise&lt;void&gt;) =&gt; {
    queue.push(promiseFunction);
    processQueue();
  };
};

const addToConvertQueue = createPromiseQueue();

const convertVideoToM3u8 = (videoFilePath: string, outputFilePath: string) =&gt; {
  addToConvertQueue(
    () =&gt;
      new Promise((resolve, reject) =&gt; {
        Ffmpeg(videoFilePath)
          .outputFormat(&quot;hls&quot;)
          .outputOptions([&quot;-hls_list_size 0&quot;, &quot;-hls_allow_cache 1&quot;])
          .output(outputFilePath)
          .on(&quot;end&quot;, () =&gt; {
            resolve();
          })
          .on(&quot;error&quot;, (error) =&gt; {
            reject(error);
          })
          .run();
      }),
  );
};
```

### 客户端播放 `.m3u8` 格式视频支持

浏览器自带的 `&lt;video&gt;` 标签并不原生支持播放 `.m3u8` 格式的视频，这里笔者引入了 [`video.js`](https://videojs.com/) 库实现播放功能。基于官方提供的[组件实现](https://videojs.com/guides/react/)，改巴改巴实现为自己的：

```tsx
import { useEffect, useRef } from &quot;react&quot;;
import videojs from &quot;video.js&quot;;
import Player from &quot;video.js/dist/types/player&quot;;

export interface VideoPlayerProps {
  id: string;
  options: {
    sources: { src: string; type?: string }[];
    controls?: boolean;
    poster?: string;
    preload?: &quot;auto&quot; | &quot;metadata&quot; | &quot;none&quot;;
    [key: string]: unknown;
  };
  className?: string;
  onReady?: (player: Player) =&gt; void;
}

/** https://videojs.com/guides/react/ */
export const VideoPlayer = (props: VideoPlayerProps) =&gt; {
  const { id, options, className = &quot;&quot;, onReady } = props;

  const videoRef = useRef&lt;HTMLDivElement&gt;(null);
  const playerRef = useRef&lt;Player&gt;(null);

  useEffect(() =&gt; {
    if (!videoRef.current) return;

    if (!playerRef.current) {
      const videoElement = document.createElement(&quot;video-js&quot;);

      videoElement.classList.add(&quot;vjs-default-skin&quot;, &quot;vjs-big-play-centered&quot;);
      videoElement.dataset.setup = &apos;{&quot;fluid&quot;: true}&apos;;
      videoRef.current.appendChild(videoElement);

      // @ts-expect-error: playerRef is writable
      const player = (playerRef.current = videojs(videoElement, options, () =&gt; {
        videojs.log(`Video player for ${id} is ready.`);
        onReady?.(player);
      }));
    }
  }, [id, onReady, options, videoRef]);

  // Dispose the Video.js player when the functional component unmounts
  useEffect(() =&gt; {
    const player = playerRef.current;

    return () =&gt; {
      if (player &amp;&amp; !player.isDisposed()) {
        player.dispose();
        // @ts-expect-error: playerRef is writable
        playerRef.current = null;
        videojs.log(`Video player for ${id} is disposed.`);
      }
    };
  }, [id, playerRef]);

  // 离开视野后自动暂停播放
  useEffect(() =&gt; {
    const video = videoRef.current;
    const player = playerRef.current;

    if (video &amp;&amp; player) {
      const pauseObserver = new IntersectionObserver(([entry]) =&gt; {
        if (!entry.isIntersecting) {
          player.pause();
        }
      });
      pauseObserver.observe(video);
      return () =&gt; {
        pauseObserver.unobserve(video);
      };
    }
  }, [videoRef, playerRef]);

  return &lt;div data-vjs-player ref={videoRef} className={className} /&gt;;
};

export default VideoPlayer;
```

一切就绪，再次点击视频，缓冲条如预期般一节节加载，实现了视频的边下载边播放，可喜可贺可喜可贺！

### 客户端构建体积（首屏速度）优化

如果您已经具备了一定的前端开发经验，就会对打包进项目的三方库非常敏感：打包未经（或无法）按需引入优化的三方库，意味着将三方库的全部代码塞到项目中，将导致项目的构建后体积大幅增长。在上一节中，笔者为了兼容 `.m3u8` 格式视频的播放，引入了广泛用于视频网站的 `video.js` 库，构建播放说说视频的组件。而它就是无法按需引入的三方库，打包后的代码体积因而大幅增长：

```plaintext
Route (app)                              Size     First Load JS
┌ ○ /                                    217 kB          338 kB
└ ○ /_not-found                          873 B          88.1 kB
+ First Load JS shared by all            87.3 kB
  ├ chunks/364-54e0b660da1a9f95.js       31.7 kB
  ├ chunks/618f8807-5ab9f851e4f8eeba.js  53.6 kB
  └ other shared chunks (total)          1.96 kB
```

并非每一个站长都会配置包含视频的时间线源（例如本文的 QQ 空间），首屏加载的时间线内容也并非一定包含视频内容，如果一股脑地将 `video.js` 打包在首屏 JS 代码内，势必无法带来最好的访问体验。

幸运的是，`next.js` 已经内置了动态引入组件的方法 `dynamic()`，如果要动态引入使用到 `video.js` 库的 `&lt;VideoPlayer&gt;` 组件，只需要编写如下代码：

```tsx
import dynamic from &quot;next/dynamic&quot;;
const VideoPlayer = dynamic(() =&gt; import(&quot;@/components/VideoPlayer&quot;));
```

如是优化后，首屏加载的 JS 体积自 `338 kB` 降至 `142 kB`。等到用户遇到包含视频的时间线内容时，才会加载与视频播放相关的 JS 资源，实现了对访问体验的优化。

```plaintext
Route (app)                              Size     First Load JS
┌ ○ /                                    20.4 kB         142 kB
└ ○ /_not-found                          873 B          88.2 kB
+ First Load JS shared by all            87.4 kB
  ├ chunks/364-54e0b660da1a9f95.js       31.7 kB
  ├ chunks/618f8807-5ab9f851e4f8eeba.js  53.6 kB
  └ other shared chunks (total)          2.06 kB
```
</content:original-text><content:updated-at>2024-10-17T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[纯前端如何实现一个转盘抽奖组件]]></title><description><![CDATA[为什么 前阵子面试的时候被问到这个问题，觉得挺有意思，于是决定亲手实现一个转盘抽奖组件试试。

翻看别人的实现方案时，发现和自己面试时答得相差很大，悲 😢。但总之，是时候开始弥补自己的 CSS 和动画技能了。

是什么

一个转盘抽奖组件主要由三部分组成，写有中奖结果的圆形转盘、指向结果的指针和开始转动的按钮。

如果每个中奖结果的概率相近，我们可以按照真实概率来划分每个奖品所占圆形的扇形比例…]]></description><link>https://blog.towind.fun/posts/lucky-draw</link><guid isPermaLink="false">lucky-draw</guid><category><![CDATA[前端开发]]></category><pubDate>Fri, 06 Sep 2024 00:00:00 GMT</pubDate><content:original-text>
## 为什么

前阵子面试的时候被问到这个问题，觉得挺有意思，于是决定亲手实现一个转盘抽奖组件试试。

翻看别人的实现方案时，发现和自己面试时答得相差很大，悲 😢。但总之，是时候开始弥补自己的 CSS 和动画技能了。

## 是什么

一个转盘抽奖组件主要由三部分组成，写有中奖结果的圆形转盘、指向结果的指针和开始转动的按钮。

如果每个中奖结果的概率相近，我们可以按照真实概率来划分每个奖品所占圆形的扇形比例。但是通常转盘中会设置抽中概率极小的大奖，按照真实比例的话将无法充分展示奖品内容，而且降低用户对转盘抽奖本身的兴趣度。所以本文实现的转盘组件选择 **均分的方式** 来划分每个奖品所占的扇形比例，符合通用的原则，也从视觉上让用户觉得中奖概率相当。

转盘转动可以有两种方式，一种是指针不动转盘动，一种是转盘不动指针动，都能很好地表达转盘抽奖的过程。对比两种方式，前者的视觉体验会更佳，且指针可以放置在任意位置（在中间往上指或在左侧往右指等等都可以）；后者则相对含蓄一些，用户的视觉负担较低，但指针只能放在中间旋转（我设想了一下在转盘外侧做圆周运动，感觉也有点意思）。因此，本文的实现选择 **指针不动转盘动** 的方式，从前端开发的角度来说，实现了一种方式，另一种方式也可以简单实现了。

当我们点击开始转动的按钮时，转盘便会开始转动。现实生活中的转盘通常由人手力驱动，转盘的 **转速会从零迅速加到最大，然后逐渐变小直至为零**。我们实现的转盘不受这些物理条件的限制，但对现实的充分模拟可以提升转盘抽奖的可信度，本文也将朝着这个方向实现。

当点击按钮时，前端即向后端请求了抽奖的结果，后续转盘的转动也不过是预设好的动画罢了。因此我们可以轻松地计算得到转盘应当旋转的角度，让指针恰恰好停留在奖品所在的扇形区域。这也意味着 **转盘旋转的角度可以是一个范围**，指针并不一定指向扇形区域的正中间，这也是对现实实际的一个模拟。

## 怎么做

在深入动画实现之前，让我们先完成 ~~简单的~~ 前置工作，把转盘抽奖组件的三个必要组成部分画出来。

首先是圆形转盘：

```html
&lt;div class=&quot;container&quot;&gt;
  &lt;div id=&quot;turntable&quot; class=&quot;turntable&quot;&gt;&lt;/div&gt;
&lt;/div&gt;
```

```scss
.container {
  width: 500px;
  padding: 40px;
  background: #f8fafc;
  display: flex;
  justify-content: center;
}

.turntable {
  position: relative;
  width: 400px;
  height: 400px;
  border-radius: 50%;
}
```

```ts
interface Prize {
  label: string;
  probability: number;
  bgColor: string;
}

const prizes: Prize[] = [
  { label: &quot;超级大奖&quot;, probability: 0.001, bgColor: &quot;#b91c1c&quot; },
  { label: &quot;特等奖&quot;, probability: 0.009, bgColor: &quot;#c2410c&quot; },
  { label: &quot;一等奖&quot;, probability: 0.01, bgColor: &quot;#7e22ce&quot; },
  { label: &quot;二等奖&quot;, probability: 0.03, bgColor: &quot;#2563eb&quot; },
  { label: &quot;三等奖&quot;, probability: 0.15, bgColor: &quot;#15803d&quot; },
  { label: &quot;安慰奖&quot;, probability: 0.3, bgColor: &quot;#1e293b&quot; },
  { label: &quot;谢谢参与&quot;, probability: 0.5, bgColor: &quot;#3f3f46&quot; },
];

const turntableDom = document.getElementById(&quot;turntable&quot;);
const proportionPerPrize = Number((100 / prizes.length).toFixed(1));

// #region 划分转盘扇形区域
const turntableConicGradient = prizes.map((prize, index) =&gt; {
  const from = (proportionPerPrize * index).toFixed(1);
  const to =
    index === prizes.length - 1
      ? 100
      : (proportionPerPrize * (index + 1)).toFixed(1);
  return `${prize.bgColor} ${from}% ${to}%`;
});
turntableDom.style.background = `conic-gradient(${turntableConicGradient.join(
  &quot;,&quot;,
)})`;
// #endregion
```

利用 CSS 函数 `conic-gradient()` 创建颜色渐变，巧妙地实现转盘扇形区域的均分。得到的转盘如下所示：

![turntable-base](./lucky-draw/turntable-base.png)

接着为每个扇形区域添加具体的奖品名称：

```scss
.prize-label {
  position: absolute;
  width: 100%;
  height: 100%;
  display: flex;
  justify-content: center;
  align-items: baseline;
  font-weight: bold;
  color: white;
  // #region 调整奖品名的位置
  line-height: 100px;
  // #endregion
}
```

```ts
const anglePerPrize = Number((360 / prizes.length).toFixed(1));

// #region 添加奖品名节点
const prizeLabels = document.createDocumentFragment();
prizes.map((prize, index) =&gt; {
  const prizeLabel = document.createElement(&quot;div&quot;);
  prizeLabel.classList.add(&quot;prize-label&quot;);
  prizeLabel.style.transform = `rotate(${
    -anglePerPrize / 2 + anglePerPrize * (index + 1)
  }deg)`;
  prizeLabel.innerText = prize.label;
  prizeLabels.appendChild(prizeLabel);
});
turntableDom.append(prizeLabels);
// #endregion

// #region 设置转盘初始角度
const turntableBaseRotate = -anglePerPrize / 2;
turntableDom.style.transform = `rotate(${turntableBaseRotate}deg)`;
// #endregion
```

我们手动创建了包含奖品名的标签节点，通过简单的计算，让它们旋转到自己所属的扇形区域上。此外，我们还为转盘设置了一个初始角度，使得第一个奖品呈现在转盘的正上方。效果如下所示：

![turntable-with-prize-labels](./lucky-draw/turntable-with-prize-labels.png)

&gt; 更精致、美观的转盘设计还是建议直接使用做好的转盘图片；纯前端实现的话一方面耗时耗力，另一方面如果请求过多的装饰图片反而会降低访问性能。
&gt;
&gt; ```html
&gt; &lt;img src=&quot;path/to/turntable.png&quot; alt=&quot;turntable&quot; id=&quot;turntable&quot; class=&quot;turntable&quot;&gt;&lt;/img&gt;
&gt; ```

接下来添加指针和抽奖按钮：

```html
&lt;div class=&quot;container&quot;&gt;
  &lt;div id=&quot;turntable&quot; class=&quot;turntable&quot;&gt;&lt;/div&gt;
  &lt;div class=&quot;arrow&quot;&gt;&lt;/div&gt;
  &lt;div id=&quot;lottery-btn&quot; class=&quot;lottery-btn&quot;&gt;抽奖&lt;/div&gt;
&lt;/div&gt;
```

```scss
.arrow {
  position: absolute;
  top: 140px;
  width: 0;
  height: 0;
  border-left: 15px solid transparent;
  border-right: 15px solid transparent;
  border-bottom: 100px solid #f8fafc;
}

.lottery-btn {
  position: absolute;
  top: 200px;
  width: 80px;
  height: 80px;
  border-radius: 50%;
  background: #f8fafc;
  display: flex;
  justify-content: center;
  align-items: center;
  color: #333;
  font-weight: bold;
  cursor: pointer;
  user-select: none;

  &amp;--disabled {
    color: #999;
    pointer-events: none;
  }
}
```

使用绝对定位将指针和按钮放到合适的位置，转盘现在看上去有模有样了：

![turntable-with-all](./lucky-draw/turntable-with-all.png)

最重要的环节到了，实现转盘抽奖的核心需求：转！

首先实现一个支持生成小数位后 `precision` 位精度的随机数的方法，它将被用于模拟抽奖结果（当然这应当由后端完成），以及计算转盘应当旋转的角度：

```ts
const getRandomNumber = (min: number, max: number, precision: number) =&gt; {
  const factor = Math.pow(10, precision);
  const random = Math.random() * (max - min) + min;
  return Math.round(random * factor) / factor;
};
```

基于刚刚实现的 `getRandomNumber()` 方法，完成我们的抽奖事件：

```ts
const animationDuration = 5000;
const rotateLapsBase = 10;
let rotateLaps = 0;

const lotteryBtnDom = document.getElementById(&quot;lottery-btn&quot;);
lotteryBtnDom.onclick = () =&gt; {
  lotteryBtnDom.classList.add(&quot;lottery-btn--disabled&quot;);

  // #region 模拟抽奖结果，由后端完成
  let prizeIndex = 0;
  let resultNum = getRandomNumber(0, 1, 3); // 本文预设奖品的中奖概率精确到小数点后 3 位，因此这里的 `precision` 设为 3
  while (resultNum &gt; 0) {
    resultNum -= prizes[prizeIndex].probability;
    if (resultNum &gt; 0) {
      prizeIndex += 1;
    }
  }
  const prize = prizes[prizeIndex];
  // #endregion

  // #region 获取转盘需要旋转的角度
  const turntableRotateDegFrom = Number(
    (anglePerPrize * prizeIndex).toFixed(1),
  );
  const turntableRotateDegTo =
    prizeIndex === prizes.length - 1
      ? 360
      : Number((anglePerPrize * (prizeIndex + 1)).toFixed(1));
  const turntableRotateDegEdgeThreshold = Number(
    (anglePerPrize / 4).toFixed(1),
  ); // 设定旋转到指定扇形区域的距边缘阈值（&lt;= anglePerPrize / 2），防止出现指针指向太靠近边缘的情况
  const turntableRotateDegBase = getRandomNumber(
    turntableRotateDegFrom + turntableRotateDegEdgeThreshold,
    turntableRotateDegTo - turntableRotateDegEdgeThreshold,
    1,
  );
  rotateLaps += rotateLapsBase; // 适配多次旋转的情况
  const turntableRotateDeg = -(rotateLaps * 360 + turntableRotateDegBase);
  // #endregion

  // #region 为转盘设置旋转动画
  turntableDom.style.transform = `rotate(${turntableRotateDeg}deg)`;
  turntableDom.style.transition = `transform ${animationDuration}ms ease-out`;
  // #endregion

  setTimeout(() =&gt; {
    alert(`您抽中了：${prize.label}！`);
    lotteryBtnDom.classList.remove(&quot;lottery-btn--disabled&quot;);
  }, animationDuration + 500);
};
```

计算实际中奖结果所在的扇形区域角度 `turntableRotateDegFrom` 至 `turntableRotateDegTo`，分别加上和减去距离边缘的阈值 `turntableRotateDegEdgeThreshold`，将得到的范围取随机数，即可得到我们最后转盘应旋转的角度 `turntableRotateDegBase`。将应旋转的角度再加上设定好的旋转圈数所对应的角度，取反即可得到动画效果中实际旋转的角度 `turntableRotateDeg`。

点击抽奖按钮，看看现在的效果吧：

![lucky-draw-base-animation](./lucky-draw/lucky-draw-base-animation.gif)

Ops，竟然是谢谢参与，我觉得有黑幕！但是可喜可贺，我们已经基本实现了转盘抽奖组件所需的全部能力。

下面，为了使转盘旋转的效果更贴近生活实际，我们可以为旋转动画设置更符合物理直觉的过渡效果。

刚刚为 `transform` 设置的动画过渡效果 `transition-timing-function: ease-out;` 相当于 `transition-timing-function: cubic-bezier(0, 0, .58, 1);`，表示动画一开始较快，随着动画的进行逐渐减速。

但是转盘从转动减速至停止的过渡时间太短，从视觉上看尤为突兀，要是稍长一些就好了。

通过 [cubic-bezier](https://cubic-bezier.com/)，我们以可视化的方式实现一个满足需要的三次贝塞尔曲线，作为动画的过渡效果。如：`transition-timing-function: cubic-bezier(.3, .9, .38, 1);`，它减速至停止的 **过渡时间更长且更柔和**，效果如下：

![lucky-draw-better-animation](./lucky-draw/lucky-draw-better-animation.gif)

完整的 Demo 奉上：

&lt;iframe
  height=&quot;600&quot;
  scrolling=&quot;no&quot;
  title=&quot;lucky-draw-demo&quot;
  src=&quot;https://codepen.io/LolipopJ/embed/QWXzwWp?default-tab=html%2Cresult&quot;
  frameBorder=&quot;no&quot;
  loading=&quot;lazy&quot;
  allowtransparency=&quot;true&quot;
  allowFullScreen={true}
&gt;
  See the Pen{&quot; &quot;}
  &lt;a href=&quot;https://codepen.io/LolipopJ/pen/QWXzwWp&quot;&gt;lucky-draw-demo&lt;/a&gt; by
  JasonSung (&lt;a href=&quot;https://codepen.io/LolipopJ&quot;&gt;@LolipopJ&lt;/a&gt;) on{&quot; &quot;}
  &lt;a href=&quot;https://codepen.io&quot;&gt;CodePen&lt;/a&gt;.
&lt;/iframe&gt;
</content:original-text><content:updated-at>2024-09-07T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[Electron 执行后台程序并在渲染器实时打印运行日志]]></title><description><![CDATA[开发图像查重工具时遇到了这样一个问题：在查重之前，用户需要先对图像文件进行索引操作，后台将调用可执行文件并为每张图像生成特征值。索引操作所需的时间与图像的数量及大小呈正相关，笔者为大约 50000 张图片（约 170GB）生成特征值，需要花费将近 90 分钟的时间。在这种情况下，如果渲染器什么也不展示，卡在那里，用户难免会非常焦虑 —— 后台是否还在运行，我是不是卡死了？ 那么需求也就明了了…]]></description><link>https://blog.towind.fun/posts/electron-real-time-print-execution-log</link><guid isPermaLink="false">electron-real-time-print-execution-log</guid><category><![CDATA[前端开发]]></category><pubDate>Mon, 05 Aug 2024 00:00:00 GMT</pubDate><content:original-text>
开发图像查重工具时遇到了这样一个问题：在查重之前，用户需要先对图像文件进行索引操作，后台将调用可执行文件并为每张图像生成特征值。索引操作所需的时间与图像的数量及大小呈正相关，笔者为大约 50000 张图片（约 170GB）生成特征值，需要花费将近 90 分钟的时间。在这种情况下，如果渲染器什么也不展示，卡在那里，用户难免会非常焦虑 —— 后台是否还在运行，我是不是卡死了？

那么需求也就明了了，正如本文的标题所述，我们需要**将后台运行的日志实时推送到渲染器**，这样用户便能看到索引操作的进度，安下心来。

## 技术背景

众所周知，一个 Electron 应用分为了 Renderer 渲染器和 Main 主进程两端。渲染器负责对客侧的展示，正如我们访问的所有网页一样，是 HTML、CSS、JavaScript 的集合，无法调用 Node 或是访问宿主机文件等。而主进程则具备有服务端应用的性质，能够调用 Node 或是与宿主机交互等。

综上所述，为了实现我们的目标，在背后依次要实现这些事情：

1. 渲染器接收用户索引操作的请求，将请求发送至主进程。
2. 主进程接收到请求，调用可执行文件开始生成图像特征值。
3. 主进程将产生的日志信息实时推送给渲染器。
4. 渲染器接收到日志信息，并向用户展示。

## 需求实现

根据刚才的分析，对[进程间通信（Inter-Process Communication，IPC）](https://www.electronjs.org/zh/docs/latest/tutorial/ipc)能力的使用将会是实现需求的关键。

实现的具体方案遵循 Electron 推荐的安全设置即上下文隔离。下面的内容假设您对[预加载器](https://www.electronjs.org/zh/docs/latest/tutorial/process-model#preload-%E8%84%9A%E6%9C%AC)有一定的了解。

### 渲染器将请求发送至主进程

渲染器发送请求至主进程是**渲染器到主进程的单向通信**，具体的实现分成三个步骤：

1. 主进程通过 `ipcMain.on()` 监听请求。

   ```ts
   // main/background.ts
   import { ipcMain } from &quot;electron&quot;;

   export enum Events {
     UPDATE_INDEX = &quot;events:updateIndex&quot;,
   }

   ipcMain.on(Events.UPDATE_INDEX, (_, args) =&gt; {
     // todo: execute binary
   });
   ```

2. 预加载器向渲染器暴露 `ipcRenderer.send()` 方法。

   ```ts
   // main/preload.ts
   import { ipcRenderer, contextBridge } from &quot;electron&quot;;

   const ipc = {
     send: (channel: string, ...args: unknown[]) =&gt; {
       ipcRenderer.send(channel, ...args);
     },
     // 由于 Electron 的安全机制，您不能直接暴露 `ipcRenderer` 以及上面的方法
     // 错误的例子：
     // send: ipcRenderer.send,
   };

   contextBridge.exposeInMainWorld(&quot;ipc&quot;, ipc);

   export type IPC = typeof ipc;
   ```

   让 TypeScript 更好地为您工作，别忘了将类型 `IPC` 暴露给 `Window` 对象：

   ```ts
   // renderer/preload.d.ts
   import type { IPC } from &quot;path/to/main/preload&quot;;

   declare global {
     interface Window {
       ipc: IPC;
     }
   }
   ```

3. 渲染器实现调用预加载器暴露的方法。

   ```tsx
   // renderer/path/to/component-trigger.tsx
   import { Events } from &quot;path/to/main/background&quot;;

   export default () =&gt; {
     const onUpdateIndex = () =&gt; {
       window.ipc.send(Events.UPDATE_INDEX);
     };

     return &lt;&gt;{/* component details */}&lt;/&gt;;
   };
   ```

   再在合适的地方编写触发逻辑，即可将请求发送至主进程。

### 主进程调用可执行文件

接着，让我们来完善主进程的逻辑：在接收到请求后，去调用本地的可执行文件。

在 Node 环境中，我们可以找老朋友 `child_process` 帮忙。`child_process.exec()` 会等待执行结束后将结果一并返回，不满足我们的需要；`child_process.spawn()` 采用事件监听机制，可以应对实时输出日志的情景，满足我们的需要。

基于 `child_process.spawn()` 编写代码如下：

```ts
// main/background.ts
import { spawn } from &quot;child_process&quot;;

const runSpawn = (cmd: string, args: string[]) =&gt; {
  const process = spawn(cmd, args);

  process.stdout.on(&quot;data&quot;, (data) =&gt; {
    // todo: on receive stdout data
  });

  process.stderr.on(&quot;data&quot;, (data) =&gt; {
    // todo: on receive stderr data
  });

  process.on(&quot;close&quot;, (code) =&gt; {
    // todo: on receive close signal
  });
};

ipcMain.on(Events.UPDATE_INDEX, (_, args) =&gt; {
  runSpawn(&quot;path/to/binary&quot;, [&quot;--update-index&quot;, &quot;--rest-args&quot;]);
});
```

### 主进程实时推送日志信息给渲染器

当事件监听器触发时，向渲染器发送日志信息，这是**主进程到渲染器的单向通信**，具体的实现同样分成三个步骤：

1. 主进程通过 `browserWindow.webContents.send()` 发送信息。

   完善前面的 `runSpawn()` 方法：

   ```ts
   // main/background.ts
   import iconv from &quot;iconv-lite&quot;;

   export enum SpawnEvents {
     SPAWN_STARTED = &quot;spawn:started&quot;,
     SPAWN_STDOUT = &quot;spawn:stdout&quot;,
     SPAWN_STDERR = &quot;spawn:stderr&quot;,
     SPAWN_FINISHED = &quot;spawn:finished&quot;,
   }

   // Compatible with default command line encoding `cp936` on Windows platform
   const iconvDecoding = process.platform === &quot;win32&quot; ? &quot;cp936&quot; : &quot;utf-8&quot;;

   const runSpawn = (cmd: string, args: string[]) =&gt; {
     const process = spawn(cmd, args);
     browserWindow.webContents.send(SpawnEvents.SPAWN_STARTED);

     process.stdout.on(&quot;data&quot;, (data) =&gt; {
       browserWindow.webContents.send(
         SpawnEvents.SPAWN_STDOUT,
         iconv.decode(Buffer.from(data, &quot;binary&quot;), iconvDecoding),
       );
     });

     process.stderr.on(&quot;data&quot;, (data) =&gt; {
       browserWindow.webContents.send(
         SpawnEvents.SPAWN_STDERR,
         iconv.decode(Buffer.from(data, &quot;binary&quot;), iconvDecoding),
       );
     });

     process.on(&quot;close&quot;, (code) =&gt; {
       browserWindow.webContents.send(SpawnEvents.SPAWN_FINISHED, code ?? 0);
     });
   };
   ```

   特别的，在 Windows 端，由于命令行工具默认采用 `cp936` 编码，在输出中文时会出现乱码的现象。因此，在上面的实现中，笔者使用了 `iconv-lite` 对标准输出、标准错误进行了重新解码。

2. 预加载器向渲染器暴露 `ipcRenderer.on()` 方法。

   ```ts
   // main/preload.ts
   import { type IpcRendererEvent } from &quot;electron&quot;;

   const ipc = {
     on: (channel: string, func: (...args: unknown[]) =&gt; void) =&gt; {
       const subscription = (_event: IpcRendererEvent, ...args: unknown[]) =&gt;
         func(...args);
       ipcRenderer.on(channel, subscription);

       return () =&gt; {
         ipcRenderer.removeListener(channel, subscription);
       };
     },
   };
   ```

   其返回值是清除监听器的方法，可以配合 `React.useEffect()` 使用。

3. 渲染器实现调用预加载器暴露的方法。

   ```tsx
   // renderer/path/to/component-listener.tsx
   import { useEffect, useState } from &quot;react&quot;;
   import { SpawnEvents } from &quot;path/to/main/background&quot;;

   export default () =&gt; {
     const [loading, setLoading] = useState&lt;boolean&gt;(false);
     const [stdout, setStdout] = useState&lt;string&gt;(&quot;&quot;);
     const [stderr, setStderr] = useState&lt;string&gt;(&quot;&quot;);

     useEffect(() =&gt; {
       const cleanupSpawnStarted = window.ipc.on(
         SpawnEvents.SPAWN_STARTED,
         () =&gt; {
           setLoading(true);
         },
       );
       const cleanupSpawnStdout = window.ipc.on(
         SpawnEvents.SPAWN_STDOUT,
         (data: string) =&gt; {
           setStdout(data);
         },
       );
       const cleanupSpawnStderr = window.ipc.on(
         SpawnEvents.SPAWN_STDERR,
         (data: string) =&gt; {
           // setStderr(data);
           setStderr((prev) =&gt; {
             return (data + &quot;\n&quot; + prev).substring(0, 2000);
           });
         },
       );
       const cleanupSpawnFinished = window.ipc.on(
         SpawnEvents.SPAWN_FINISHED,
         (code: number) =&gt; {
           setLoading(false);
         },
       );

       return () =&gt; {
         cleanupSpawnStarted();
         cleanupSpawnStdout();
         cleanupSpawnStderr();
         cleanupSpawnFinished();
       };
     }, []);

     return &lt;&gt;{/* component details */}&lt;/&gt;;
   };
   ```

   一般来说，可执行文件会将日志信息重定向至 `stderr` 标准错误，运行的最终结果重定向至 `stdout` 标准输出。在本文中，我们需要展示的是 `stderr` 的内容。

   如果渲染器还需要对 `stdout` 的结果进行下一步处理，同样可以在对应的组件中添加监听器：`window.ipc.on(SpawnEvents.SPAWN_STDOUT, (data: string) =&gt; {})`。

### 渲染器展示接收到的日志信息

现在，所有的链路都已经打通，查收编写代码努力的结晶吧！

![实时展示日志信息](./electron-real-time-print-execution-log/real-time-execution-log.gif)
</content:original-text><content:updated-at>2024-08-05T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[部署一个给朋友使用的 Minecraft 模组服务器]]></title><description><![CDATA[笔者在今年五月份部署了一个与朋友同玩共乐的 Minecraft 服务器，稳定运行至今。忽然想记录为一篇博客，分享分享折腾的经历。 笔者结合个人喜好（最新版本，模组优先）和大众推荐（Fabric 更适合新版本 Minecraft），决定基于 Fabric 搭建一个可以添加模组的 Minecraft 服务器。此类服务器简称为模组服务器，还有基于 Paper, Spigot 等搭建的插件服务器…]]></description><link>https://blog.towind.fun/posts/run-mc-server</link><guid isPermaLink="false">run-mc-server</guid><category><![CDATA[技术琐事]]></category><pubDate>Fri, 05 Jul 2024 00:00:00 GMT</pubDate><content:original-text>
笔者在今年五月份部署了一个与朋友同玩共乐的 Minecraft 服务器，稳定运行至今。忽然想记录为一篇博客，分享分享折腾的经历。

笔者结合个人喜好（最新版本，模组优先）和大众推荐（Fabric 更适合新版本 Minecraft），决定基于 Fabric 搭建一个可以添加模组的 Minecraft 服务器。此类服务器简称为模组服务器，还有基于 [Paper](https://github.com/PaperMC/Paper), [Spigot](https://www.spigotmc.org) 等搭建的插件服务器，可以综合自身需求，选择最合适的搭建方案。

## 部署 Minecraft 服务器

### 安装 Java 环境

Java 版的 Minecraft 服务器依赖于 Java 启动，因此在一切的最开始，需要在服务器上安装 Java 环境。

Java 版的 Minecraft 自 1.18 版本开始需要 Java &gt;= 17。参考网上俯拾皆是的教程，通过包管理工具或前往 [Oracle OpenJDK](https://jdk.java.net/) 页面下载并安装合适版本的 Java。

输入 `java -version` 命令查看是否安装成功，在笔者的服务器上打印的结果如下：

```bash
$ java -version
openjdk version &quot;21.0.2&quot; 2024-01-16
OpenJDK Runtime Environment (build 21.0.2+13-alpine-r0)
OpenJDK 64-Bit Server VM (build 21.0.2+13-alpine-r0, mixed mode, sharing)
```

### 下载 Minecraft 服务器启动器

前往 Fabric 提供的[下载 Minecraft 服务器启动器页面](https://fabricmc.net/use/server)，选择欲部署的 Minecraft 服务器版本，Fabric 加载器和 Fabric 模组版本：

![下载 Minecraft 服务器启动器](./run-mc-server/download-fabric-mc-server-launcher.png)

在服务器上通过 `curl` 命令下载。如上图所示，可以执行命令：

```bash
curl -OJ https://meta.fabricmc.net/v2/versions/loader/1.21/0.15.11/1.0.1/server/jar
```

得到 Java 可执行的归档文件 `fabric-server-mc.1.21-loader.0.15.11-launcher.1.0.1.jar`。

### 启动服务器

新建一个存放 Minecraft 服务器文件的目录，移动到此目录，通过 `java` 命令执行刚刚得到的文件：

```bash
java -Xmx2G -jar fabric-server-mc.1.21-loader.0.15.11-launcher.1.0.1.jar nogui
```

将自动在**当前所在的目录**生成必要的数据文件。首次启动将会以失败告终，此时需要手动编辑目录下的 `eula.txt`，同意 Minecraft 的最终用户许可协议：

```txt
eula=true
```

再次启动就一切 OK 了。

&gt; Minecraft 服务器默认监听 25565 端口，请确保防火墙放行了该端口的 TCP 类型请求。

测试一切正常运行，可以通过服务器公网 IP 连接到服务器后，使用 `screen` 命令管理 Minecraft 服务器进程：

```bash
screen -dmS mc-server java -Xmx2G -jar fabric-server-mc.1.21-loader.0.15.11-launcher.1.0.1.jar nogui
```

这样，我们就建立了一个名为 `mc-server` 的 Screen 终端，需要查看运行日志时使用 `screen -r mc-server` 进入。

![查看 Minecraft 服务端日志](./run-mc-server/server-log.png)

是的，这就是全部。接下来就是发挥创造力的环节了，正如我们在 Minecraft 中一直做的那样。

## 让朋友们连接到 Minecraft 服务器

笔者推荐使用开源作者[龙腾猫跃](https://github.com/LTCatt)开发的 [Plain Craft Launcher](https://github.com/Hex-Dragon/PCL2)（[爱发电](https://afdian.net/p/0164034c016c11ebafcb52540025c377)）来下载与启动 Minecraft 客户端。

下载时需选择与服务端相同的 Fabric Loader 和 Fabric API 版本：

![通过 PCL 下载 Minecraft 客户端](./run-mc-server/download-mc-client.png)

使用正版账号或离线账号启动客户端：

![通过 PCL 启动 Minecraft 客户端](./run-mc-server/start-mc-client.png)

## 添加与管理模组

我们可以向 Minecraft 服务器添加各种各样的 Fabric 模组。

模组存放于服务器根目录的 `mods/` 目录下，服务器初始化后会默认包含一个 Fabric 依赖的必要模组 `mods/fabric-api-1.0.1+1.21.jar`。我们也只需要像这样，把 `.jar` 格式的模组文件放置到 `mods/` 目录下，就算成功添加模组了。

您可以在 [Modrinth](https://modrinth.com/mods) 和 [CurseForge](https://www.curseforge.com/minecraft/search?class=mc-mods) 探索和下载想要使用的模组。

下面举几个笔者使用到的模组的例子。

### AutoModpack / 自动分发客户端模组

按模组的使用环境，可以划分为以下四类：

- 客户端模组：安装在客户端。服务端安装无效。
- 服务端模组：安装在服务端。客户端安装无效。
- 客户端或服务端模组：通常安装在服务端，在客户端同时安装可以增强体验。
- 客户端和服务端模组：在客户端和服务端同时安装才有效。

为了跟朋友们都有相似的游戏体验（也顺便照顾不大懂模组安装的朋友），考虑去实现服务端与客户端的模组同步能力。笔者找到了模组 [AutoModpack](https://modrinth.com/mod/automodpack) 来实现此功能。

AutoModpack 是一个**客户端和服务端模组**，安装了此模组的客户端会自动同步服务端上托管的模组到本地，保证不同客户端体验的一致性。因此对于一起玩的朋友，只需要让他们在客户端的 `mods/` 目录下放入此模组，并在首次进入服务器时确认同步客户端模组就可以了。

![同步客户端模组](./run-mc-server/sync-client-mods.png)

对于不应当被同步到客户端去的模组，AutoModpack 也提供了简单的配置方式：以 `server-` 开头重命名文件。例如对于只需要安装在服务端的模组 [Dynmap](https://modrinth.com/plugin/dynmap)，将模组文件 `mods/Dynmap-3.7-beta-6-fabric-1.21.jar` 重命名为 `mods/server-Dynmap-3.7-beta-6-fabric-1.21.jar` 即可。

### Dynmap / 服务器地图

![服务器地图，摄制于 2024.07.05 17:18](./run-mc-server/server-map-202407051718.png)

想要用浏览器在线查看 Minecraft 服务器地图，查看当前在线玩家的游玩情况，直接向在线的玩家发送消息？这一切仅需要一个服务端模组 [Dynmap](https://modrinth.com/plugin/dynmap) 就可以实现！

成功启用 Dynmap 后，Web 服务将默认监听本机的 8123 端口，请确保防火墙放行了该端口的 TCP 类型请求。

起始时通过浏览器查看会是一片黑暗，伴随玩家在世界里的探索，Dynmap 会逐渐绘制成形完整的地图。

### Fabric Tailor / 离线服务器自定义换肤

对于离线服务器，哪怕以正版用户的身份登录，也无法获取到自己上传的皮肤；由于技术问题，部分 Minecraft 启动器也无法修改高版本客户端的玩家皮肤。大家登入服务器发现彼此都是 N-word，实在不能带来愉快的视觉体验。

服务端模组 [Fabric Tailor](https://modrinth.com/mod/fabrictailor) 可以解决这个问题，它向游戏添加了 `/skin` 命令，玩家可以调用此命令实现自定义换肤。例如：

```bash
# 设置为指定 URL 链接对应的皮肤
/skin set URL classic https://s.namemc.com/i/b80558ff4b834410.png # 经典身材
/skin set URL slim https://s.namemc.com/i/b80558ff4b834410.png # 纤细身材

# 设置为指定正版用户上传的皮肤
/skin set player Lolipop0703
```

## 服务器运维

### 配置服务器

参考[此文档](https://minecraft.fandom.com/wiki/Server.properties)按需修改服务器根目录下的 `server.properties` 配置文件。

其中值得一提的有：

- `motd`：服务器描述信息，在服务器列表展示。可以通过这个[小工具](https://minecraft.tools/en/motd.php?motd)生成与预览。
- `online-mode`：如果你有并没有购买正版 Minecraft 的朋友，可设置为 `false` 来允许他们进入服务器。
- `spawn-protection`：出生点方块保护，默认值为 `16`，保护以出生点为中心的 `2 * 16 + 1 = 33` 块方格。可以设置小一点，避免朋友们因破坏不了方块导致的困惑。当然，如果没有设置管理员，这个选项会自动被禁用。

此外，将自定义的服务器图标保存到服务器根目录，命名为 `server-icon.png`，即可在服务器列表展示。

### 服务器状态卡片

&lt;img
  alt=&quot;Minecraft 服务器状态&quot;
  src=&quot;https://mcapi.us/server/image?ip=mc.towind.fun&quot;
/&gt;

一些开放服务可以用来查询 Minecraft 服务器状态，并生成类似上面这样的卡片。想要生成同样的卡片？访问 [MCApi.us](https://mcapi.us) 了解更多。

### 通过域名连接到服务器

假设您已拥有一个域名并通过 DNS 服务商解析到了服务器，那么连接服务器时不再需要填写服务器的公网 IP 地址，使用域名就行。

笔者的服务器就可以通过域名 `mc.towind.fun` 连接，Minecraft 客户端将查询到域名背后的 IP 地址，访问其 25565 端口。

假设您还安装和配置好了 Nginx 服务，我们可以将服务器地图进行转发。例如：

```conf
server {
  listen 443 ssl;
  ssl_certificate /path/to/fullchain.pem;
  ssl_certificate_key /path/to/privkey.pem;

  server_name mc.towind.fun;

  location /map/ {
    proxy_pass http://127.0.0.1:8123/;
  }
}
```

现在，我们不再需要放行 8123 端口，服务器地图通过 Nginx 监听的 443 端口转发，访问链接 https://mc.towind.fun/map 即可。

### 服务器备份

时常 Minecraft 备份服务器数据并上传到云盘是一个非常好的习惯，笔者在&lt;Link to=&quot;/posts/backup-mc-server&quot;&gt;另一篇博客&lt;/Link&gt;里实现了数据的定时备份与上传，供君参考。
</content:original-text><content:updated-at>2024-07-13T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[基于 SteamCMD 部署一个给朋友使用的饥荒联机版服务器]]></title><description><![CDATA[本文重现了笔者在自己的 CentOS 7 (64-bit) 系统中部署饥荒联机版服务器的全过程，供君参考。 Steam 版的饥荒联机版与 Wegame 版数据不互通，也无法相互联机。

笔者主要参考了如下两个部署教程：

Guides/Don’t Starve Together Dedicated Servers How to setup dedicated server with cave on…]]></description><link>https://blog.towind.fun/posts/run-dont-starve-server</link><guid isPermaLink="false">run-dont-starve-server</guid><category><![CDATA[技术琐事]]></category><pubDate>Tue, 02 Jul 2024 00:00:00 GMT</pubDate><content:original-text>
本文重现了笔者在自己的 CentOS 7 (64-bit) 系统中部署饥荒联机版服务器的全过程，供君参考。

Steam 版的饥荒联机版与 Wegame 版数据不互通，也无法相互联机。

笔者主要参考了如下两个部署教程：

- [Guides/Don’t Starve Together Dedicated Servers](https://dontstarve.fandom.com/wiki/Guides/Don%E2%80%99t_Starve_Together_Dedicated_Servers)
- [How to setup dedicated server with cave on Linux](https://steamcommunity.com/sharedfiles/filedetails/?id=590565473)

## 安装 `steamcmd`

安装的步骤可以直接参考 SteamCMD 的[官方文档](https://developer.valvesoftware.com/wiki/SteamCMD)（[中文版](https://developer.valvesoftware.com/w/index.php?title=SteamCMD:zh-cn&amp;uselang=zh)）。

需要注意的是，SteamCMD 仅提供 32 位二进制文件。如果您的系统仅能运行 64 位进程（例如 OpenWRT 系统），将**不能**正常使用。

为了安全考虑，`steamcmd` 官方建议创建新的系统用户运行，而不是拥有最高命令权限的 `root` 用户。因此，首先创建一个名为 `steam` 的用户：

```bash
# 创建 steam 用户
sudo useradd -m steam
# 修改 steam 用户登录密码
sudo passwd steam
```

对于 64 位系统，需要安装如下的必要依赖以运行 `steamcmd`：

```bash
yum -y install glibc.i686 libstdc++.i686
```

切换至 `steam` 用户：

```bash
su - steam
```

安装 `steamcmd`：

```bash
cd ~
curl -sqL &quot;https://steamcdn-a.akamaihd.net/client/installer/steamcmd_linux.tar.gz&quot; | tar zxvf -
```

运行 `steamcmd`：

```bash
/home/steam/steamcmd.sh
```

耐心等待更新文件下载、安装完毕……

## 安装饥荒联机版

安装饥荒联机版运行的必要依赖：

```bash
yum -y install libcurl.i686
ln -s /usr/lib/libcurl.so.4 /usr/lib/libcurl-gnutls.so.4
```

设置饥荒联机版安装目录：

```bash
Steam&gt; force_install_dir /home/steam/steamapps/DST
```

匿名登录到 `steamcmd`：

```bash
Steam&gt; login anonymous
```

安装饥荒联机版：

```bash
Steam&gt; app_update 343050 validate
```

测试饥荒联机版的运行。特别的，由于相对路径的缘故，执行 `dontstarve_dedicated_server_nullrenderer` 前一定要先移动到它所在的目录：

```bash
cd /home/steam/steamapps/DST/bin
./dontstarve_dedicated_server_nullrenderer
```

如果没有其它的报错，那么打印内容大致如下：

```plaintext
$ ./dontstarve_dedicated_server_nullrenderer
./dontstarve_dedicated_server_nullrenderer: /lib/libcurl-gnutls.so.4: no version information available (required by ./dontstarve_dedicated_server_nullrenderer)
[00:00:00]: PersistRootStorage is now /home/steam/.klei//DoNotStarveTogether/Cluster_1/Master/
[00:00:00]: Starting Up
[00:00:00]: Version: 617969
[00:00:00]: Current time: Tue Jul  2 18:22:40 2024

[00:00:00]: System Name: Linux
[00:00:00]: Host Name: iZ2vc17uca9zl48iicx7ojZ
[00:00:00]: Release(Kernel) Version: 3.10.0-1160.108.1.el7.x86_64
[00:00:00]: Kernel Build Timestamp: #1 SMP Thu Jan 25 16:17:31 UTC 2024
[00:00:00]: Machine Arch: x86_64
[00:00:00]: Don&apos;t Starve Together: 617969 LINUX
[00:00:00]: Build Date: 9567
[00:00:00]: Mode: 32-bit
...
```

此时，会自动创建存档目录 `/home/steam/.klei/DoNotStarveTogether/Cluster_1`，如果后续不再需要可直接删除。

## 添加饥荒联机版服务器管理脚本

`screen` 命令可以用来创建独立的命令行窗口执行进程，方便对进程的管理，例如追踪运行的日志信息。笔者选用它来管理饥荒联机版服务器的进程。

```bash
yum install -y screen
```

首先创建存放脚本的文件夹：

```bash
mkdir /home/steam/scripts
```

下面开始编写管理服务器过程中最常用的两个脚本。

### 自动更新脚本

创建自动更新饥荒联机版的脚本 `/home/steam/scripts/update_dst.sh`，编写内容如下：

```sh
#!/bin/sh

# 更新饥荒联机版
cd /home/steam
./steamcmd.sh +@ShutdownOnFailedCommand 1 +@NoPromptForPassword 1 +force_install_dir /home/steam/steamapps/DST +login anonymous +app_update 343050 validate +quit

# 更新已安装的模组
cd /home/steam/steamapps/DST/bin
./dontstarve_dedicated_server_nullrenderer -cluster CustomSaveName -only_update_server_mods
```

其中 `CustomSaveName` 为您的存档目录名称，可自定义修改，如不配置将默认使用 `Cluster_1`。

添加脚本的执行权限，后略：

```bash
chmod +x /home/steam/scripts/update_dst.sh
```

### 启动（重启）服务器脚本

创建启动（重启）饥荒联机版服务器的脚本 `/home/steam/scripts/start_dst.sh`，编写内容如下：

```sh
#!/bin/sh

# 保存服务器数据并退出
screen -S dst_master -X stuff $&apos;c_shutdown()\n&apos;
screen -S dst_caves -X stuff $&apos;c_shutdown()\n&apos;

# 更新饥荒联机版
/home/steam/scripts/update_dst.sh

cd /home/steam/steamapps/DST/bin
# 启动地上世界服务器
screen -dmS dst_master ./dontstarve_dedicated_server_nullrenderer -cluster CustomSaveName -console -shard Master
# 启动洞穴世界服务器
screen -dmS dst_caves ./dontstarve_dedicated_server_nullrenderer -cluster CustomSaveName -console -shard Caves
```

脚本将自动保存服务器数据（如果不使用 `c_shutdown()` 而直接关闭 Screen 的话，将丢失度过黑夜前未保存的所有数据），检查并安装饥荒联机版更新包，最后重新启动服务器。

服务器启动后，执行命令 `screen -r dst_master` 或 `screen -r dst_caves` 查看日志，您会发现打印有如下内容：

```plaintext
[00:00:05]: [200] Account Failed (6): &quot;E_INVALID_TOKEN&quot;
[00:00:05]: !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
[00:00:05]: !!!! Your Server Will Not Start !!!!
[00:00:05]: !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
[00:00:05]: No auth token could be found.
[00:00:05]: Please visit https://accounts.klei.com/account/game/servers?game=DontStarveTogether
[00:00:05]: to generate server configuration files
```

这意味着我们最后还需要配置 Auth Token 等信息，使得服务器能正确地注册并被联机用户发现。

## 正式启动饥荒联机版服务器

访问 https://accounts.klei.com/account/game/servers?game=DontStarveTogether，创建一个饥荒联机版服务器并获取 Auth Token：

![创建服务器](./run-dont-starve-server/add-new-server.png)

填写基本的配置信息：

![配置服务器](./run-dont-starve-server/configure-server.png)

下载设置并上传到主机，将解压后的文件放置到存档目录：

```bash
unzip DST_Lolipop.zip
mv MyDediServer /home/steam/.klei/DoNotStarveTogether/CustomSaveName

# 如果使用了 root 用户解压，别忘了修改文件夹所有权
# chown -R steam /home/steam/.klei/DoNotStarveTogether/CustomSaveName
```

最后的最后，执行前面编写的启动脚本：

```bash
/home/steam/scripts/start_dst.sh
```

在游戏大厅里搜索服务器，开始愉快地玩耍吧！

![搜索服务器](./run-dont-starve-server/search-server.png)

## 饥荒联机版服务器运维

### 添加模组

在饥荒联机版服务器添加并启用模组，需配置两部分内容。

#### 下载模组

第一部分是告诉服务器要下载哪些模组。编辑 `/home/steam/steamapps/DST/mods/dedicated_server_mods_setup.lua`，使用指令 `ServerModSetup()` 或 `ServerModCollectionSetup()` 添加需要下载的模组，例如：

```lua
-- /home/steam/steamapps/DST/mods/dedicated_server_mods_setup.lua
ServerModSetup(&quot;345692228&quot;)
ServerModCollectionSetup(&quot;3286974182&quot;)
```

通过上面的配置，服务器将自动下载 Steam 创意工坊上的模组 [#345692228](https://steamcommunity.com/sharedfiles/filedetails/?id=345692228)，以及模组集合 [#3286974182](https://steamcommunity.com/sharedfiles/filedetails/?id=3286974182) 中的所有模组。

由于每次执行 `steamcmd.sh` 命令检查游戏更新后会重置 `dedicated_server_mods_setup.lua`，建议手动备份该文件，这样的话未来增删模组时可以直接复用。例如：

```bash
cp dedicated_server_mods_setup.lua dedicated_server_mods_setup.temp.lua
```

优化前面我们编写的更新服务器脚本 `/home/steam/scripts/update_dst.sh`，在检查游戏更新之后还原备份文件：

```bash
cd /home/steam
./steamcmd.sh +@ShutdownOnFailedCommand 1 +@NoPromptForPassword 1 +force_install_dir /home/steam/steamapps/DST +login anonymous +app_update 343050 validate +quit

# 还原备份文件
cd /home/steam/steamapps/DST/mods
cp dedicated_server_mods_setup.temp.lua dedicated_server_mods_setup.lua

cd /home/steam/steamapps/DST/bin
./dontstarve_dedicated_server_nullrenderer -cluster CustomSaveName -only_update_server_mods
```

#### 启用模组

第二部分是告诉服务器要启用哪些模组。有两种配置方式，选择其一即可：

1. `/home/steam/.klei/DoNotStarveTogether/CustomSaveName/Caves/modoverrides.lua` 用于管理洞穴世界模组的启用与配置；`/home/steam/.klei/DoNotStarveTogether/CustomSaveName/Master/modoverrides.lua` 用于管理地上世界模组的启用与配置。
2. `/home/steam/steamapps/DST/mods/modsettings.lua` 用于管理模组的启用。需留意的是：1）本方式通常用于模组的开发调试；2）不能对模组进行详细设置，模组将始终使用默认配置；3）使用部分模组时可能会报错，此时请切换为第一种配置方法。

笔者选用第一种配置方法，新建文件 `modoverrides.lua`，编写内容如下：

```lua
-- modoverrides.lua
return {
    [&quot;workshop-345692228&quot;] = { enabled = true },
    [&quot;workshop-xxxxxxxxx&quot;] = { enabled = true }
}
```

需注意的是，`modoverrides.lua` 不支持一键启用某个模组合集的所有模组，还是需要获取这些模组的编号并编写配置。

添加可执行权限并拷贝到对应目录：

```bash
chmod +x modoverrides.lua
cp modoverrides.lua /home/steam/.klei/DoNotStarveTogether/CustomSaveName/Caves/
cp modoverrides.lua /home/steam/.klei/DoNotStarveTogether/CustomSaveName/Master/
```

执行脚本 `/home/steam/scripts/start_dst.sh` 重启服务器即可。

![服务器模组](./run-dont-starve-server/server-with-mods.png)

### 自定义世界

笔者建议在电脑上手动启动一个自定义好的世界，再将生成的配置文件 `worldgenoverride.lua` 和 `leveldataoverride.lua` 分别粘贴到服务器的对应文件夹里。地上世界和洞穴世界的配置文件有所区别，不要搞混喽：

- `/home/steam/.klei/DoNotStarveTogether/CustomSaveName/Caves/leveldataoverride.lua`
- `/home/steam/.klei/DoNotStarveTogether/CustomSaveName/Caves/worldgenoverride.lua`
- `/home/steam/.klei/DoNotStarveTogether/CustomSaveName/Master/leveldataoverride.lua`
- `/home/steam/.klei/DoNotStarveTogether/CustomSaveName/Master/worldgenoverride.lua`

其中，`worldgenoverride.lua` 是世界生成规则，修改后需要重新生成世界才能生效。

### 进一步配置服务器

之前步骤中下载的基本配置已足够支持正常游玩。如果还想要更深度的定制，可以参考[此文档](https://steamcommunity.com/sharedfiles/filedetails/?id=1616647350)，按需修改以下三个文件：

- `/home/steam/.klei/DoNotStarveTogether/CustomSaveName/cluster.ini`
- `/home/steam/.klei/DoNotStarveTogether/CustomSaveName/Caves/server.ini`
- `/home/steam/.klei/DoNotStarveTogether/CustomSaveName/Master/server.ini`

### 定时重启并更新服务器

可以使用 `crontab` 命令实现，但默认情况下，我们新建的 `steam` 用户是没有使用该命令的权限的，需要首先切到管理员用户添加权限：

```bash
echo steam &gt;&gt; /etc/cron.allow
```

切回 `steam` 用户，编辑计划任务 `crontab -e`，添加如下内容：

```plaintext
0 6 * * * /home/steam/scripts/start_dst.sh
```

上面的配置表示，在系统时间每天凌晨 6 点整，自动重启并更新饥荒联机版服务器。
</content:original-text><content:updated-at>2024-07-18T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[使用 Nginx 治理我的服务]]></title><description><![CDATA[这些天在阿里云的 ECS 服务器上捣鼓自己的东西，通过 Nginx 转发请求，允许以域名的方式访问到笔者开设的不同站点、服务。 笔者撰写本篇文章，晒晒在服务器上都做了哪些工作，也希望能为您提供一些启发。

安装最新版本的 Nginx

笔者使用的服务器为 CentOS 7 系统，默认的 yum 源中包含的 Nginx 版本为 （2021-05-21）。

更新 yum 源，添加 Nginx 的官方源：…]]></description><link>https://blog.towind.fun/posts/nginx-services</link><guid isPermaLink="false">nginx-services</guid><category><![CDATA[技术琐事]]></category><pubDate>Mon, 13 May 2024 00:00:00 GMT</pubDate><content:original-text>
这些天在阿里云的 ECS 服务器上捣鼓自己的东西，通过 Nginx 转发请求，允许以域名的方式访问到笔者开设的不同站点、服务。

笔者撰写本篇文章，晒晒在服务器上都做了哪些工作，也希望能为您提供一些启发。

## 安装最新版本的 Nginx

笔者使用的服务器为 CentOS 7 系统，默认的 yum 源中包含的 Nginx 版本为 `1.20.1`（2021-05-21）。

更新 yum 源，添加 Nginx 的官方源：

```bash
rpm -ivh http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm
```

确认 Nginx 官方源拉取成功：

```bash
$ yum repolist
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
 * centos-sclo-rh: mirrors.ustc.edu.cn
 * centos-sclo-sclo: mirrors.ustc.edu.cn
nginx                                                                        | 2.9 kB  00:00:00
nginx/x86_64/primary_db                                                      |  91 kB  00:00:00
repo id                            repo name                                                  status
nginx/x86_64                       nginx repo                                                   338
```

重新安装 Nginx：

```bash
yum install nginx
```

查看当前的 Nginx 版本：

```bash
$ nginx -v
nginx version: nginx/1.26.0
```

## 提供静态内容服务

&gt; Web 服务器的一个重要任务是提供文件（比如图片或者静态 HTML 页面）服务。

### 静态站点服务

#### 部署静态站点

要对外提供静态站点非常简单，只需要将站点的静态资源放置在服务器上，再通过 Nginx 暴露出去。

一个简单的例子是：

```conf
# /etc/nginx/nginx.conf
http {
  server {
    listen 443 ssl;
    http2 on;

    ssl_certificate /etc/letsencrypt/live/towind.fun/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/towind.fun/privkey.pem;

    server_name towind.fun www.towind.fun blog.towind.fun;
    root /var/www/towind.fun/blog;
  }
}
```

通过上面的配置，笔者只需要将自己博客的静态资源放置在 `/var/www/towind.fun/blog` 目录，就可以通过 https://towind.fun 等域名访问到啦。

当然，笔者不希望有这么多个域名显示完全一样的东西，我们可以配置域名跳转，将请求重定向到一个域名上：

```conf
# /etc/nginx/nginx.conf
http {
  server {
    # ...https configurations
    server_name towind.fun www.towind.fun;
    rewrite ^/(.*)$ https://blog.towind.fun/$1 permanent;
  }
}
```

现在，访问 https://towind.fun 和 https://www.towind.fun 时，浏览器将自动 301 重定向到 https://blog.towind.fun。

此外，笔者为了更好的 SEO，缩短了链接的级数，即从 `/YYYY/MM/DD/blog-title` -&gt; `/YYYYMMDD/blog-title`。那么就需要将过去被搜索引擎收录的链接，重定向到新的链接，避免用户访问到 404 页面。可以编写配置如下：

```conf
# /etc/nginx/nginx.conf
http {
  server {
    # ...https configurations
    server_name blog.towind.fun;
    rewrite &quot;^/(\d{4})/(\d{2})/(\d{2})/(.+)$&quot; /$1$2$3/$4 permanent;
  }
}
```

通过简单的正则匹配即可实现链接重定向。

#### 抽取通用配置

Nginx 配置中存在大量重复的内容，我们可以将这些内容提取出来，单独放置在某个目录下，通过 `include` 指令引入。

例如对于 HTTPS 配置，可以提取为：

```conf
# /etc/nginx/conf.shared.d/https.conf
listen 443 ssl;
http2 on;

ssl_certificate /etc/letsencrypt/live/towind.fun/fullchain.pem;
ssl_certificate_key /etc/letsencrypt/live/towind.fun/privkey.pem;
```

在需要的 `server` 块中引入：

```conf
# /etc/nginx/nginx.conf
http {
  server {
    include /etc/nginx/conf.shared.d/https.conf;
    server_name blog.towind.fun;
  }
}
```

又如希望用户在访问 `http://` 时自动跳转到 `https://`，可以提取为：

```conf
# /etc/nginx/conf.shared.d/http.conf
listen 80;
return 301 https://$http_host$request_uri;
```

像这样抽取出不同服务里重复的配置，能够有效地降低后续的维护成本。

#### 静态站点持续部署

由于静态站点的源码均托管在 Github 上，当新的代码提交后，希望能够自动更新服务器上的静态资源内容。笔者通过 `crontab` 配置定时任务来实现这个需求：

```bash
0 0,12,18 * * * /path/to/update-blog.sh &gt;&gt; /path/to/update-blog.log 2&gt;&amp;1
```

上面的配置表示：在每天的 0, 12, 18 点整自动执行更新博客的脚本 `/path/to/update-blog.sh`，执行的标准输出和错误输出重定向到指定日志文件 `/path/to/update-blog.log`。

至于更新脚本的实现则颇为简单，如果构建后的静态文件已经存放到了 Github 仓库的某个分支，那么只需要到本地的目录执行 `git pull` 命令即可。例如：

```bash
# /path/to/update-blog.sh
cd /var/www/towind.fun/blog
git pull
```

如果没有存放构建后的静态文件，或构建后的静态文件无法直接使用（例如 `bashPath` 不同），那么额外执行一次构建命令即可。例如：

```bash
# /path/to/update-example.sh
cd /path/to/example
git pull
npm run build
```

考虑到我们不会在服务器上编写代码并推送，可以使用 HTTPS 协议的远程地址，而不用经过 SSH 验证：

```bash
cd /path/to/example
# Change from git@github.com:username/example.git
git remote set-url origin https://github.com/username/example.git
```

### 静态站点访问性能优化

针对静态站点的访问性能优化，笔者主要配置了 Nginx 中**压缩**和**缓存**两部分内容，另外关闭了负优化的 Cloudflare CDN。

#### Gzip 压缩

配置 Nginx 启用 `gzip` 压缩，能够**显著减少**发送给客户端的静态文件体积。配置内容如下：

```conf
# /etc/nginx/nginx.conf
http {
  # 启用 gzip 压缩功能
  gzip on;
  # 压缩文件的最小大小为 10KB
  gzip_min_length 10K;
  # 压缩等级为 6。等级越低压缩速度越快，文件压缩比越小；反之速度越慢，压缩比越大
  gzip_comp_level 6;
  # 压缩的 MIME 类型
  gzip_types text/plain text/css application/json text/javascript application/javascript application/x-javascript text/xml application/xml application/xml+rss;
}
```

对于原来 1151KB 的脚本文件：

```bash
$ ll --block-size=k | grep main.js
-rw-r--r-- 1 root root 1151K main.js
```

压缩后发送给客户端只有 442KB 大小，减少了大约 62% 的体积：

![assets-gzip](./nginx-services/assets-gzip.png)

在启用 `gzip` 压缩之前，笔者从访问自己的博客到文章内容显示出来，要等待大约 5 秒钟的时间。说实话，若不是自己家的站点，早已不耐烦地 `ctrl + w` 关闭了。如今只需大约 2 秒钟的时间，给访问体验带来了质的提升。

实际观察 Github Pages 的网络响应就会发现，返回给客户端的脚本或样式等文件也都经过了压缩（`br` 编码），可惜笔者到现在才知道去配置，知识积累的重要性不言而喻。

#### Cache-Control 缓存

配置 Nginx 启用 `Cache-Control` 缓存，由客户端控制是否使用本地已缓存的文件。配置内容如下：

```conf
# /etc/nginx/conf.shared.d/cache.conf
# 协商缓存验证 .html 文件
location ~* .(html)$ {
  add_header Cache-Control &quot;no-cache&quot;;
}
# 缓存 .css, .js 文件，缓存过期时间为 1 天，缓存过期时确保获取到最新文件
location ~* .(css|js)$ {
  add_header Cache-Control &quot;public, must-revalidate, max-age=86400&quot;;
}
# 缓存图片、视频等文件，缓存过期时间为 1 年，不得对文件进行转换
location ~* .(png|jpg|jpeg|gif|ico|svg|mp4|ogg|ogv|webm|htc|xml|woff|gz|zip|7z)$ {
  add_header Cache-Control &quot;public, no-transform, max-age=31536000&quot;;
}
```

#### CDN 服务

笔者使用 Cloudflare 作为 DNS 解析服务器，但考虑到站点面向的用户主要来自中国，且访问量不会很大，因此关闭了 Cloudflare 提供的 DNS 代理服务。

![cloudflare-cdn-speed-down-for-me](https://i.imgur.com/9H8utju.png)

### 静态文件服务

笔者也对外提供的静态文件下载的服务，配置形如：

```conf
# /etc/nginx/conf.d/download.conf
server {
  include /etc/nginx/conf.shared.d/http.conf;
  server_name download.towind.fun;
}

server {
  include /etc/nginx/conf.shared.d/https.conf;
  server_name download.towind.fun;
  root /var/www/towind.fun/download;
  # 允许浏览静态文件目录
  autoindex on;
  # 显示静态文件大小
  autoindex_exact_size on;
  # 显示文件时间为服务器时间
  autoindex_localtime on;
  # 设置字符集为 utf-8，避免中文乱码
  charset utf-8;
}
```

上面的配置表明：笔者将对外提供下载的静态文件放置在服务器的 `/var/www/towind.fun/download` 目录下；当访问 https://download.towind.fun 时，就可以看见所有可下载的静态文件。

## 提供代理服务器

&gt; Nginx 的一个常见用途是作为一个代理服务器，作用是接收请求并转发给被代理的服务器，从中取得响应，并将其发送回客户端。

### 访问内网服务

笔者将一些服务部署在了没有公网 IP 地址的内网服务器上，为了能通过域名访问到这些服务，首先使用了 frp 进行内网穿透：将拥有公网 IP 地址的服务器（公网服务器）作为 frp 服务端，将内网服务器作为 frp 客户端即可。

配置 Nginx，将特定域名的请求转发到对应的 frp 服务端端口：

```conf
# /etc/nginx/conf.d/xxx.conf
server {
  include /etc/nginx/conf.shared.d/https.conf;
  server_name xxx.towind.fun;
  location = / {
    proxy_pass http://127.0.0.1:15244;
  }
}
```

上面的配置表示：当通过 `https://xxx.towind.fun` 访问公网服务器时，请求将转发至本地的 15244 端口，再经由 frp 访问到内网服务器上对应的服务。

### 内网服务访问性能优化

Nginx 提供了服务端的 Proxy 缓存功能，如果从内网服务器返回的响应头上包含缓存控制的信息，Nginx 将自动缓存资源到公网服务器，而无需每次都去请求内网服务器。

核心的配置内容如下：

```conf
# /etc/nginx/nginx.conf
http {
  proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=defaultcache:10m max_size=1g;

  server {
    proxy_cache defaultcache;
    # （非必要）响应头添加自定义的 Nginx-Proxy-Cache 字段，如果值为 HIT 则表示命中了公网服务器本地的缓存
    add_header Nginx-Proxy-Cache $upstream_cache_status;
  }
}
```

上面的配置表示：

- 缓存文件保存在 `/var/cache/nginx` 目录。
- 缓存文件使用 2 级目录存储，第一级目录包含最多 16^1 个文件夹，第二级目录包含最多 16^2 个文件夹。即总共最多包含 16^1 \* 16^2 = 4096 个文件夹。
- 在共享内存中设置了一块别名为 `defaultcache`，大小为 10MB 的存储区域，用于存储 key 字符串。有助于 Nginx 快速判断请求是否命中本地的缓存。
- 缓存文件占用的最大空间为 1GB。达到配额时，Nginx 会自动删除掉使用频率最低的缓存文件。

这样可以有效降低内网服务器（源服务器）的负担。

一个命中 Proxy 缓存的例子如下：

![assets-gzip](./nginx-services/proxy-cache-hit.png)
</content:original-text><content:updated-at>2024-07-17T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[基于原生 Node 备份软路由上的 Minecraft 服务器存档，并通过 Alist 上传到云端]]></title><description><![CDATA[笔者最近在 OpenWRT 软路由上部署了一个 Minecraft 服务器，出于对数据安全的焦虑，于是折腾了一下存档备份的相关事宜，记录为此文。 在 CurseForge 等模组站上已有方便好用的 Minecraft 服务器存档备份插件，除非您喜欢折腾或高自由度的定制，不用像笔者这样编写一整个脚本。

完整的脚本可见此。

编写备份脚本
前置准备

为了脚本编写方便，约定应该在 Minecraft…]]></description><link>https://blog.towind.fun/posts/backup-mc-server</link><guid isPermaLink="false">backup-mc-server</guid><category><![CDATA[后端开发]]></category><pubDate>Sat, 11 May 2024 00:00:00 GMT</pubDate><content:original-text>
笔者最近在 OpenWRT 软路由上部署了一个 Minecraft 服务器，出于对数据安全的焦虑，于是折腾了一下存档备份的相关事宜，记录为此文。

在 CurseForge 等模组站上已有方便好用的 Minecraft 服务器存档备份插件，除非您喜欢折腾或高自由度的定制，不用像笔者这样编写一整个脚本。

完整的脚本[可见此](https://github.com/LolipopJ/LolipopJ.github.io/blob/cd9d45af2a97f596ec6b3ada4c069f88a9e9bbd7/source/static/scripts/backup-mc-server.js)。

## 编写备份脚本

### 前置准备

为了脚本编写方便，约定应该在 Minecraft 服务器的根目录执行脚本。校验当前脚本的执行目录：

```js
const cwd = process.cwd();
if (!fs.existsSync(path.resolve(cwd, &quot;eula.txt&quot;))) {
  throw new Error(
    &quot;You should execute this script at root dir of MineCraft server where `eula.txt` exists.&quot;,
  );
}
```

支持指定备份文件存储的目录 `BACKUP_DIR`，同时 `checkupDir()` 确保目录存在：

```js
const BACKUP_DIR = &quot;backups&quot;;

const checkupDir = (dir) =&gt; {
  if (!fs.existsSync(dir)) {
    fs.mkdirSync(dir, { recursive: true });
  }
};

const backupDir = path.resolve(cwd, BACKUP_DIR);
checkupDir(backupDir);
```

### 生成备份文件

支持指定备份文件的列表，除了最重要的 `world/` 以外，还可以备份 `server.properties`、`world_nether/` 和 `world_the_end/` 等文件或目录。

```js
const BACKUP_FILES = [
  &quot;banned-ips.json&quot;,
  &quot;banned-players.json&quot;,
  &quot;config&quot;,
  &quot;mods&quot;,
  &quot;ops.json&quot;,
  &quot;server.properties&quot;,
  &quot;whitelist.json&quot;,
  &quot;world&quot;,
  &quot;world_nether&quot;,
  &quot;world_the_end&quot;,
];

const resolvedBackupFiles = BACKUP_FILES.filter((file) =&gt; {
  if (fs.existsSync(path.resolve(cwd, file))) {
    return true;
  }
  return false;
});
```

原生 Node 并没有提供打包压缩的方法，为了避免引入其它的依赖，考虑使用系统自带的 `tar` 命令实现。为此，需要使用到 Node 的 `child_process`：

```js
const child_process = require(&quot;child_process&quot;);
const util = require(&quot;util&quot;);
const exec = util.promisify(child_process.exec);
```

现在，可以通过 `exec()` 来执行系统上的命令了。可编写文件备份方法如下：

```js
const backupFilename = genFilename(); // 省略文件名生成方法...
await exec(`tar -czf ${backupFilename} ${resolvedBackupFiles.join(&quot; &quot;)}`);
```

需注意的是，我们在执行 `tar` 命令时，常传入 `-v` 标识，在屏幕上打印压缩或解压的文件列表（很酷）。但是，在使用 `exec()` 时，子进程会将命令的标准输出或错误一并返回，如果文件数量过多，标准输出超过预设大小，会导致报错：`RangeError [ERR_CHILD_PROCESS_STDIO_MAXBUFFER]: stdout maxBuffer length exceeded`。用 `child_process.spawn()` 可以避免这个问题，但考虑到我们并不在乎压缩命令的执行过程，最好的办法就是去掉 `-v` 标识。

到此为止，已经能够将所需的 Minecraft 服务器存档文件打包压缩，备份到系统本地了。

### 移除历史备份文件

即使每天执行一次备份任务，长久累积也将占用大量的空间，更何况一个备份的大小已然几百 MB 起步。因此需考虑本地保留的备份文件数量，及时移除历史的备份文件。

首先需要获取备份目录下已有的备份文件信息：

```js
const filenames = fs.readdirSync(backupDir);
const backupFileList = filenames.map((filename) =&gt;
  fs.statSync(path.resolve(backupDir, filename)),
);
```

支持指定保存的备份文件数量，得到需要移除的文件列表：

```js
const BACKUP_MAX_NUM = 7; // 保留最新的 N 个备份文件，此处为 7 个

const backupFiles = backupFileList
  .filter((file) =&gt; file.isFile())
  .sort((a, b) =&gt; {
    return b.mtimeMs - a.mtimeMs;
  });
const oldBackupFiles = backupFiles.slice(BACKUP_MAX_NUM);
```

移除这些文件即可：

```js
const oldBackupFilenames = oldBackupFiles.map((file) =&gt; file.name);
oldBackupFilenames.forEach((filename) =&gt; {
  fs.rmSync(path.resolve(backupDir, filename));
});
```

这样在每次执行脚本时，都会自动清理掉本地多余的备份文件，保证文件系统容量健康。

## 设置定时任务

基于 `crontab` 实现定时任务调度，使用 `crontab -e` 命令编写任务列表：

```bash
0 4 * * * cd /path/to/mc-server &amp;&amp; node /path/to/backup-mc-server.js
```

笔者发现定时任务实际执行时间是正午 12 点，而非预期的凌晨 4 点，推测系服务器使用的 UTC 时区导致。

尽管配置了 OpenWRT 的时区为 `Asia/Shanghai`，但仍然不生效：

```bash
$ date -R
Wed, 15 May 2024 07:00:00 +0000
```

笔者通过安装 `zoneinfo-asia` 解决了问题：

```bash
$ opkg update
$ opkg install zoneinfo-asia
Installing zoneinfo-asia (2023c-2) to root...
Downloading https://mirrors.vsean.net/openwrt/releases/23.05.2/packages/x86_64/packages/zoneinfo-asia_2023c-2_x86_64.ipk
Installing zoneinfo-core (2023c-2) to root...
Downloading https://mirrors.vsean.net/openwrt/releases/23.05.2/packages/x86_64/packages/zoneinfo-core_2023c-2_x86_64.ipk
Configuring zoneinfo-core.
Configuring zoneinfo-asia.
$ /etc/init.d/system restart
$ date -R
Wed, 15 May 2024 15:00:00 +0800
```

这样，在北京时间凌晨 4 点，系统将自动调用备份脚本。如果彼时仍有用户在游玩，脚本可能会运行失败，可以在执行脚本之前关闭 Minecraft 服务器，完成后重新启动。

## （可选）通过 Alist 上传到云端

&gt; 为了这盘醋，包了这顿饺子。

万一硬盘挂了呢？笔者认为保存在软路由本地丝毫没有安全感，于是决定在备份后即时上传到云端。

笔者已经在软路由上安装并配置好了 Alist，连接到了自己的 OneDrive。下面将进一步实现上传备份文件到 OneDrive 或任何其他的云盘。

### 获取 Alist token

调用 Alist 接口时需要传入 token，因此首先需要获取 token：

```js
const ALIST_ADDRESS = &quot;YOUR_ALIST_ADDRESS&quot;;
const ALIST_USERNAME = &quot;YOUR_ALIST_USERNAME&quot;;
const ALIST_PASSWORD = &quot;YOUR_ALIST_PASSWORD&quot;;

const headers = new Headers();
headers.append(&quot;Content-Type&quot;, &quot;application/json&quot;);

const raw = JSON.stringify({
  username: ALIST_USERNAME,
  password: ALIST_PASSWORD,
});

const requestOptions = {
  method: &quot;POST&quot;,
  headers,
  body: raw,
  redirect: &quot;follow&quot;,
};

const res = await fetch(`${ALIST_ADDRESS}/api/auth/login`, requestOptions);
const resText = await res.text();
const resObj = JSON.parse(resText);

const alistToken = resObj.data.token;
```

### 上传备份文件到云盘

现在，编写 Alist 上传文件的方法：

```js
const ALIST_BACKUP_DIR = &quot;/path/to/mc-backups&quot;;

const backupFile = fs.statSync(backupFilename);
const backupFileBasename = path.basename(backupFilename);
const alistFilePath = path.resolve(ALIST_BACKUP_DIR, backupFileBasename);

const headers = new Headers();
headers.append(&quot;Authorization&quot;, alistToken);
headers.append(&quot;As-Task&quot;, &quot;true&quot;);
headers.append(&quot;Content-Length&quot;, `${backupFile.size}`);
headers.append(&quot;File-Path&quot;, encodeURIComponent(alistFilePath));

const fileStream = fs.createReadStream(backupFilename);
const requestOptions = {
  method: &quot;PUT&quot;,
  headers,
  body: fileStream,
  redirect: &quot;follow&quot;,
  duplex: &quot;half&quot;,
};

await fetch(`${ALIST_ADDRESS}/api/fs/put`, requestOptions);
```

通过 `headers.append(&quot;As-Task&quot;, &quot;true&quot;);` 将文件上传设为任务，避免阻塞其它命令的执行。在 Alist 管理后台可以看到上传的进度：

![upload-to-alist](./backup-mc-server/upload-to-alist.png)

到这一步，执行备份脚本时，将自动把新生成的备份文件上传到云盘。

### 移除云盘历史备份文件

同样，云盘的空间也不是无限的，我们采取与移除本地历史备份文件相同的策略。

首先获取云盘上已有的备份文件列表：

```js
const headers = new Headers();
headers.append(&quot;Authorization&quot;, alistToken);
headers.append(&quot;Content-Type&quot;, &quot;application/json&quot;);

const raw = JSON.stringify({
  path: ALIST_BACKUP_DIR,
});

const requestOptions = {
  method: &quot;POST&quot;,
  headers: headers,
  body: raw,
  redirect: &quot;follow&quot;,
};

const res = await fetch(`${ALIST_ADDRESS}/api/fs/list`, requestOptions);
const resText = await res.text();
const resObj = JSON.parse(resText);

const backupDirFileList = resObj.data.content || [];
```

获取需要移除的备份文件列表：

```js
const backupFiles = backupDirFileList
  .filter((file) =&gt; !file.is_dir)
  .sort((a, b) =&gt; {
    if (a.modified &gt; b.modified) {
      return -1;
    } else if (a.modified &lt; b.modified) {
      return 1;
    } else {
      return 0;
    }
  });

// 由于新的备份文件正在上传中，因此应当保留最新的 BACKUP_MAX_NUM - 1 个备份文件
const oldBackupFiles = backupFiles.slice(BACKUP_MAX_NUM - 1);
const oldBackupFilenames = oldBackupFiles.map((file) =&gt; file.name);
```

最后，移除这些文件即可：

```js
const headers = new Headers();
headers.append(&quot;Authorization&quot;, alistToken);
headers.append(&quot;Content-Type&quot;, &quot;application/json&quot;);

const raw = JSON.stringify({
  names: oldBackupFilenames,
  dir: ALIST_BACKUP_DIR,
});

const requestOptions = {
  method: &quot;POST&quot;,
  headers: headers,
  body: raw,
  redirect: &quot;follow&quot;,
};

await fetch(`${ALIST_ADDRESS}/api/fs/remove`, requestOptions);
```

这样在每次执行脚本时，云盘的系统容量健康也得到了保障。

## 结尾

稍微润色优化一下备份脚本，执行的输出结果如下：

```bash
$ node /path/to/backup-mc-server.js
Create dir `/path/to/mc-server/backups` successfully.
Creating backup file `/path/to/mc-server/backups/backup-mcserver-2024-05-11-11-10-51.tar.gz` ...
Create backup file `/path/to/mc-server/backups/backup-mcserver-2024-05-11-11-10-51.tar.gz` successfully.
Log in alist successfully.
Start upload task successfully: local file `/path/to/mc-server/backups/backup-mcserver-2024-05-11-11-10-51.tar.gz` ==&gt; alist `/path/to/alist/backups/backup-mcserver-2024-05-11-11-10-51.tar.gz`.

===========================
Backup file is generated: true
Old backup files are removed: true
Task that upload backup file to alist is started: true
Old backup files in alist are removed: true
===========================
```

啊，满满的安心感！收工。
</content:original-text><content:updated-at>2024-05-23T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[遇到 AntD 组件中文乱码问题，可以试试这么解决]]></title><description><![CDATA[项目中使用了 AntD 4.x 的  组件，开发环境显示正常，生产环境显示乱码，如下图所示： 问题原因

 组件底层的国际化既由 AntD 提供的  控制（如上图的“年”，显示正常），又由 Moment 控制（如上图的“月”，显示乱码）。

经查询，当我们以 ISO8859-1 方式读取 UTF-8 编码的中文时，会出现如“ç”±æœˆè¦�å¥½å¥½å­¦ä¹ å¤©å¤©å�‘ä¸Š…]]></description><link>https://blog.towind.fun/posts/antd-comp-garbled-characters</link><guid isPermaLink="false">antd-comp-garbled-characters</guid><category><![CDATA[前端开发]]></category><pubDate>Mon, 09 Oct 2023 00:00:00 GMT</pubDate><content:original-text>
项目中使用了 AntD 4.x 的 `&lt;DatePicker /&gt;` 组件，开发环境显示正常，生产环境显示乱码，如下图所示：

![error](./antd-comp-garbled-characters/error.png)

## 问题原因

`&lt;DatePicker /&gt;` 组件底层的国际化既由 AntD 提供的 `&lt;ConfigProvider /&gt;` 控制（如上图的“年”，显示正常），又由 Moment 控制（如上图的“月”，显示乱码）。

经查询，当我们以 ISO8859-1 方式读取 UTF-8 编码的中文时，会出现如“ç”±æœˆè¦�å¥½å¥½å­¦ä¹ å¤©å¤©å�‘ä¸Š”这样的符号型乱码，正如上图所示。

因此产生问题的关键在于，**为何浏览器没有正确地以 UTF-8 格式读取 Moment 提供的中文语言包文本。**

经排查，发现在访问生产环境时，服务端返回的 HTML 文档设置了编码为 ISO8859-1：

![response-content-type](./antd-comp-garbled-characters/response-content-type.png)

一切便水落石出。

## 如何解决

最好的解决方案是联系服务端同学，修改 `Content-Type=text/html;charset=UTF-8`。

在这个项目中，使用了 CDN 的方式引入 Moment，因此可以采用的一个临时解决方案为强制指定编码，例如：

```html
&lt;script
  src=&quot;https://gw.alipayobjects.com/os/lib/??moment/2.29.1/moment.js,moment/2.29.1/locale/zh-cn.js&quot;
  charset=&quot;utf-8&quot;
&gt;&lt;/script&gt;
```
</content:original-text><content:updated-at>2023-10-09T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[为什么我使用 Umi 的 model 简易数据流管理插件]]></title><description><![CDATA[Umi 是一款企业级的 React 前端应用框架，云巧产业数字组件中心推荐使用基于 Umi 的 Koi 框架统一前端应用研发流程，支撑前端项目从研发、联调到上线、发布的全流程。 本文假设您正在或计划使用 Umi 或 Koi 作为底层框架支撑前端应用的开发，并且对 Umi 有一定的了解。

数据治理的原则

React 的核心特征是“数据驱动视图”，用公式表达即 ，通过数据变化来驱动视图变化。React…]]></description><link>https://blog.towind.fun/posts/umi-plugin-usemodel</link><guid isPermaLink="false">umi-plugin-usemodel</guid><category><![CDATA[前端开发]]></category><pubDate>Sun, 23 Oct 2022 00:00:00 GMT</pubDate><content:original-text>
Umi 是一款企业级的 React 前端应用框架，云巧产业数字组件中心推荐使用基于 Umi 的 Koi 框架统一前端应用研发流程，支撑前端项目从研发、联调到上线、发布的全流程。

本文假设您正在或计划使用 Umi 或 Koi 作为底层框架支撑前端应用的开发，并且对 Umi 有一定的了解。

## 数据治理的原则

React 的核心特征是“数据驱动视图”，用公式表达即 `UI = render(data)`，通过数据变化来驱动视图变化。React 将组件内部自有的数据称作 state（状态），通过管理 state 来实现对组件的管理。

通过 Props 传参，可以在 React 中实现简单的父子、子父和兄弟组件间数据传递。对于跨级组件间的数据传递，React 提供了基于生产者-消费者模式的 Context API 来实现全局通信。

随着应用的膨胀，组件内部的状态变得愈加复杂，数据流管理的成本也越来越高。如果说所有代码的末路都是成为一座难以维护的大山的话，在那之前，我们应当好好想想如何尽可能多地延长代码的寿命，去重新思考我们的 React 项目的代码组织逻辑。

Umi 建议将所有组件降级为“无状态组件”，仅仅依赖 Props 或 Context 进行渲染。这样，在 UI 层面仅关心用户交互和渲染的逻辑，在单独的数据层去关心数据处理的逻辑。以 Umi 项目为例，具体而言就是：

- `src/models` 中的文件管理数据层的逻辑，包含网络请求、数据处理等。
- `src/pages` 中的页面组件与数据层进行交互，并将得到的数据通过 Props 或 Context 传递给通用组件，进行页面渲染。
- `src/components` 中的通用组件仅仅依赖从 Props 或 Context 得到的数据进行渲染，不与数据层发生直接交互。

## 介绍 model 简易数据流插件

Umi 的 model 简易数据流插件就是基于 Context 的封装，使得数据能够在项目全局共享与使用。相比原生的 Context API，model 简易数据流插件更加便于使用。

使用 Context API 时，需要创建上下文对象，并在渲染树顶层包装上下文的 Provider：

```tsx
// src/contexts/userContext.tsx
import React, { createContext, useState } from &quot;react&quot;;
export const UserContext = createContext();
export const UserContextProvider = (props) =&gt; {
  const [username, setUsername] = useState&lt;string&gt;(&quot;&quot;);
  return (
    &lt;UserContext.Provider value={{ username }}&gt;
      {props.children}
    &lt;/UserContext.Provider&gt;
  );
};

// src/layouts/index.tsx
import React from &quot;react&quot;;
import { Outlet } from &quot;umi&quot;;
import { UserContextProvider } from &quot;@/contexts/userContext&quot;;
export const Layout = () =&gt; {
  return (
    &lt;UserContextProvider&gt;
      &lt;Outlet /&gt;
    &lt;/UserContextProvider&gt;
  );
};
export default Layout;
```

使用简易数据流插件可以略去创建上下文对象和包装 Provider 的过程，仅需要按约定目录导出一个自定义的 `hook` 函数即可：

```ts
// src/models/userModel.ts
import { useState } from &quot;react&quot;;
export default function () {
  const [username, setUsername] = useState&lt;string&gt;(&quot;&quot;);
  return { username };
}
```

在组件使用上下文中存储的数据时，Context API 需要：

```tsx
import React, { useContext } from &quot;react&quot;;
import { UserContext } from &quot;@/contexts/userContext&quot;;
export const pageElement = () =&gt; {
  const { username } = useContext(UserContext);
  return &lt;&gt;{username}&lt;/&gt;;
};
export default pageElement;
```

而使用简易数据流插件可以略去引入指定上下文对象的过程，Umi 已经自动为它创建了一个命名空间：

```tsx
import React from &quot;react&quot;;
import { useModel } from &quot;umi&quot;;
export const pageElement = () =&gt; {
  const { username } = useModel(&quot;userModel&quot;);
  return &lt;&gt;{username}&lt;/&gt;;
};
export default pageElement;
```

由于调用了 `useContext` 的组件总会在 `context` 值变化时重新渲染，可能需要使用 `useMemo` 或 `memo` 来优化重渲染开销较大的组件：

```tsx
import React, { useContext, useMemo } from &quot;react&quot;;
import { UserContext } from &quot;@/contexts/userContext&quot;;
export const pageElement = () =&gt; {
  const { username } = useContext(UserContext);
  return useMemo(() =&gt; {
    return &lt;ExpensiveTree username={username} /&gt;;
  }, [username]);
};
export default pageElement;
```

使用简易数据流插件时同样可以使用 `useMemo` 或 `memo` 来进行优化，但更好的选择是直接利用 `useModel()` 提供的过滤方法，只关心需要的数据：

```tsx
import React from &quot;react&quot;;
import { useModel } from &quot;umi&quot;;
export const pageElement = () =&gt; {
  const { username } = useModel(&quot;userModel&quot;, (model) =&gt; ({
    username: model.username,
  }));
  return &lt;ExpensiveTree username={username} /&gt;;
};
export default pageElement;
```

综上所述，Umi 的 model 简易数据流插件实现了对 Context API 的较好封装，能够降本增效，简化代码的编写。对于需要使用 Context 管理数据流的情况，都可以使用 model 简易数据流插件替代。

## 什么时候使用 model 简易数据流插件

近年来，微前端架构的兴起为企业前端应用开发注入了新的活力，随着巨型前端应用拆解成一个个小微型前端应用，页面数据的治理难度也大大降低了。

当您使用 Umi 或 Koi 开发**小微型前端应用**遇到全局状态共享或数据流管理需求时，model 简易数据流插件具有**便利**、**低心智负担**的优势，不妨好好利用它来管理页面数据。

针对更复杂的数据流管理需求（例如数据可预知甚至可回溯等），model 简易数据流插件显得有些捉襟见肘了：它只负责将数据全局化。别担心，开源社区提供了大量专注于做好数据流管理这件事的方案，例如 [Redux](https://github.com/reduxjs/redux) 和 [MobX](https://github.com/mobxjs/mobx) 等；Umi 官方目前也提供了对接 [dva](https://github.com/dvajs/dva) 和 [Valtio](https://github.com/pmndrs/valtio) 数据流管理库的插件，开箱即用。
</content:original-text><content:updated-at>2022-10-23T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[定时器 SetTimeout 在后台失效？试试 Web Worker 吧]]></title><description><![CDATA[业务上有这样一个需求：「若用户不活跃超过 12 个小时，自动退出当前页面，并切换路由到首页」。 想都没想，直接在  里用  定个时，12 个小时后触发相应跳转事件：

没想到，今天上班来，切换到没有关闭的标签页，发现还在当前页面，掐指一算怎么也有 12 个小时了，这是怎么一回事儿……？

昨天晚上走的时候还在和前辈探讨页面卸载（）事件与浏览器后台优化的坑，于是首先就想到了可能是浏览器优化的缘故…]]></description><link>https://blog.towind.fun/posts/js-webworker-settimeout</link><guid isPermaLink="false">js-webworker-settimeout</guid><category><![CDATA[前端开发]]></category><pubDate>Thu, 22 Sep 2022 00:00:00 GMT</pubDate><content:original-text>
业务上有这样一个需求：「若用户不活跃超过 12 个小时，自动退出当前页面，并切换路由到首页」。

想都没想，直接在 `useEffect()` 里用 `setTimeout()` 定个时，12 个小时后触发相应跳转事件：

```tsx
import React, { useEffect } from &quot;react&quot;;

const LEAVE_PAGE_COUNTDOWN = 12 * 60 * 60 * 1000; // 12h

/** 离开页面的方法 */
const leavePage = () =&gt; {
  // ...离开当前页面的业务代码
};

export default () =&gt; {
  useEffect(() =&gt; {
    // 初始化时设置定时器
    const timer = setTimeout(() =&gt; {
      leavePage();
    }, LEAVE_PAGE_COUNTDOWN);

    return () =&gt; {
      // 页面卸载时清除定时器
      if (timer) clearTimeout(timer);
    };
  }, []);
};
```

没想到，今天上班来，切换到没有关闭的标签页，发现还在当前页面，掐指一算怎么也有 12 个小时了，这是怎么一回事儿……？

昨天晚上走的时候还在和前辈探讨页面卸载（`unload`）事件与浏览器后台优化的坑，于是首先就想到了可能是浏览器优化的缘故，导致定时器没有正常执行。以「setTimeout」和「后台失效」为搜索关键词，很快找到了原因和优化解决方案。

## 失效原因

系现代浏览器为了节能与性能优化做的处理。

若页面处于非激活的状态，那么此页面中通过 `setTimeout()` 或 `setInterval()` 创建的定时器可能会**停止工作**或**以较慢的速度工作**。页面的非激活状态包括不限于：切换到其它标签页、最小化窗口和息屏等。在移动端，这样的性能优化尤为常见。

因此会发生另外一种常见的现象：如果浏览器页面里有一个基于 `setInterval()` 实现的计时器，当用户切换页面或回到桌面后，计时器将停止计时或计时频率减慢，导致计时功能异常。

## 基于 SetTimeout / SetInterval 的解决方案

定时器失效带来的最直接影响是：JavaScript 代码不再能够正确获取定时器**计划执行的时间**或**已经执行的次数**。

一个很容易想到的解决方案是，当页面切回前台时，重新校准 SetTimeout 定时器时间。

### 使用 SetTimeout + 监听 visibilitychange 事件

通过监听窗口的 `visibilitychange` 事件（兼容性[见于此](https://developer.mozilla.org/zh-CN/docs/Web/API/Document/visibilitychange_event#browser_compatibility)），可以判断页面是否切换到前台：

```ts
window.addEventListener(&quot;visibilitychange&quot;, () =&gt; {
  switch (document.visibilityState) {
    case &quot;visible&quot;:
      // 当前页面被切换到前台（可见或部分可见）
      break;
    case &quot;hidden&quot;:
      // 当前页面被切换到后台（不可见）
      break;
    case &quot;prerender&quot;:
      // 当前页面被预渲染，且用户不可见
      break;
    case &quot;unloaded&quot;:
      // 当前页面被卸载
      break;
  }
});
```

对于这次业务上遇到的 12 小时自动切换路由这一需求，对即时性和定时器的精度要求并不高，且重新校准的逻辑容易编写，可以码出 React 代码如下：

```tsx
import React, { useEffect } from &quot;react&quot;;

const LEAVE_PAGE_TIMESTAMP = &quot;__leave_page_timestamp&quot;;
const LEAVE_PAGE_COUNTDOWN = 12 * 60 * 60 * 1000;

/** 离开页面的方法 */
const leavePage = () =&gt; {
  // ...离开当前页面的业务代码
};

/** 获得倒计时时间 */
const getLeavePageCountdown = (): number =&gt; {
  const timestamp = sessionStorage.getItem(LEAVE_PAGE_TIMESTAMP);
  const countdown = timestamp
    ? Number(timestamp) - new Date().getTime()
    : LEAVE_PAGE_COUNTDOWN;
  return countdown &gt; 0 ? countdown : 0;
};

/** 获得离开页面定时器 */
const getLeavePageTimeout = (): NodeJS.Timeout =&gt; {
  return setTimeout(() =&gt; leavePage(), getLeavePageCountdown());
};

export default () =&gt; {
  useEffect(() =&gt; {
    // 初始化时设置离开页面的时间
    sessionStorage.setItem(
      LEAVE_PAGE_TIMESTAMP,
      String(new Date().getTime() + LEAVE_PAGE_COUNTDOWN),
    );

    return () =&gt; {
      // 页面卸载时清除 SessionStorage
      sessionStorage.removeItem(LEAVE_PAGE_TIMESTAMP);
    };
  }, []);

  useEffect(() =&gt; {
    // 初始化时设置定时器
    let timer = getLeavePageTimeout();

    const onWindowVisibilityChange = () =&gt; {
      // 重新校准定时器
      if (document.visibilityState === &quot;visible&quot;) {
        // 清除已有定时器
        if (timer) clearTimeout(timer);
        // 设置新的定时器
        timer = getLeavePageTimeout();
      }
    };
    // 添加页面可见性变化监听器
    window.addEventListener(&quot;visibilitychange&quot;, onWindowVisibilityChange);

    return () =&gt; {
      // 页面卸载时清除定时器和监听器
      if (timer) clearTimeout(timer);
      window.removeEventListener(&quot;visibilitychange&quot;, onWindowVisibilityChange);
    };
  }, []);
};
```

上面的代码做了这些事情：

1. 当用户进入到页面时，在 SessionStorage 存储了应当执行业务需求的时间戳。
2. 启动一个定时器，在指定时间以后执行业务需求。
3. 启动一个监听器，当页面可见性发生改变，变为「可见」时，校准定时器：清除已有的定时器，然后启动一个新的定时器，在新的指定时间以后执行业务需求。其中，新的指定时间由存储的时间戳和当前的时间计算得来。

### 使用 SetInterval 轮训

哇噻，有够麻烦。换一种思路，使用轮训的实现方式，基于 SetInterval 不断比较当前的时间戳和应当离开页面的时间戳，若当前的时间戳大于应当离开页面的时间戳，执行离开页面的业务方法就好了。这种实现方式无需费力地重新校准时间，是一个讨巧的选择：

```tsx
import React, { useEffect } from &quot;react&quot;;

const LEAVE_PAGE_COUNTDOWN = 12 * 60 * 60 * 1000;

/** 离开页面的方法 */
const leavePage = () =&gt; {
  // ...离开当前页面的业务代码
};

export default () =&gt; {
  useEffect(() =&gt; {
    // 获取执行业务需求的时间戳
    const timestamp = new Date().getTime() + LEAVE_PAGE_COUNTDOWN;

    // 初始化时设置轮询器
    const timer = setInterval(() =&gt; {
      const now = new Date().getTime();
      if (now &gt;= timestamp) {
        leavePage();
      }
    }, 1000);

    return () =&gt; {
      // 页面卸载时清除轮询器
      if (timer) clearInterval(timer);
    };
  }, []);
};
```

### 似乎都不太优雅

但是，以上的实现都会导致一些体验上的问题：用户从后台切换到该页面时，若超过了 12 个小时，定时器或轮询器一运行，唰的一下子路由发生改变，用户会感到非常奇怪。

虽然加上一些 Notification 告知刚刚发生了啥会减少用户的不适，但终究我们还是会希望浏览器能**完全正常**地运行定时器方法（这是我们不想要被后台优化的功能），而不需要做这些带来额外开销且**不符合直觉**的适配（下一任程序员看到代码就头疼）。

此外，这样的做法并不能满足对**准确度要求高**的定时器需求。

基于以上需求，本文的主角 **Web Worker** 给出了现代、普适的解决方案。

## 生产实践的解决方案：使用 Web Worker

&gt; Web Worker 为 Web 内容在后台线程中运行脚本提供了一种简单的方法。线程可以执行任务而不干扰用户界面……一个 worker 是使用一个构造函数创建的一个对象运行一个命名的 JavaScript 文件：这个文件包含将在工作线程中运行的代码；workers 运行在另一个全局上下文中，不同于当前的 window。
&gt;
&gt; \-\- [MDN](https://developer.mozilla.org/zh-CN/docs/Web/API/Web_Workers_API/Using_web_workers)

Web Worker 能够为 JavaScript 创建多线程环境，允许将主线程中的任务分配给 Worker 线程处理，主线程和 Worker 线程之间可以进行通信。当遇到计算密集型或高延迟的任务时，常使用 Web Worker 进行性能优化：在 Worker 线程进行复杂的计算操作，进而避免主线程阻塞或卡死。

Worker 线程一旦创建成功，将**始终运行**，不会被主线程上的活动中断。但是，这也意味着 Worker 使用完毕后应当立即关闭，避免造成额外的系统开销。

只需要了解 Web Worker 的基本用法，就能很好地实现本次业务上的需求。将 `setTimeout()` 方法移动到 Worker 中去，只要浏览器不关闭，Worker 将保持运行的状态，在正确的时机向主线程返回离开页面的消息。

### Webpack 的方式

首先编写一个 Web Worker 脚本文件 `leavePage.worker.js`：

```js
let timer;

self.onmessage = (event) =&gt; {
  // console.log(&quot;Received message from main thread&quot;, event.data);

  if (timer) {
    clearTimeout(timer);
  }

  timer = setTimeout(() =&gt; {
    // 向主线程发出消息
    self.postMessage(`Time to leave page ${new Date()}`);
    // event.data 即离开页面的倒计时（ms）
  }, event.data);
};
```

如果您的项目基于 Webpack 4.x，那么需要配置 [`worker-loader`](https://github.com/webpack-contrib/worker-loader) 或 [`worker-plugin`](https://github.com/GoogleChromeLabs/worker-plugin) 等 loader 或插件才能通过 `new Worker(url)` 的方式正常引入 Web Worker。

在 React 代码里读取脚本文件，即可创建 Worker 线程并监听它返回的消息：

```tsx
import React, { useEffect } from &quot;react&quot;;

/** 离开页面的方法 */
const leavePage = () =&gt; {
  // ...离开当前页面的业务代码
};

export default () =&gt; {
  useEffect(() =&gt; {
    // 新建 Worker 线程
    // const worker = new Worker(&quot;path/to/leavePage.worker.js&quot;);

    // 向 Worker 线程发出消息，设定 12h 后返回消息
    worker.postMessage(12 * 60 * 60 * 1000);

    // 监听 Worker 返回的消息
    worker.onmessage = (event) =&gt; {
      // 一旦接收到消息，执行离开页面的业务代码
      console.log(&quot;Received message from worker thread&quot;, event.data);
      leavePage();
      // 一旦完成响应，关闭 Worker 线程
      worker.terminate();
    };

    return () =&gt; {
      // 页面卸载时关闭 Worker 线程
      worker.terminate();
    };
  }, []);
};
```

对于 Webpack 5.x 以上的项目，Webpack 已内置了对 Web Worker 的支持，可[查阅文档](https://webpack.js.org/guides/web-workers)使用。

### 动态加载的方式

如果不想在 Webpack 上加 loader 或插件，也可以考虑「动态」地加载脚本文件，这需要一点点小技巧。首先将 Worker 包含的具体内容以字符串的形式导出：

```js
// leavePage.worker.js
const leavePageWorker = `
var timer;
self.onmessage = function (event) {
  // ...
};
`;

export default leavePageWorker;
```

在主线程的代码里导入字符串并创建真正的 Worker 线程：

```ts
import LeavePageWorker from &quot;path/to/leavePage.worker.js&quot;;

const loadWebWorker = (code: string): Worker =&gt; {
  const blob = new Blob([&quot;(&quot; + code + &quot;)()&quot;]);
  return new Worker(URL.createObjectURL(blob));
};

const leavePageWorker = loadWebWorker(LeavePageWorker);
```

需注意的是，使用动态加载的方式意味着 Worker 的代码将不经 Webpack 而直接调用，所以应当使用兼容性更好的「古早 JavaScript 语法」，例如 `var` `function(){}` 等。

由于浏览器的 Content Security Policy (CSP) 策略，通过此方法创建 Worker 可能会失败，可以参考[此介绍](https://stackoverflow.com/questions/30280370/how-does-content-security-policy-csp-work)进行解决。

### Umi 项目的方式

根据 [Umi 文档](https://v3.umijs.org/config#workerloader)，对于 Umi 3.4.1+ 的项目，可以进行如下配置启用对 Web Worker 的支持：

```ts
// config.ts
export default defineConfig({
  workerLoader: {},
});
```

然而 Umi 文档并没有提 Web Worker 的引入方式，不过查阅 [Umi 源码](https://github.com/umijs/umi/blob/3.x/packages/bundler-webpack/src/getConfig/getConfig.ts#L364-L372)发现：

```ts
if (config.workerLoader) {
  webpackConfig.module
    .rule(&quot;worker&quot;)
    .test(/.*worker.(ts|js)/) // Web Worker 文件命名规则
    .use(&quot;worker-loader&quot;)
    .loader(require.resolve(&quot;@umijs/deps/compiled/worker-loader&quot;))
    .options(config.workerLoader);
}
```

可知 Umi 基于 `worker-loader`，将 `worker.ts` 或 `worker.js` 结尾的文件当作 Web Worker 处理。那么可以这样编写主线程的代码：

```tsx
import React, { useEffect } from &quot;react&quot;;
import LeavePageWorker from &quot;path/to/leavePage.worker.js&quot;;

export default () =&gt; {
  useEffect(() =&gt; {
    // 新建 Worker 线程
    const worker: Worker = new LeavePageWorker();

    // 像之前一样监听 worker 事件即可
    // ...

    return () =&gt; {
      // 别忘了使用完后关闭 Worker 线程
      worker.terminate();
    };
  }, []);
};
```

Worker 的编译和运行均在后台执行，这意味着即使出现报错也不会显式提醒您。您可以随时在开发者工具里找到编译得到的 Worker 的代码：

![在开发者工具中查看 Worker 源码](./js-webworker-settimeout/webworker-source.jpg)

对于 Umi 3.4.1 以前版本的项目，可以通过 [`chainWebpack`](https://v3.umijs.org/config#chainwebpack) 添加 `worker-loader` 或 `worker-plugin` 插件的支持。

Umi 4.x 内置 Webpack 5.x 作为默认 Bundler，因此[查阅文档](https://webpack.js.org/guides/web-workers)使用即可。

### 三方库的方式

如果不介意 Web Worker 编写是否原生（笔者从不介意！），更推荐选用封装了 Web Worker 能力的三方库，例如 [`alewin/useWorker`](https://github.com/alewin/useWorker) 和 [`developit/greenlet`](https://github.com/developit/greenlet/) 等。

它们降低了使用 Web Worker 的心智成本，使得调用 Web Worker 就像编写普通的 `async` 异步函数一样；重要的是，不必再担心引入 Web Worker 时带来的各种各样的奇怪问题（CDN 部署时，可能发生同源问题）。

以 `alewin/useWorker` 为例，可以这样改进前面的代码：

```tsx
import React, { useEffect } from &quot;react&quot;;
import { useWorker } from &quot;@koale/useworker&quot;;

/**
 * 休眠 @timeout 毫秒
 */
const setTimeoutAsync = (timeout: number) =&gt; {
  return new Promise&lt;void&gt;((resolve) =&gt;
    setTimeout(() =&gt; {
      resolve();
    }, timeout),
  );
};

export default () =&gt; {
  const [setTimeoutWorker, { kill: killSetTimeoutWorker }] =
    useWorker(setTimeoutAsync);

  useEffect(() =&gt; {
    const runLeavePageWorker = async () =&gt; {
      await setTimeoutWorker(12 * 60 * 60 * 1000);
      // ... 在此处执行离开页面的业务代码
    };

    runLeavePageWorker();

    return () =&gt; {
      killSetTimeoutWorker();
    };
  }, []);
};
```

## 参考文章

- [Web Worker 使用教程 - 阮一峰](https://www.ruanyifeng.com/blog/2018/07/web-worker.html)
- [记一次定时器问题的优雅解决](https://juejin.cn/post/6855583384375132174)
- [web worker onmessage - Uncaught SyntaxError: Unexpected token \&lt;](https://stackoverflow.com/questions/49171791/web-worker-onmessage-uncaught-syntaxerror-unexpected-token)
- [Combination of async function + await + setTimeout](https://stackoverflow.com/questions/33289726/combination-of-async-function-await-settimeout)
</content:original-text><content:updated-at>2022-09-22T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[连接到 Windows 端的 PostgreSQL 数据库]]></title><description><![CDATA[假设，您身边有两台电脑，一台打算用来做 PostgreSQL 数据库服务器，一台用来做客户端。服务器上的 PostgreSQL 14 安装在  目录下，数据库文件保存在  目录，欲访问的数据库名为 ，访问数据库的用户为 。 配置 

编辑数据库配置文件 ，设置监听的远程连接地址。将  项的值设置为 ，如下所示：

配置 

编辑数据库客户端认证配置文件 ，设置允许连接到数据库的客户端 IP 地址。

该…]]></description><link>https://blog.towind.fun/posts/connect-with-pgsql</link><guid isPermaLink="false">connect-with-pgsql</guid><category><![CDATA[后端开发]]></category><pubDate>Sat, 12 Feb 2022 00:00:00 GMT</pubDate><content:original-text>
假设，您身边有两台电脑，一台打算用来做 PostgreSQL 数据库服务器，一台用来做客户端。服务器上的 PostgreSQL 14 安装在 `C:\Program Files\PostgreSQL\14` 目录下，数据库文件保存在 `C:\Program Files\PostgreSQL\14\data` 目录，欲访问的数据库名为 `db_name`，访问数据库的用户为 `db_user`。

## 配置 `postgresql.conf`

编辑数据库配置文件 `C:\Program Files\PostgreSQL\14\data\postgresql.conf`，设置监听的远程连接地址。将 `listen_addresses` 项的值设置为 `*`，如下所示：

```conf
# - Connection Settings -

listen_addresses = &apos;*&apos;  # what IP address(es) to listen on
```

## 配置 `pg_hba.conf`

编辑数据库客户端认证配置文件 `C:\Program Files\PostgreSQL\14\data\pg_hba.conf`，设置允许连接到数据库的客户端 IP 地址。

该文件的注释处已经给出了各种配置的详细说明。这里，我们将重点放在对允许的远程连接地址的配置上。

`pg_hba.conf` 对 ADDRESS 地址的解析基于 [CIDR 无类别域间路由](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing)。

那么，对于 IPv4 地址，有如下五种典型情况：

| ADDRESS           | 说明                                           |
| ----------------- | ---------------------------------------------- |
| 210.41.111.111/32 | 仅允许 IP 地址为 210.41.111.111 的远程连接请求 |
| 210.41.111.0/24   | 允许 IP 地址为 210.41.111.\* 的远程连接请求    |
| 210.41.0.0/16     | 允许 IP 地址为 210.41.\*.\* 的远程连接请求     |
| 210.0.0.0/8       | 允许 IP 地址为 210.\*.\*.\* 的远程连接请求     |
| 0.0.0.0/0         | 允许所有 IP 地址的远程连接请求                 |

那么，如果我们的客户端的 IP 地址为 `210.41.112.112`，可以添加条目如下：

```conf
# TYPE  DATABASE        USER            ADDRESS                 METHOD
host    db_name         db_user         210.41.112.112/32       scram-sha-256
# host    db_name         db_user         210.41.112.0/24         scram-sha-256
# host    db_name         db_user         210.41.0.0/16           scram-sha-256
# host    db_name         db_user         210.0.0.0/8             scram-sha-256
# host    db_name         db_user         0.0.0.0/0               scram-sha-256
```

更小的范围，意味着更高的安全性。

## 重启数据库服务

为了让配置生效，我们需要重启启动 PostgreSQL 数据库。

在 Windows 端，PostgreSQL 作为一个服务运行。按 Windows 键，搜索 `service`，进入服务应用，找到 `postgresql-x64-*` 服务，右键重新启动即可。

![重启 PostgreSQL 数据库](./connect-with-pgsql/restart-pgsql.png)

## 连接到数据库

### 同一路由器下

配置 `pg_hba.conf`，允许相同路由器下其它主机的连接请求。例如，允许 IP 地址为 `192.168.0.0` 至 `192.168.255.255` 的连接请求：

```conf
# TYPE  DATABASE        USER            ADDRESS                 METHOD
host    db_name         db_user         192.168.0.0/16          scram-sha-256
```

重新启动数据库服务。

假设服务器的内网 IP 地址为 `192.168.1.104`，数据库监听端口为 `5432`。那么，当我们通过客户端连接远程数据库时，主机名应填写 `192.168.1.104`，端口应填写 `5432`，这样，我们就能顺利访问到服务器上的数据库了。

### 远程连接

假如您的用作客户端和服务器的电脑分隔两地，现在想要远程连接服务器上的数据库，该怎么办呢？下面是我常用的办法。

首先，需要获取服务器的公网 IP 地址，可以在[这里](https://www.ip138.com/)查询。这里假设服务器通过路由器与公网连接。

接着，配置路由器的**虚拟服务器**。可以通过如下步骤实现：

1. 将服务器的内网 IP 地址与它的硬件 MAC 地址绑定。别忘了在服务器的网络属性处，关闭随机硬件地址功能。
2. 配置虚拟服务器，映射路由器的外部端口为服务器的数据库端口。

最后，修改 `pg_hba.conf`，根据客户端的公网 IP 地址进行配置。如果客户端的网络环境经常变化，尽管不安全，但也可以考虑设置允许所有 IP 地址进行连接。

```conf
# TYPE  DATABASE        USER            ADDRESS                 METHOD
host    db_name         db_user         0.0.0.0/0               scram-sha-256
host    db_name         db_user         ::/0                    scram-sha-256
```

重新启动数据库服务。

例如，路由器的公网 IP 地址为 `210.41.111.111`，服务器的内网 IP 地址为 `192.168.1.104`，虚拟服务器外部端口为 `22212`，数据库监听端口为 `5432`。那么，当我们通过客户端连接远程数据库时，主机名应填写 `210.41.111.111`，端口应填写 `22212`，这样，我们就能顺利访问到服务器上的数据库了。
</content:original-text><content:updated-at>2022-02-12T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[这位客官，要来一张我珍藏许久的图片吗]]></title><description><![CDATA[笔者自高中到现在，游走于 Pixiv 若干载，不慎收藏了许多名家雅作。 独乐乐不如众乐乐！笔者想做一个 web 页面来随机访问我的收藏，不过在此之前，可以先实现服务端上的内容。再之后做网页时，不过是简单的读取数据库罢了！

最初，笔者以为得将我的库存全部放到服务器上项目中去，然后随机访问其中的图片实现功能，但这样做很难得同步，遂搁置。不过，笔者在最近发现有一个 Pixiv 图片代理网站 可以快速下载…]]></description><link>https://blog.towind.fun/posts/random-get-me-a-picture</link><guid isPermaLink="false">random-get-me-a-picture</guid><category><![CDATA[后端开发]]></category><pubDate>Thu, 13 Jan 2022 00:00:00 GMT</pubDate><content:original-text>
笔者自高中到现在，游走于 Pixiv 若干载，不慎收藏了许多名家雅作。

独乐乐不如众乐乐！笔者想做一个 web 页面来随机访问我的收藏，不过在此之前，可以先实现服务端上的内容。再之后做网页时，不过是简单的读取数据库罢了！

最初，笔者以为得将我的库存全部放到服务器上项目中去，然后随机访问其中的图片实现功能，但这样做很难得同步，遂搁置。不过，笔者在最近发现有一个 [Pixiv 图片代理网站](https://pixiv.re/) 可以快速下载到图片，大喜，于是开始了这个小工程。

![请求需包含 Referer](./random-get-me-a-picture/pixiv-cat.png)

实现此功能分为两个阶段：一，为本地的图片生成数据库索引条目。二，开发 Telegram Bot 接口，随机从数据库索引中获取一张图片转发给聊天。

## 为本地的 Pixiv 图片建立索引

### 初始化数据库 Pixiv 图片索引信息

如果使用 Pixiv 图片代理的方法，只需要将 Artwork 的基本信息上传给我们的数据库即可。

根据[官方文档](https://core.telegram.org/bots/api#sending-files)，当发送图片文件时，Telegram API 对图片的大小有限制：直接使用 HTTP URL 的方式不超过 5 MB，服务器上传图片的方式不超过 10 MB（以[文件的格式](https://core.telegram.org/bots/api#senddocument)发送时，不超过 50 MB，本文不采用文件的格式发送）。因此，在设计数据库时，还需要考虑图片的大小。

设计 Sequenlize 数据库模型 `ServicePixivCollection.js` 如下：

```js
const { DataTypes } = require(&quot;sequelize&quot;);

module.exports = {
  id: {
    type: DataTypes.INTEGER,
    autoIncrement: true,
    primaryKey: true,
  },
  // Pixiv artwork ID. Example: 95400283 for for 95400283_p${picIndex}.${picType}
  picId: {
    type: DataTypes.INTEGER,
    allowNull: false,
  },
  // Artwork index. Example: 1 for ${pixivId}_p1.${picType}
  picIndex: {
    type: DataTypes.INTEGER,
    allowNull: false,
  },
  // Artwork suffix type. Example: jpg for ${pixivId}_p${picIndex}.jpg
  picType: {
    type: DataTypes.TEXT,
    allowNull: false,
  },
  // Artwork size, MB
  picSize: {
    type: DataTypes.FLOAT,
    allowNull: false,
  },
  // Artwork save date
  picCreatedAt: {
    type: DataTypes.DATE,
    allowNull: false,
  },
};
```

如采用最小实现，也可以将上面的 `picId`, `picIndex` 和 `picType` 合并为一个数据项，例如 `picName`，直接保存图片的名字。笔者考虑到后续可能会增添新的功能，于是将它们单独拎出来储存。

处理不同路径下的图片时，可能需要保存执行的情况。例如 A 目录在当前时间点进行维护，获取了所有的图片，下次读取 A 目录时，应当从上次维护的时间开始获取最新的图片。现在新加了 B 目录，如果从上次维护 A 的时间点开始读取图片的话，在时间点之前的图片将无法上传。因此，针对不同文件夹的维护，可以分别建立一个单独的数据项。

设计数据库模型 `ServiceProcess.js` 如下：

```js
const { DataTypes } = require(&quot;sequelize&quot;);

module.exports = {
  id: {
    type: DataTypes.INTEGER,
    autoIncrement: true,
    primaryKey: true,
  },
  serviceId: {
    type: DataTypes.UUID,
    defaultValue: DataTypes.UUIDV4,
    allowNull: false,
  },
  serviceName: {
    type: DataTypes.TEXT,
    allowNull: false,
  },
  serviceConfig: {
    type: DataTypes.TEXT,
  },
  serviceSharedData: {
    type: DataTypes.TEXT,
  },
  lastExecAt: {
    type: DataTypes.DATE,
  },
  haveExecTime: {
    type: DataTypes.INTEGER,
    allowNull: false,
    defaultValue: 0,
  },
};
```

为 Sequelize 添加该模型，向 `sequelize.js` 添加如下代码：

```js
const servicePixivCollectionModel = require(&quot;path/to/ServicePixivCollection&quot;);
const serviceProcessModel = require(&quot;path/to/ServiceProcess&quot;);

sequelize.define(&quot;ServicePixivCollection&quot;, servicePixivCollectionModel);
sequelize.define(&quot;ServiceProcess&quot;, serviceProcessModel);
```

配置扫描图片的间隔时间和本地图片路径，编写 `config.js` 如下:

```js
module.exports = {
  pixiv: {
    randomGetFromCollection: {
      duration: 3600,
      path: [&quot;C:\\path\\to\\collection&quot;],
    },
  },
};
```

扫描指定目录的图片文件，图片文件名称应满足从 Pixiv 下载图片的名称格式。例如：`95400283_p0.jpg`，`95400283` 为数据库中的 `picId`，`_p0` 的 `0` 为 `picIndex`，`.jpg` 中的 `jpg` 为 `picType`。编写生成图片索引的代码 `pixiv.js` 如下：

```js
const { readdir, stat } = require(&quot;fs/promises&quot;);
const path = require(&quot;path&quot;);

const config = require(&quot;path/to/config&quot;).pixiv;

const Sequelize = require(&quot;path/to/sequelize&quot;);

const generateCollectionIndex = async function () {
  const serviceName = &quot;Generate Collection Index&quot;;
  const bToMB = 1024 * 1024;

  // File size with decimal places
  const fileSizeReservedDecimalPlace = 3;
  const fileSizeReservedDecimalNum = 10 ** fileSizeReservedDecimalPlace;

  const sequelize = await Sequelize();
  const ServicePixivCollection = sequelize.models.ServicePixivCollection;
  const ServiceProcess = sequelize.models.ServiceProcess;

  let collectionPaths = config.generateCollectionIndex.path;
  if (!Array.isArray(collectionPaths)) {
    collectionPaths = [collectionPaths];
  }

  // Get filenames in collection paths
  let allFiles = [];
  for (const collectionPath of collectionPaths) {
    let files = [];
    files = files.concat(await readdir(collectionPath));

    // Only keep files with Pixiv naming style
    const reg = /^\d+_p\d+.(jpg|png|gif)$/;
    files = files.filter((filename) =&gt; {
      return reg.test(filename);
    });

    // Get file stat and resolve file info
    for (let i = 0; i &lt; files.length; i++) {
      const filename = files[i];
      const filePath = path.join(collectionPath, filename);

      const picIdSplitArr = filename.split(&quot;_p&quot;);
      const picId = Number(picIdSplitArr[0]);

      const picIndexSplitArr = picIdSplitArr[1].split(&quot;.&quot;);
      const picIndex = Number(picIndexSplitArr[0]);

      const picType = picIndexSplitArr[1];

      const picStat = await stat(filePath);
      const picSize =
        Math.floor((picStat.size / bToMB) * fileSizeReservedDecimalNum) /
        fileSizeReservedDecimalNum; // MB
      const picCreatedAt = picStat.mtimeMs; // ms

      files[i] = {
        picName: filename,
        picId,
        picIndex,
        picType,
        picSize,
        picCreatedAt,
      };
    }

    // Only keep files that recently saved
    const serviceProcess = await ServiceProcess.findOne({
      where: { serviceName, serviceConfig: collectionPath },
    });
    if (serviceProcess) {
      // Only update or create Pixiv artwork that saved after last time this service is done
      let lastUpdateIndexTime = serviceProcess.dataValues.lastExecAt;
      if (lastUpdateIndexTime) {
        lastUpdateIndexTime = new Date(lastUpdateIndexTime).getTime();
        files = files.filter((pic) =&gt; {
          return pic.picCreatedAt &gt; lastUpdateIndexTime;
        });
      }
    } else {
      await ServiceProcess.create({
        serviceName,
        serviceConfig: collectionPath,
      });
    }

    allFiles = allFiles.concat(files);
  }

  const updateIndexAt = new Date().toISOString();

  // Update or create pic index
  for (const picFile of allFiles) {
    // 注意：此处的 updateOrCreate() 为笔者自定义的方法
    // 其作用为：当存在 item 时，更新 item；不存在时，创建 item
    await sequelize.updateOrCreate(
      ServicePixivCollection,
      {
        picId: picFile.picId,
        picIndex: picFile.picIndex,
      },
      picFile,
    );
  }

  // Update service process record
  ServiceProcess.update(
    {
      lastExecAt: updateIndexAt,
    },
    {
      where: { serviceName },
    },
  );
  ServiceProcess.increment(&quot;haveExecTime&quot;, { where: { serviceName } });
};

module.exports = {
  generateCollectionIndex,
};
```

设置定时扫描图片，基于 `toad-scheduler` 库编写服务代码如下：

```js
const {
  ToadScheduler,
  SimpleIntervalJob,
  AsyncTask,
} = require(&quot;toad-scheduler&quot;);

const pixivTask = require(&quot;path/to/pixiv&quot;);
const config = require(&quot;path/to/config&quot;).pixiv;

const initService = async function () {
  const taskGenerateCollectionIndex = new AsyncTask(
    &quot;Generate Pixiv Collection Index&quot;,
    async () =&gt; {
      await pixivTask.generateCollectionIndex();
    },
    (error) =&gt; {
      console.error(error);
    },
  );
  const jobGenerateCollectionIndex = new SimpleIntervalJob(
    {
      seconds: config.generateCollectionIndex.duration,
      runImmediately: true,
    },
    taskGenerateCollectionIndex,
  );

  const scheduler = new ToadScheduler();
  scheduler.addSimpleIntervalJob(jobGenerateCollectionIndex);
};

module.exports = initService;
```

程序启动时，运行 `initService()` 即可。服务器将自动读取指定目录下的 Pixiv 图片文件，并在数据库中建立索引。[下一步](#随机获取数据库中的一个-pixiv-图片访问地址)，我们将随机从数据库中读取一张作品的信息。

### 思路：爬取 Pixiv 图片的源文件地址

如果使用亲自获取图片并转发的方法，除了前面需要初始化数据库的图片索引信息外，还需要构建 Axios 请求，爬取网页源代码，从中读取 Artwork 的下载链接等信息，再上传到数据库。

例如，对于下载 [`95400283_p0.jpg`](https://www.pixiv.net/artworks/95400283)，最少需要获取其源文件链接 `https://i.pximg.net/img-original/img/2022/01/09/07/27/17/95400283_p0.jpg` 中的 `/2022/01/09/07/27/17` 部分，并上传到数据库中。

更多的，在爬取源代码时，可以记录下图片的作者信息，图片是否可以在工作时安全观看（登录后才能爬取此类内容，可能需要在请求时添加个人账户信息）等，对日后处理展示内容大有裨益。

处理网页源代码时，可以使用 [cheerio](https://github.com/cheeriojs/cheerio) 库增加效率。

## 为 Telegram Bot 添加随机获取 Pixiv 图片的接口

### 随机获取数据库中的一个 Pixiv 图片访问地址

编写 `randomGetPixivCollection.js` 代码如下：

```js
const Sequelize = require(&quot;path/to/sequelize&quot;);

const randomGetPixivCollection = async function () {
  try {
    const sequelize = await Sequelize();
    const ServicePixivCollection = sequelize.models.ServicePixivCollection;

    // Gain the total number of Pixiv artworks
    const artworksCount = await ServicePixivCollection.count();

    // Generate a random value
    const randomArtworkId = Math.floor(Math.random() * artworksCount) + 1;

    // Get random artwork
    const artwork = await ServicePixivCollection.findOne({
      where: { id: randomArtworkId },
    });

    // Resolve artwork object
    const data = artwork.dataValues;

    const picId = data.picId;
    const picIndex = data.picIndex;
    const picType = data.picType;

    data.picName = `${picId}_p${picIndex}.${picType}`;
    data.picNameMD = `${picId}\\_p${picIndex}\\.${picType}`;
    data.picUrl = `https://www.pixiv.net/artworks/${picId}`;

    let picProxyUrlParam;
    if (picIndex &gt; 0) {
      picProxyUrlParam = `${picId}-${picIndex + 1}.${picType}`;
    } else {
      picProxyUrlParam = `${picId}.${picType}`;
    }
    data.picProxyUrl = `https://pixiv.cat/${picProxyUrlParam}`;

    return {
      ok: true,
      data,
      error: undefined,
    };
  } catch (error) {
    return {
      ok: false,
      data: undefined,
      error,
    };
  }
};

module.exports = randomGetPixivCollection;
```

根据 Pixiv.cat 网站的使用说明，对单张图片的 Pixiv 作品，应访问 `https://pixiv.cat/${picId}.${picType}`。而对于多张图片的 Pixiv 作品（漫画），应访问 `https://pixiv.cat/${picId}-${picIndex + 1}.${picType}`。

可能会影响体验，需要改进的地方是：当作品名为 `${picId}_p0.${picType}` 时，我们不知道该作品是否为单张图片，还是漫画作品，无法正确判断应该访问的 URL 链接。[在后面](#直接使用-pixiv-图片代理)，我们将针对此情况做处理。

### 添加 Telegram Bot 命令

接下来为 Bot 添加指令，以调用随机获取图片的接口。同样，这里给出使用 Pixiv 图片代理的具体实现，以及亲自获取图片并转发的可能思路。

#### 直接使用 Pixiv 图片代理

注意，使用这种方式上传的图片**不超过 5 MB**。

编写 Bot 的配置如下：

```js
bot.onText(/\/random_pixiv/, async (msg) =&gt; {
  const chatId = msg.chat.id;

  const res = await randomGetPixivCollection();
  if (res.ok === true) {
    // Send placeholder message
    const placeholderMessage = await bot.sendMessage(
      chatId,
      `Geeeeting a random Pixiv artwork ...`,
    );

    const data = res.data;
    const {
      id,
      picNameMD,
      picUrl,
      picSize,
      picProxyUrl,
      picId,
      picIndex,
      picType,
    } = data;

    const caption = `[source](${picUrl})`;

    let msgReplied = false;

    if (picSize &lt; 5) {
      // Artwork size is smaller than 5 MB, send photo message
      const sendPhotoOptions = {
        caption,
        parse_mode: &quot;MarkdownV2&quot;,
        disable_web_page_preview: true,
      };

      try {
        await bot.sendPhoto(chatId, picProxyUrl, sendPhotoOptions);

        msgReplied = true;
      } catch (err) {
        if (picIndex == 0) {
          try {
            // Comic mode artwork with index=0 may send failed
            // Use comic mode url instead
            const picProxyUrl = `https://pixiv.cat/${picId}-1.${picType}`;
            await bot.sendPhoto(chatId, picProxyUrl, sendPhotoOptions);

            msgReplied = true;
          } catch (err) {
            //
          }
        }
      }
    }

    // Artwork size is not smaller than 5 MB or send failed again,
    // send caption message
    if (!msgReplied) {
      await bot.sendMessage(chatId, caption, {
        parse_mode: &quot;MarkdownV2&quot;,
        disable_web_page_preview: false,
      });
    }

    // Remove placeholder message
    bot.deleteMessage(chatId, placeholderMessage.message_id);
  } else {
    bot.sendMessage(
      chatId,
      &quot;Get random pixiv artwork failed. You may try to call it again later!&quot;,
    );
  }
});
```

在上面这段代码中，当以 `bot.sendPhoto()` 的方法尝试发送 5 MB 以下的图片失败时，首先重新构建请求 URL 为 `https://pixiv.cat/${data.picId}-1.${data.picType}`，再次进行发送。如果还是失败（可能图片被作者删除），则发送简单的链接文本。由此，解决了前面提到的无法判断作品是否为漫画作品的问题。

与 Telegram 上的机器人对话，发送命令 `\random_pixiv`，结果如下：

![Bot service](./random-get-me-a-picture/bot-service.png)

#### 思路：获取 Pixiv 图片并转发

注意，使用这种方式上传的图片**不超过 10 MB**。

我们无法直接在 `bot.sendPhoto()` 方法中使用 Pixiv 源站图片的获取链接，这是因为 Pixiv 设置了反爬虫机制，只接收请求头的 Referer 包含 `https://www.pixiv.net/` 的请求。

![请求需包含 Referer](./random-get-me-a-picture/pixiv-request-referer.png)

因此，我们需要在自己的服务器上构造 Axios 请求，设置 Referer 请求头，然后发送请求向 Pixiv 服务器获取图片，再将图片转为 `multipart/form-data` 格式发送给 Telegram 会话。

## 为 Koa 添加随机获取 Pixiv 图片的接口

Koa 应用程序本身也提供了路由功能，在这里可以很轻松地将获取随机 Pixiv 图片的服务对接到 Koa 上去。

编写路由文件如下：

```js
const router = require(&quot;koa-router&quot;)();

const randomGetPixivCollection = require(&quot;path/to/randomGetPixivCollection&quot;);

router.get(&quot;/random&quot;, async function (ctx) {
  const res = await randomGetPixivCollection();
  if (res.ok === true) {
    ctx.redirect(res.data.picProxyUrl);
  } else {
    ctx.body = &quot;Get random Pixiv artwork failed.&quot;;
  }
});

module.exports = router;
```

然而，这里并没有解决前面提到的漫画作品问题，留待用户在跳转后自行操作。

## 参考文章

- [爬取 pixiv 每日推荐](https://blog.csdn.net/weixin_45826022/article/details/109406389)
</content:original-text><content:updated-at>2022-01-15T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[从零开始使用 Telegram Bot]]></title><description><![CDATA[本文基于 Koa 从零开始搭建一个简单的 Telegram Bot 应用服务，支持获取 Github Issues 的评论并转发到 Telegram 频道，帮助笔者更好地将捣玩 Telegram！ 时间推移至 2024 年，笔者现在更建议使用 Bun 开发应用服务，开箱即用的高性能服务以及完备的 TypeScript 支持，能大大提升开发体验。下面为撰写于 2022 年初的原文。

本文假设您已…]]></description><link>https://blog.towind.fun/posts/start-telegram-bot</link><guid isPermaLink="false">start-telegram-bot</guid><category><![CDATA[后端开发]]></category><pubDate>Sun, 09 Jan 2022 00:00:00 GMT</pubDate><content:original-text>
本文基于 Koa 从零开始搭建一个简单的 Telegram Bot 应用服务，支持获取 Github Issues 的评论并转发到 Telegram 频道，帮助笔者更好地将捣玩 Telegram！

&gt; 时间推移至 2024 年，笔者现在更建议使用 [Bun](https://bun.sh) 开发应用服务，开箱即用的高性能服务以及完备的 TypeScript 支持，能大大提升开发体验。下面为撰写于 2022 年初的原文。

本文假设您已对 Node.js 和 Koa 有一定的了解。

## 初始化 Koa 项目

[Koa](https://koajs.com/) 是为 Node.js 设计的下一代 Web 框架，其幕后开发者主要来自知名的 Express 团队。

尽管使用 [koa-generator](https://github.com/i5ting/koa-generator) 来初始化 Koa 项目是一个不错的选择，但笔者还是喜欢从头开始的感觉。

那么首先，新建文件夹并进入，使用 `npm init` 初始化 `package.json`。

安装必要的依赖：

```bash
npm install koa koa-router koa-bodyparser dotenv
```

安装开发依赖：

```bash
npm install -D nodemon
```

安装服务器部署时使用的依赖：

```bash
npm install pm2
```

为了简便，这里笔者使用了别人进行封装后的 Telegram Bot API 库 [node-telegram-bot-api](https://github.com/yagop/node-telegram-bot-api)：

```bash
npm install node-telegram-bot-api
```

规划项目目录结构大致如下：

```plaintext
.
├── .env
├── app.js
├── package.json
├── bin
│   └── www.js
└── routes
│   └── index.js
└── service
    ├── bot.js
    └── index.js
```

其中，`.env` 为环境配置文件，包括 Telegram Bot Token 在内的信息在此处配置；`bin/www.js` 为项目启动时执行的文件，这意味着在配置脚本命令时，应当使用 `nodemon bin/www` 及 `pm2 start bin/www`；`routes` 目录为路由目录，用来存放可调用的接口；最后，`service` 目录为服务目录，在这里连接 Bot 和数据库，并执行定时任务。

## 连接到 Telegram Bot

### 创建新的 Bot

首先，通过在 Telegram 上与 [BotFather](https://core.telegram.org/bots#6-botfather) 交互，创建一个新的 Telegram Bot。

![create a new bot](./start-telegram-bot/create-bot.png)

记录下当中的 **HTTP API** 的值即 Telegram Bot Token，作为项目的环境变量保存，切勿上传到远程代码仓库中。

```js
const token = process.env.TELEGRAM_BOT_TOKEN;
```

### 与 Bot 建立连接

我们的项目可能无法直接访问到 Telegram 的服务器，可以使用 [**SOCKS5 代理**](https://github.com/mattcg/socks5-https-client)解决这个问题：

```js
const TelegramBot = require(&quot;node-telegram-bot-api&quot;);
const proxySocks5Agent = require(&quot;socks5-https-client/lib/Agent&quot;);

requestOptions = {
  agentClass: proxySocks5Agent,
  agentOptions: {
    socksHost: process.env.PROXY_SOCKS5_HOST,
    socksPort: process.env.PROXY_SOCKS5_PORT,
    socksUsername: process.env.PROXY_SOCKS5_USERNAME,
    socksPassword: process.env.PROXY_SOCKS5_PASSWORD,
  },
};

const bot = new TelegramBot(token, {
  polling: true,
  request: requestOptions,
});
```

如何 SOCKS5 工作不正常（[这是](https://github.com/yagop/node-telegram-bot-api/issues/696#issuecomment-613023532)一个可能的原因），也可以尝试使用 **HTTP 代理**：

```js
const TelegramBot = require(&quot;node-telegram-bot-api&quot;);

requestOptions = {
  proxy: process.env.PROXY_HTTP,
};

const bot = new TelegramBot(token, {
  polling: true,
  request: requestOptions,
});
```

对 Bot 进行测试，添加如下代码：

```js
bot.onText(/\/start/, (msg) =&gt; {
  // console.log(msg)
  bot.sendMessage(msg.chat.id, &quot;Hi, this is Telly Bot!&quot;);
});
```

打开 Telegram，对 Bot 发送 `/start`，看看是否会得到 `Hi, this is Telly Bot!` 的回应。

### 使用网络钩子与 Bot 交互

Telegram Bot 可以通过轮询（polling）和网络钩子（webhook）两种不同的方式来获取用户发送的消息，在前面的代码中，我们使用的是轮询的方式。

轮询的方式无需额外的配置，更适合本地快速进行开发测试；而网络钩子的方式更适合项目部署。那么，一个健全的 Telegram Bot 应当使用**网络钩子**的方式来实现。

为了接收用户对 Telegram Bot 发送的消息，在网络钩子的方式中，我们需要一个 **HTTPS 协议的公网地址**，除了直接使用自己的服务器，还可以怎么办呢？别急，有 [ngrok](https://ngrok.com/) 为我们排忧解难：它是一款反向代理工具，可以将本地的地址映射到公网上去。

![ngrok](./start-telegram-bot/ngrok.png)

如上图所示，当 ngrok 运行时，Telegram Bot 发向 `https://a75b-182-141-75-13.ngrok.io` 的请求，将转发给运行在本地 `http://localhost:4000` 上的程序。

这样，只需要同时运行我们的项目和 ngrok，我们就可以正常地接收到信息并进行处理了。修改连接 Bot 的代码如下：

```js
const bot = new TelegramBot(token, {
  request: requestOptions,
});

bot.setWebHook(`${process.env.WEBHOOK_HOST}/bot${token}`);

globalThis.bot = bot;
```

现在，Telegram 上机器人收到的消息会立即发送给我们的服务器。最后，在服务器需要处理接收到的 POST 类型请求 `/bot${TELEGRAM_BOT_TOKEN}`，告知 Telegram 我们已经收到新的消息了。可以在 `routes/index.js` 中添加代码如下：

```js
router.post(`bot${token}`, (ctx) =&gt; {
  globalThis.bot.processUpdate(ctx.request.body);
  ctx.status = 200;
});
```

需要补充的是，通过上面代码中 Bot API 库提供的 [`processUpdate`](https://github.com/yagop/node-telegram-bot-api/blob/master/doc/api.md#telegrambotprocessupdateupdate) 方法，可以对接收到的消息进行相应的处理，触发正确的事件并执行回调方法。

现在，我们的机器人将不再笨拙地轮询 Telegram 服务器，查看是否有未处理的消息，而是静静等待 Telegram 服务器发送过来的请求。

## 转发 Github Issues 到 Telegram 频道

[Chen 先生](https://billc.io/)自己的 Telegram 频道会定时发送他更新的推文，笔者也想整一个，最简单的实现方式是申请一个 Twitter 开发者账号，定时调用 API 获取最新推文信息即可 —— 但是没能申请到。暂退一步，先把笔者在 Github Issues 上的碎碎念同步给频道吧。

接下来的内容假设您已对 PostgreSQL 和数据库 ORM 工具有一定的了解。

### 连接到数据库

同步功能需要数据库的支持，当然也为了未来更多功能的实现，在这里，先与本机的数据库建立连接。以 [PostgreSQL](https://www.enterprisedb.com/downloads/postgres-postgresql-downloads) 为例，首先安装 [node-postgres](https://github.com/brianc/node-postgres) 库：

```bash
npm install pg
```

新建文件 `config.js` 来存储连接到数据库的配置：

```js
const config = {
  database: {
    postgresql: {
      host: &quot;localhost&quot;,
      port: 5432,
      database: &quot;telly_bot_db&quot;,
      user: &quot;telly_bot_db_user&quot;,
      password: &quot;telly_bot_db_pwd&quot;,
      timezone: &quot;+08:00&quot;,
    },
  },
};

module.exports = config;
```

### 使用 ORM 管理数据库

通过 ORM 工具来对数据库进行管理与查询，可以避免手动运维的窘境。这里选用 [Sequelize](https://github.com/sequelize/sequelize) 库，安装必要的依赖：

```bash
npm install sequelize pg-hstore
```

修改 `db/index.js` 代码如下：

```js
const { Sequelize } = require(&quot;sequelize&quot;);
const pgsqlConfig = require(&quot;../config&quot;).database.postgresql;
const options = {
  timezone: pgsqlConfig.timezone || &quot;+08:00&quot;,
};
const sequelize = new Sequelize(
  `postgres://${pgsqlConfig.user}:${pgsqlConfig.password}@${pgsqlConfig.host}:${pgsqlConfig.port}/${pgsqlConfig.database}`,
  options,
);

(async () =&gt; {
  try {
    await sequelize.authenticate();
    console.log(
      `Connection with ${pgsqlConfig.database} has been established successfully.`,
    );
    await sequelize.sync({ alter: true });
    console.log(&quot;All models were synchronized successfully.&quot;);
  } catch (error) {
    console.error(
      `Unable to connect to the database ${pgsqlConfig.database}:`,
      error,
    );
  }
})();

module.exports = sequelize;
```

为了实现自动转发 Github Issues 中的评论，我们需要一张数据表来存储上一次转发的评论（或编辑记录）的**最后更新日期**（`lastUpdateCommentAt`)。这样，下一次执行任务时，只需要查看该日期之后是否有新的评论（或编辑记录）就可以了。对于每一个 Issue，都会在该表中创建一条数据。为 Sequelize 添加模型 `db/model/ServiceGithubIssueComment.js` 如下：

```js
const { DataTypes } = require(&quot;sequelize&quot;);

module.exports = {
  // The ID of the forwarding Github Issue service
  id: {
    type: DataTypes.INTEGER,
    autoIncrement: true,
    primaryKey: true,
  },
  // Url of Github Issue. Example: ${USERNAME}/${REPOSITORY}/issues/${ISSUE_NUM}
  issueUrl: {
    type: DataTypes.TEXT,
    allowNull: false,
  },
  // Only forward the comments of these users, empty means forward all
  issueUserId: {
    type: DataTypes.ARRAY(DataTypes.TEXT),
  },
  // The ID of the channel to which the comment was forwarded
  forwardChannelId: {
    type: DataTypes.TEXT,
    allowNull: false,
  },
  // The last date the issue comments were updated
  lastUpdateCommentAt: {
    type: DataTypes.DATE,
  },
  // Date the service was last run
  lastExecServiceAt: {
    type: DataTypes.DATE,
  },
};
```

一个模型将成为数据库中的一张数据表。向 `db/index.js` 中添加如下代码：

```js
const serviceGithubIssueCommentModel = require(&quot;./model/ServiceGithubIssueComment&quot;);
sequelize.define(&quot;ServiceGithubIssueComment&quot;, serviceGithubIssueCommentModel);
// sequelize.sync({ alter: true })
```

注意，将模型绑定给 sequelize 对象的操作 `sequelize.define()` 需要放在 `sequelize.sync()` 方法之前。

当 `sequelize.define()` 执行完成后，我们可以随时使用 `sequelize.models.ServiceGithubIssueComment` 来获取模型实例。通过模型实例，我们就可以在对应的数据表中执行各种 SQL 查询语句了。

### 获取指定 Issue 中的最新评论

Github REST API 文档推荐使用 [@octokit/core](https://github.com/octokit/core.js) 库来执行请求：

```bash
npm install @octokit/core
```

向 `config.js` 中添加相应的配置。以获取笔者的[碎碎念](https://github.com/LolipopJ/LolipopJ/issues/2)为例：

```js
const config = {
  github: {
    forwardIssueComment: {
      duration: 3600,
      task: [
        {
          owner: &quot;LolipopJ&quot;,
          repo: &quot;LolipopJ&quot;,
          issueNumber: 2,
          issueUserId: [42314340],
          forwardChannelId: &quot;@lolipop_thoughts&quot;,
          since: &quot;2022-01-01T00:00:00.000Z&quot;,
        },
      ],
    },
  },
};
```

其中，`duration` 为两次执行期间间隔的时间（秒）。此外，配置中存在 `issueUserId` 项，这是因为我们可能只想要转发自己发送的评论，在后面只需要根据该项过滤该用户 ID 的评论即可（可以通过 `https://api.github.com/users/your_github_user_name` 查看指定 Github 账户的 ID）。

[这里](https://docs.github.com/en/rest/reference/issues#list-issue-comments)是获取指定 Issues 中的评论的方法。编写 `service/github.js` 代码如下（**仅做参考**：代码截取实现功能的部分，刨除提高鲁棒性的部分，也去除了第一次执行的部分）：

```js
const { Octokit } = require(&quot;@octokit/core&quot;);
const config = require(&quot;../config&quot;).github;
const octokit = new Octokit(octokitOptions);

const bot = globalThis.bot;
const sequelize = globalThis.sequelize;

const forwardGithubIssueComment = async function () {
  const issues = config.forwardIssueComment.task;
  const ServiceGithubIssueComment = sequelize.models.ServiceGithubIssueComment;

  for (const issue of issues) {
    const owner = issue.owner;
    const repo = issue.repo;
    const issueNumber = issue.issueNumber;
    const forwardChannelId = issue.forwardChannelId;
    const issueUserId = issue.issueUserId;
    const issueUrl = `${owner}/${repo}/issues/${issueNumber}`;

    const queryConfig = {
      issueUrl,
      issueUserId,
      forwardChannelId,
    };
    const perPage = 100;
    let page = 0;

    // 查询 Github Issues 的评论的最后更新日期 lastUpdateCommentAt
    const issueServiceInfo = await ServiceGithubIssueComment.findOne({
      where: queryConfig,
    });
    const lastUpdateCommentDate =
      issueServiceInfo.dataValues.lastUpdateCommentAt;

    // 将 lastUpdateCommentAt 加上 1ms 作为下一次查询的起始日期
    const since = new Date(
      new Date(lastUpdateCommentDate).getTime() + 1,
    ).toISOString();

    // 调用 Github API 获取指定 issue 的评论信息
    // 查询的评论更新日期从 since 开始
    let issueComments = [];
    while (issueComments.length === perPage * page) {
      ++page;
      const res = await octokit.request(
        &quot;GET /repos/{owner}/{repo}/issues/{issue_number}/comments&quot;,
        {
          owner,
          repo,
          issue_number: issueNumber,
          since,
          per_page: perPage,
          page,
        },
      );
      issueComments = issueComments.concat(res.data);
    }

    // 如果设置了 issueUserId 项，则只保留数组中用户 ID 的评论
    if (Array.isArray(issueUserId) &amp;&amp; issueUserId.length &gt; 0) {
      issueComments = issueComments.filter((comment) =&gt; {
        const commentUserId = comment.user.id;
        if (issueUserId.includes(commentUserId)) {
          return true;
        } else {
          return false;
        }
      });
    }
  }
};
```

如果 Issue 存放在私人仓库中，则需要用到 [Personal Access Token](https://github.com/settings/tokens/new?scopes=repo) 进行鉴权。在创建 `octokit` 对象时传递相应参数：

```js
const octokitOptions = {};
const authToken = process.env.GITHUB_PERSONAL_ACCESS_TOKEN;
if (authToken) {
  octokitOptions.auth = authToken;
}
const octokit = new Octokit(octokitOptions);
```

### 定时转发评论到 Telegram 频道

由于 Github Issues 中的评论为 Markdown 格式，在转发到频道时，就需要对内容进行解析。幸运的是，Telegram Bot 的 [`sendMessage()`](https://core.telegram.org/bots/api#sendmessage) 方法可以设置 `parse_mode` 选项，可以将**大部分**的 Markdown 内容顺利解析为正确的消息样式。但不幸的是，由于 Telegram 本身的一些限制，对于一些无法解析的符号会报错，针对这一部分评论，笔者选择直接发送评论的网页地址作为替代。

继续编写 `service/github.js` 代码如下（**仅做参考**）：

```js
if (issueComments.length &gt; 0) {
  let lastUpdateCommentAt = new Date(0).toISOString();

  // 转发评论到 Telegram 频道
  for (const issueComment of issueComments) {
    try {
      await bot.sendMessage(forwardChannelId, issueComment.body, {
        parse_mode: &quot;MarkdownV2&quot;,
      });
    } catch (error) {
      await bot.sendMessage(forwardChannelId, issueComment.html_url);
    }

    const issueCommentUpdatedAt = issueComment.updated_at;
    if (issueCommentUpdatedAt &gt; lastUpdateCommentAt) {
      lastUpdateCommentAt = issueCommentUpdatedAt;
    }
  }

  // 维护数据库，保存 Github Issue 评论的最后更新日期
  await ServiceGithubIssueComment.update(
    {
      lastUpdateCommentAt,
      lastExecServiceAt: new Date().toISOString(),
    },
    {
      where: queryConfig,
    },
  );
}
```

到这里，我们已经基本实现了所需要的全部功能。最后需要做的事情，就是设置每隔一定时间自动运行此服务，持续获取最新的评论信息。这里笔者用到了 `toad-scheduler` 库：

```bash
npm install toad-scheduler
```

在 `service/index.js` 中编写计划任务代码如下：

```js
const {
  ToadScheduler,
  SimpleIntervalJob,
  AsyncTask,
} = require(&quot;toad-scheduler&quot;);

const githubService = require(&quot;./github&quot;);

const config = require(&quot;../config&quot;);

const scheduler = new ToadScheduler();
const taskForwardGithubIssueComment = new AsyncTask(
  &quot;Forward Github Issue Comment&quot;,
  async () =&gt; {
    await githubService.forwardGithubIssueComment();
  },
  (error) =&gt; {
    console.error(error);
  },
);
const jobForwardGithubIssueComment = new SimpleIntervalJob(
  {
    seconds: config.github.forwardIssueComment.duration,
    runImmediately: true,
  },
  taskForwardGithubIssueComment,
);

scheduler.addSimpleIntervalJob(jobForwardGithubIssueComment);
```

一切就绪，运行我们的 Bot 程序！

```bash
npm run pm2
```

笔者的频道顺利收到了来自 Github Issue 中的评论信息！

![Forward Github Issue&apos;s comments to my channel](./start-telegram-bot/forward-to-my-channel.png)

当然，该服务还有许多可以优化的地方，例如：当评论发生更新时，应编辑已发送的频道消息为最新评论内容，而不是重新发一条新的消息等。不再在此文赘述。

## 参考文章

- [开发一个 Telegram Bot](https://www.wandouip.com/t5i13823/)
- [node-telegram-bot-api usage](https://github.com/yagop/node-telegram-bot-api/blob/master/doc/usage.md)
- [How to set webhooks using express local server and NGROK](https://github.com/leobloise/node-telegram-bot-api-wb-tutorial)
</content:original-text><content:updated-at>2024-07-09T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[把自己的简历做成 Web 页面]]></title><description><![CDATA[去年投简历的时候，在 Github 上找了个开源的，星星很多的仓库 best-resume-ever 来制作自己的简历。其中的 Creative 模板我觉得很喜欢，就用它制作了我人生中的第一份找工作用的简历： 然后到了现在，到了秋招真正找工作走向社会的季节了，又该制作自己的简历了。一年的时光给自己的人生又增添了几分色彩，原先简历模板已然不够用了。正巧，这个仓库由 Vue 编写…]]></description><link>https://blog.towind.fun/posts/build-my-resume</link><guid isPermaLink="false">build-my-resume</guid><category><![CDATA[前端开发]]></category><pubDate>Thu, 19 Aug 2021 00:00:00 GMT</pubDate><content:original-text>
去年投简历的时候，在 Github 上找了个开源的，星星很多的仓库 [best-resume-ever](https://github.com/salomonelli/best-resume-ever) 来制作自己的简历。其中的 Creative 模板我觉得很喜欢，就用它制作了我人生中的第一份找工作用的简历：

![my first resume](./build-my-resume/my-first-resume.jpg)

然后到了现在，到了秋招真正找工作走向社会的季节了，又该制作自己的简历了。一年的时光给自己的人生又增添了几分色彩，原先简历模板已然不够用了。正巧，这个仓库由 Vue 编写，可以用自己已有的知识对简历做一些改造手术。

## 改造简历

举四个改造的例子好了。

### 添加 Chip 纸片

在 Creative 模板的技能专长栏目中，使用了纸片的形式展示比较擅长的编程语言，很好看！为纯纯的文字多增添了些不一样的色彩。

而在项目经历栏目中，为每一个项目提供了 platform 属性，也就是项目基于的语言或平台等。但是这一个属性是以文字的形式展示的，我觉得并不好看，在颜色上也没有突出的作用。于是我计划使用纸片的形式在展示 platform 属性。

模仿我经常使用的 Vue UI 组件库 Vuetify 的编写，添加样式表类如下：

```less
.chip {
  display: inline-block;
  color: white;
  background-color: @accent-color;
  overflow: hidden;
  vertical-align: middle;
  padding: 5px;
  margin-left: 5px;
  border-radius: 4px;
  font-size: 0.8em;

  &amp;-secondary {
    color: rgba(0, 0, 0, 0.87);
    background-color: #e0e0e0;
  }
}
```

其中 `.chip` 为默认的纸片样式表，使用主题色作为背景，白色作为文本颜色；而 `.chip-secondary` 可以覆盖原有的配色，使用灰色作为背景，黑色作为文本颜色。

添加 Vue 模板代码如下，当 platform 属性不为空时，显示拥有 platform 内容的纸片；当 dev 属性不为空时，显示有“开发中”文本的次要配色的纸片。

```html
&lt;span v-if=&quot;project.platform&quot; class=&quot;chip&quot;&gt;{{ project.platform }}&lt;/span&gt;
&lt;span v-if=&quot;project.dev&quot; class=&quot;chip chip-secondary&quot;&gt;{{ lang.underDev }}&lt;/span&gt;
```

显示的结果如下：

![resume chips](./build-my-resume/resume-chips.png)

### 修改页面布局

这个项目里的所有主题基于 A4 纸张（21 \* 29.7cm）设计：

```css
/**
 * https://github.com/salomonelli/best-resume-ever/blob/master/src/pages/resume.vue
 */
.page {
  width: 21cm;
  height: 29.68cm;
}
```

现在互联网企业的简历大多使用专门的招聘网站上传提交，似乎不必拘泥于 A4 纸的大小了。电脑屏幕那么大，何不将整个简历直接“贴脸上”呢！

首先将页面的宽度和高度设置为 100%，即宽度覆盖整个浏览器，高度足以容纳所有简历上所有内容。

然后利用省心的 Flex 布局，将页面右边的正文内容分为左右两栏。

编写页面的布局形如：

```html
&lt;div class=&quot;resume&quot;&gt;
  &lt;div class=&quot;left-column&quot;&gt;&lt;!-- 左栏 --&gt;&lt;/div&gt;
  &lt;div class=&quot;right-column&quot;&gt;
    &lt;div class=&quot;right-column-section&quot;&gt;&lt;!-- 右栏的左侧 --&gt;&lt;/div&gt;
    &lt;div class=&quot;right-column-section&quot;&gt;&lt;!-- 右栏的右侧 --&gt;&lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
```

样式表内容形如：

```less
.resume {
  display: flex;
}

.left-column {
  flex: 1;
}

.right-column {
  flex: 3;

  display: flex;
  justify-content: space-around;
}

.right-column-section {
  flex: 0 0 48%;
}
```

现在我的简历看上去就显得很大气了：

![resume preview](./build-my-resume/resume-preview.png)

但是，使用 Flex 布局把 `right-column` 分成的两栏，如果两边刚好高度差不多，那便没有什么问题；但如果某一栏比另一栏高很多，页面就会显得参差不齐。手动把一些元素挪到另一栏似乎能够解决这个问题，但在实现上非常不优雅。

因此，后来我选用了 Multiple-column 布局作为 `right-column` 的布局。

编写页面的布局形如：

```html
&lt;div class=&quot;resume&quot;&gt;
  &lt;div class=&quot;left-column&quot;&gt;&lt;!-- 左栏 --&gt;&lt;/div&gt;
  &lt;div class=&quot;right-column&quot;&gt;&lt;!-- 右栏 --&gt;&lt;/div&gt;
&lt;/div&gt;
```

样式表内容形如：

```less
.resume {
  display: flex;
}

.left-column {
  flex: 1;
}

.right-column {
  flex: 3;

  display: block;
  column-count: 2;
  column-gap: 4%;
}
```

还有个小问题，我希望我的项目经历中的每一段经历都是完整的，内容不随着分栏分离。只需要为它们设置 `break-inside: avoid;` 即可：

```less
.section-content__item {
  break-inside: avoid;
}
```

### 适配移动端界面

既然计划部署为网页，那么横向三栏的设计显然对移动端设备很不友好。

如果网页和右栏均为 Flex 布局，可以使用 `flex-direction` 来快速调整页面布局的方向。编写样式表代码如下：

```less
@media (max-width: 960px) {
  .resume {
    flex-direction: column;
  }

  .right-column {
    flex-direction: column;
  }
}
```

Easy as a cake. 当页面宽度小于 960px 时，将把原有的三栏纵向依次排列出来。效果如下：

![resume mobile preview](./build-my-resume/resume-mobile.png)

如果网页为 Flex 布局，右栏为 Multiple-column 布局，修改网页的 `flex-direction` 和右栏的 `column-count` 即可：

```less
@media (max-width: 960px) {
  .resume {
    flex-direction: column;
  }

  .right-column {
    column-count: 1;
  }
}
```

### 添加夜间模式

使用 CSS 提供的 [CSS Variables](https://developer.mozilla.org/en-US/docs/Web/CSS/--*) 能力来实现页面的主题切换，其浏览器兼容性[见于此](https://caniuse.com/?search=CSS%20Variables)。

首先创建一个主题配置文件 `src/assets/themes.json` 来存储不同主题的颜色，例如：

```json
{
  &quot;light&quot;: {
    &quot;backgroundColor&quot;: &quot;#fafafa&quot;,
    &quot;textColor&quot;: &quot;rgba(0, 0, 0, 0.87)&quot;
  },
  &quot;dark&quot;: {
    &quot;backgroundColor&quot;: &quot;#121212&quot;,
    &quot;textColor&quot;: &quot;rgba(255, 255, 255, 0.87)&quot;
  }
}
```

编写 Vue 脚本如下：

```js
&lt;script&gt;
const themes = require(&quot;@/assets/themes&quot;);

export default {
  data() {
    return {
      themeMode: &quot;light&quot;,
    };
  },
  methods: {
    setThemeMode(mode) {
      this.themeMode = mode;
      document.documentElement.style.setProperty(
        &quot;--theme-background-color&quot;,
        themes[mode].backgroundColor
      );
      document.documentElement.style.setProperty(
        &quot;--theme-text-color&quot;,
        themes[mode].textColor
      );
    },
  },
  created() {
    this.setThemeMode(&quot;light&quot;);
  },
}
&lt;/script&gt;
```

当我们执行 `this.setThemeMode(&quot;light&quot;)` 时，相当于覆盖（或添加）了如下的 CSS 样式表：

```css
:root {
  --theme-background-color: #fafafa;
  --theme-text-color: rgba(0, 0, 0, 0.87);
}
```

同理，当未来执行 `this.setThemeMode(&quot;dark&quot;)` 时，则会把之前在 `src/assets/themes.json` 配置的颜色覆盖到此处。

最后，只需要用上我们定义好的这些 CSS 变量就可以了，例如：

```less
.resume {
  color: var(--theme-text-color);
}

.right-column {
  background-color: var(--theme-background-color);
}
```

## 部署为静态网页

这个项目从 2017 年开始，到现在差不多 4 年的时间。从现在的角度来看，它依赖了许多不必要，或是已废弃的包，这是自然。

因此，我想将简历页面从这个项目分离出来，独立为一个简单的 Vue 项目。最重要的是，自从用上了 [npm-check-updates](https://www.npmjs.com/package/npm-check-updates) 这个库，我的追新强迫症就越来越看不得这么多可以升级的依赖。

### 使用 Vue CLI 打包 Vue 项目

[Vue CLI](https://cli.vuejs.org) 提供了 Vue 项目从开发到打包为静态文件并部署的全套解决方案。

总之先全局安装 Vue CLI：

```bash
PS C:\Users\Lolipop\Github&gt; yarn global add @vue/cli
...
# 由于安装过其它版本的 Vue CLI
# 因此这里使用绝对路径访问最新版本的 Vue CLI
PS C:\Users\Lolipop\Github&gt; C:\Users\Lolipop\AppData\Local\Yarn\bin\vue.cmd --version
@vue/cli 4.5.13
```

创建新的 Vue 项目：

```bash
PS C:\Users\Lolipop\Github&gt; C:\Users\Lolipop\AppData\Local\Yarn\bin\vue.cmd create resume
```

习惯性更新依赖为最新版本：

```bash
PS C:\Users\Lolipop\Github&gt; cd resume
PS C:\Users\Lolipop\Github\resume&gt; ncu -u -t minor
Using yarn
Upgrading C:\Users\Lolipop\Github\resume\package.json
[====================] 17/17 100%

 @vue/cli-plugin-babel    ~4.5.0  →  ~4.5.13
 @vue/cli-plugin-eslint   ~4.5.0  →  ~4.5.13
 @vue/cli-plugin-router   ~4.5.0  →  ~4.5.13
 @vue/cli-service         ~4.5.0  →  ~4.5.13
 eslint                   ^6.7.2  →   ^6.8.0
 eslint-plugin-prettier   ^3.3.1  →   ^3.4.0
 less                     ^3.0.4  →  ^3.13.1
 prettier                 ^2.2.1  →   ^2.3.2
 vue-template-compiler   ^2.6.11  →  ^2.6.14
 core-js                  ^3.6.5  →  ^3.16.2
 vue                     ^2.6.11  →  ^2.6.14
 vue-router               ^3.2.0  →   ^3.5.2

Run yarn install to install new versions.

PS C:\Users\Lolipop\Github\resume&gt; yarn install
```

启动项目服务端渲染，确保能正常运行：

```bash
PS C:\Users\Lolipop\Github\resume&gt; yarn serve
```

将原有的 Vue 文件移动过来，处理发生错误的依赖关系，再根据需要安装必要的其它依赖。

别忘了配置 `vue.config.js` 中的 `publicPath` 项。我们的项目将部署在域名的根路径，例如 `https://lolipopj.github.io/resume`，因此需要配置如下：

```js
// vue.config.js
module.exports = {
  publicPath: process.env.NODE_ENV === &quot;production&quot; ? &quot;/resume/&quot; : &quot;/&quot;,
};
```

Okay... 一切就绪，最后只需要执行：

```bash
PS C:\Users\Lolipop\Github\resume&gt; yarn build
...

 DONE  Build complete. The dist directory is ready to be deployed.
 INFO  Check out deployment instructions at https://cli.vuejs.org/guide/deployment.html

Done in 14.90s.
```

Beatiful. 现在，只需要将 `dist/` 目录下的静态资源部署即可。

### 部署 Github page

一如既往，主分支的提交触发 Github Actions，自动执行打包构建操作，并将它们上传到 gh-pages 分支，部署为 Github page。

首先创建一个 [Personal access token](https://github.com/settings/tokens/new)，赋予 repo 的所有权限。在仓库的 Secrets 处添加新的秘密，命名为例如 `ACCESS_TOKEN`。将刚刚创建的 token 作为此秘密的值即可。

创建并编写 `.github/workflows/deploy.yml` 如下：

```yml
name: Resume Deployment

on:
  push:
    branches:
      - main

jobs:
  pages:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Use Node.js 14.x
        uses: actions/setup-node@v1
        with:
          node-version: &quot;14.x&quot;
      - name: Cache NPM dependencies
        uses: actions/cache@v2
        with:
          path: node_modules
          key: ${{ runner.OS }}-npm-cache
          restore-keys: |
            ${{ runner.OS }}-npm-cache
      - name: Install Dependencies
        run: |
          npm install
      - name: Build
        run: |
          npm run build
      - name: Deploy
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          personal_token: ${{ secrets.ACCESS_TOKEN }}
          publish_dir: dist
          publish_branch: gh-pages
```

提交代码！现在，我的简历顺利部署到了 [Github page](https://lolipopj.github.io/resume/) 上。

## 导出简历 PDF 文件

或许直接将简历网站地址发送给 HR 很酷，但是招聘系统还是需要我提交简历的 PDF 文档。

&gt; 接下来的内容主要参考了 best-resume-ever 已有的[实现](https://github.com/salomonelli/best-resume-ever/blob/master/scripts/export.js)。

知名的 [puppeteer](https://www.npmjs.com/package/puppeteer) 项目可以帮助我们生成页面的 PDF 文档，让我们开始吧：

```bash
PS C:\Users\Lolipop\Github\resume&gt; yarn add -D puppeteer@10.2.0
```

由于我们需要先启动本地服务，才能用 puppeteer 访问。这意味着我们需要先执行 `yarn serve` 命令，当服务启动成功后，再执行后续的操作。我们可以通过 [concurrently](https://www.npmjs.com/package/concurrently) 和 [rxjs](https://www.npmjs.com/package/rxjs) 实现：

```bash
PS C:\Users\Lolipop\Github\resume&gt; yarn add -D concurrently@6.2.1 rxjs@7.3.0
```

concurrently 可以在一个终端中同时运行多个命令，一旦某个命令运行失败，便中止刚刚当前终端运行的所有命令。

假设我们编写的脚本文件为 `scripts/export.js`，那么在 `package.json` 中可以添加这样一条命令：

```json
{
  &quot;scripts&quot;: {
    &quot;export&quot;: &quot;concurrently \&quot;npm run serve\&quot; \&quot;node scripts/export.js\&quot; --success first --kill-others&quot;
  }
}
```

我们将依次执行 `npm run serve` 和 `node scripts/export.js`。当第一条命令成功时返回 0，失败时返回 1。任意一条命令结束或失败时中止此脚本。

rxjs 是一个用于编写异步、事件驱动的程序的库，我们可以使用它来监听页面是否就绪，避免在尚未加载完成的情况下就打印简历 PDF 文档：

```js
const http = require(&quot;http&quot;);
const { interval } = require(&quot;rxjs&quot;);
const { filter, first, mergeMap } = require(&quot;rxjs/operators&quot;);

const config = require(&quot;../config&quot;);
const port = config.DEV_PORT;

const fetchServeResponse = () =&gt; {
  return new Promise((res, rej) =&gt; {
    try {
      const req = http.request(`http://localhost:${port}/`, (response) =&gt;
        res(response.statusCode),
      );
      req.on(&quot;error&quot;, (err) =&gt; rej(err));
      req.end();
    } catch (err) {
      rej(err);
    }
  });
};

const waitForServerReady = () =&gt; {
  return interval(1000).pipe(
    mergeMap(async () =&gt; {
      try {
        const statusCode = await fetchServeResponse();
        if (statusCode === 200) {
          return true;
        }
        return false;
      } catch (err) {
        return false;
      }
    }),
    filter((ok) =&gt; !!ok),
  );
};
```

接下来，利用 puppeteer 的强大功能，打印出简历的 PDF 文档，顺便再给屏幕截个图好了：

```js
const puppeteer = require(&quot;puppeteer&quot;);
const path = require(&quot;path&quot;);
const fs = require(&quot;fs&quot;);

const config = require(&quot;../config&quot;);
const port = config.DEV_PORT || 8088;
const defaultScreenshotWidth = config.EXPORT_SCREENSHOT_WIDTH || 1600;
const defaultScreenshotHeight = config.EXPORT_SCREENSHOT_HEIGHT || 1000;
const defaultPdfWidth = config.EXPORT_PDF_WIDTH || 1600;
const defaultPdfHeight = config.EXPORT_PDF_HEIGHT || 1000;

const { lastValueFrom } = require(&quot;rxjs&quot;);
const { first } = require(&quot;rxjs/operators&quot;);

const convert = async function () {
  await lastValueFrom(waitForServerReady().pipe(first()));

  console.log(&quot;Connected to server ...&quot;);
  console.log(&quot;Exporting ...&quot;);

  try {
    const savePath = path.join(__dirname, &quot;../export/&quot;);

    if (!fs.existsSync(savePath)) {
      fs.mkdirSync(savePath);
    }

    const exportResume = async function ({
      code = &quot;cn&quot;,
      screenshotFullPage = true,
      screenshotQuality = 100,
    }) {
      const url = `http://localhost:${port}/#/${code}?exportMode=true`;
      const filename = `resume-${code}`;

      const codeUpperCase = code.toUpperCase();
      const screenshotWidth =
        config[`EXPORT_SCREENSHOT_WIDTH_${codeUpperCase}`] ||
        defaultScreenshotWidth;
      const screenshotHeight =
        config[`EXPORT_SCREENSHOT_HEIGHT_${codeUpperCase}`] ||
        defaultScreenshotHeight;
      const pdfWidth =
        config[`EXPORT_PDF_WIDTH_${codeUpperCase}`] || defaultPdfWidth;
      const pdfHeight =
        config[`EXPORT_PDF_HEIGHT_${codeUpperCase}`] || defaultPdfHeight;

      const browser = await puppeteer.launch({
        args: [&quot;--no-sandbox&quot;],
        defaultViewport: {
          width: screenshotWidth,
          height: screenshotHeight,
        },
      });

      const page = await browser.newPage();

      await page.goto(url, {
        waitUntil: &quot;networkidle0&quot;,
      });

      await page.screenshot({
        path: `${savePath}${filename}.jpeg`,
        fullPage: screenshotFullPage,
        quality: screenshotQuality,
      });

      await page.pdf({
        path: `${savePath}${filename}.pdf`,
        width: pdfWidth,
        height: pdfHeight,
      });

      await browser.close();
    };

    await exportResume({
      code: &quot;cn&quot;,
    });

    await exportResume({
      code: &quot;en&quot;,
    });
  } catch (err) {
    console.log(&quot;Export failed.&quot;);
    throw new Error(err);
  }

  console.log(&quot;Finished exports.&quot;);
};
```

在 URL 的结尾我设置了 `?exportMode=true`，供 Vue Router 查询使用。正常访问网页时，默认为「非导出模式」，网页上会显示切换语言和切换夜间模式按钮等；使用此脚本时，访问的网页为「导出模式」，隐藏掉不必要的内容。

然后又到了爷最喜欢的**约定大于配置**环节，在上面的代码里，假设每个翻译版本的简历有不同的 URL 值，对应不同的 `code`。例如 `http://localhost:8088/#/cn` 为简历的中文版本，对应的 `code` 值为 `cn`。这样，最终导出文件为 `export/resume-cn.jpeg` 和 `export/resume-cn.pdf`。

如果需要配置不同翻译版本的简历的 PDF 文档（或截图）的大小，可以在 `config.json` 中配置。例如需要配置中文版本简历 PDF 文档的高度，设置 `&quot;EXPORT_PDF_HEIGHT_CN&quot;: 850` 即可，其中 `_CN` 为 `code` 的大写值前面加上短横线。

不过，手动配置打印 PDF 高度在实现上并不优雅，仔细想想，既然 puppeteer 能够模仿浏览器中的所有行为，那么：在固定页面宽度的情况下，获取当前页面的高度也是理所应当能够做到的吧。[`page.evaluate()`](https://pptr.dev/#?product=Puppeteer&amp;version=v10.2.0&amp;show=api-pageevaluatepagefunction-args) 方法可以实现这个需求：

```js
const exportResume = async function ({ autoFitPdf = true }) {
  // ...
  const pdfHeight = autoFitPdf
    ? await page.evaluate(() =&gt; {
        const body = document.body,
          html = document.documentElement;

        const pageHeight = Math.max(
          body.scrollHeight,
          body.offsetHeight,
          html.clientHeight,
          html.scrollHeight,
          html.offsetHeight,
        );

        // 确保容纳下所有的内容，
        // 而不会因小数点后的差值分页
        return pageHeight + 10;
      })
    : config[`EXPORT_PDF_HEIGHT_${codeUpperCase}`] || defaultPdfHeight;
  // ...
};
```

完整的脚本文件[见于此](https://github.com/LolipopJ/resume/blob/8ae81dcffe2b84c71d3ce4c8f3adb705b4f98b91/scripts/export.js)。

最后，见证劳动的成果吧。执行刚刚我们编写的脚本：

```bash
PS C:\Users\Lolipop\Github\resume&gt; yarn export
...
[0] &lt;s&gt; [webpack.Progress] 100%
[0]
[0]
[0]
[0]   App running at:
[0]   - Local:   http://localhost:8088/
[0]   - Network: http://192.168.237.206:8088/
[0]
[0]   Note that the development build is not optimized.
[0]   To create a production build, run yarn build.
[0]
[1] Connected to server ...
[1] Exporting ...
[1] Finished exports.
[1] node scripts/export.js exited with code 0
--&gt; Sending SIGTERM to other processes..
[0] npm run serve exited with code 1
Done in 42.14s.
```

为了避免导出失败或异常，在此期间应避免修改页面源文件。

顺利地导出了我的简历，此外还有截图和英文版本：

![resume export](./build-my-resume/resume-export.png)
</content:original-text><content:updated-at>2021-08-20T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[使用 jsDelivr 加速 Github 仓库搭建自己的图床服务]]></title><description><![CDATA[使用此类公益服务时应保留敬畏之心，不要滥用服务，消耗他人的善意。 今天突然想去搞个图床，使用 CDN 加速图片资源。因为博客放在小水管服务器上，直接用这个服务器向用户传输图片资源对带宽有很大影响。

遂上网搜索有无免费图床的服务，看到不少将 jsDelivr 用作图床使用的教程，便自己实践一番。

在国内，直接使用 Github 链接来加载图片是很慢的，甚至于加载不出来；但访问 jsDelivr…]]></description><link>https://blog.towind.fun/posts/github-jsdelivr-hold-image</link><guid isPermaLink="false">github-jsdelivr-hold-image</guid><category><![CDATA[技术琐事]]></category><pubDate>Fri, 13 Aug 2021 00:00:00 GMT</pubDate><content:original-text>
&gt; 使用此类公益服务时应保留敬畏之心，不要滥用服务，消耗他人的善意。

今天突然想去搞个图床，使用 CDN 加速图片资源。因为博客放在小水管服务器上，直接用这个服务器向用户传输图片资源对带宽有很大影响。

遂上网搜索有无免费图床的服务，看到不少将 jsDelivr 用作图床使用的教程，便自己实践一番。

在国内，直接使用 Github 链接来加载图片是很慢的，甚至于加载不出来；但访问 jsDelivr 速度较快。通过 jsDelivr 来加速 Github 上的图片资源，即可以实现我们想要的图床服务。

使用其它的可以加速 Github 资源的 CDN 服务来替换 jsDelivr 也可以；这应该算是目前对于个人开发者来说，最简单且最经济的方式了。

## 创建图床仓库

在 Github 上自建一个**公开**的仓库即可，与其它仓库区分开，您可以命名为 `img-holder`。

为什么要是公开的仓库？因为 jsDelivr 是 A free CDN for Open Source。咳咳，因为非公开的仓库别人也访问不到啦。

接下来，就可以往这个仓库里扔图片文件了。

## 使用 jsDelivr 加速

使用也非常简单，jsDiliver 提供了这个例子：

```plaintext
// load any GitHub release, commit, or branch
https://cdn.jsdelivr.net/gh/user/repo@version/file
```

按照这个格式依葫芦画瓢，例如在编写 Markdown 时，可以这样使用：

```md
![CDN host image](https://cdn.jsdelivr.net/gh/user/repo/file)
```

其中 `user` 为 Github 账户名，`repo` 为仓库名，`file` 为文件路径。

## 实际使用示例

在我的 `img-folder` 仓库中存放了一个 [`pic/less-spend.gif`](https://github.com/LolipopJ/img-folder/blob/master/pic/less-spend.gif) 文件。

编写 Markdown 内容如下：

```md
![Spend my money less](https://cdn.jsdelivr.net/gh/lolipopj/img-folder/pic/less-spend.gif)
```

渲染为 HTML 文件，显示内容如下：

![Spend my money less](https://cdn.jsdelivr.net/gh/lolipopj/img-folder/pic/less-spend.gif)

值得注意的是，由于 jsDelivr 本身的缓存机制，刚刚上传到 Github 仓库上的图片资源可能无法成功获取。系正常现象，等待一段时间便可。

看到一些博客提到可以使用 [PicGo](https://github.com/Molunerfinn/PicGo) 来管理自己的图床资源，电脑和手机端都可以使用，甚至还提供了 VSCode 插件。Cool，对于使用图片较多的用户可能非常有帮助。
</content:original-text><content:updated-at>2024-07-25T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[使用 EditorConfig 和 Prettier 优雅地配置 VSCode 代码格式化]]></title><description><![CDATA[编写代码时使用 EditorConfig EditorConfig 能够帮助跨各种 IDE 开发同一项目的不同开发人员保持一致的编码风格。

VSCode 没有内置对 EditorConfig 的支持，需要在插件市场中手动搜索并安装插件。

EditorConfig 会自动读取工作区中的  文件，更详细的配置说明可以参考官方介绍。下面是笔者常用的配置：

推送仓库前使用 Prettier

为了进一步确…]]></description><link>https://blog.towind.fun/posts/editorconfig-prettier</link><guid isPermaLink="false">editorconfig-prettier</guid><category><![CDATA[技术琐事]]></category><pubDate>Sat, 07 Aug 2021 00:00:00 GMT</pubDate><content:original-text>
## 编写代码时使用 EditorConfig

EditorConfig 能够帮助跨各种 IDE 开发同一项目的不同开发人员保持一致的编码风格。

VSCode 没有内置对 EditorConfig 的支持，需要在插件市场中手动搜索并安装[插件](https://marketplace.visualstudio.com/items?itemName=EditorConfig.EditorConfig)。

EditorConfig 会自动读取工作区中的 `.editorconfig` 文件，更详细的配置说明可以参考[官方介绍](https://editorconfig-specification.readthedocs.io/)。下面是笔者常用的配置：

```editorconfig
root = true

[*]
indent_style = space
indent_size = 4
end_of_line = lf
charset = utf-8
insert_final_newline = true
trim_trailing_whitespace = true
```

## 推送仓库前使用 Prettier

为了进一步确保代码风格符合编码规范，可以在上库前使用 Prettier 修复代码格式。

将 Prettier 安装为项目开发依赖：

```bash
yarn add -D prettier
```

在 `package.json` 中添加 Prettier 运行脚本：

```json
{
  &quot;scripts&quot;: {
    &quot;prettier&quot;: &quot;prettier --write **/* --ignore-unknown&quot;
  }
}
```

现在，执行 `yarn prettier` 命令，工具将按照 Prettier 预置的规则自动格式化所有支持的后缀格式的文件了。

在项目根目录创建 `.prettierrc.json` 文件，可以进一步配置 Prettier 格式化规则，可参考[官方文档](https://prettier.io/docs/en/options.html)。例如：

```json
{
  // JSON 文件中不应添加注释，需去除
  &quot;semi&quot;: true, // 句末是否添加分号
  &quot;singleQuote&quot;: true // 是否使用单引号
}
```

假如我们的项目中会包含 PHP 以及 Java 等语言的代码，要让 Prettier 处理它们的格式，该怎么做呢？

在默认情况下，Prettier 并不支持 PHP 或 Java 代码的格式化，这就需要我们单独添加支持其它编程语言的 [Prettier 插件](https://prettier.io/docs/en/plugins.html)作为项目的依赖。其中，以 `@prettier` 开头的插件为官方维护插件，例如 [`@prettier/plugin-php`](https://github.com/prettier/plugin-php)；其它命名插件为社区开发插件，例如 [`prettier-plugin-java`](https://github.com/jhipster/prettier-java)。

```bash
yarn add --dev @prettier/plugin-php prettier-plugin-java
```

Prettier 将自动加载与它在同一 `node_modules` 文件夹中的 Prettier 插件，并自动识别新增的代码后缀（如 `.php` 和 `.java`）。因此在最后，只需要再执行一遍 `yarn prettier`，就可以实现格式化项目中的 PHP 及 Java 代码了。
</content:original-text><content:updated-at>2023-12-14T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[更换持续集成工具，从 Travis 到 Github Actions]]></title><description><![CDATA[我真傻，真的，单单受文档的推荐就选择了 Travis 作为部分项目的持续集成工具，没有料到它早已于 2020 年 12 月更换了免费政策，不再为开源项目提供免费的用于持续集成使用的 Credits 了。当赠送的 10000 个点数用完，就需要付费才能进行构建了。 当然，作为经济驱动的公司，近些天来又受 Github Actions 等其它持续集成工具打压了盈利空间…]]></description><link>https://blog.towind.fun/posts/switch-travis-to-github-workflow</link><guid isPermaLink="false">switch-travis-to-github-workflow</guid><category><![CDATA[技术琐事]]></category><pubDate>Sat, 10 Jul 2021 00:00:00 GMT</pubDate><content:original-text>
我真傻，真的，单单受文档的推荐就选择了 Travis 作为部分项目的持续集成工具，没有料到它早已于 2020 年 12 月更换了免费政策，不再为开源项目提供免费的用于持续集成使用的 Credits 了。当赠送的 10000 个点数用完，就需要付费才能进行构建了。

当然，作为经济驱动的公司，近些天来又受 Github Actions 等其它持续集成工具打压了盈利空间，抛弃开源用户选择转型做起了商人事业也并非不可理解。感谢它曾为开发者提供的便利，不过作为一个佛系开发者，终于还是需要转投到别的免费工具上去了——Gihub Actions。

## 编写新的 workflow.yml

那么首先，我们就需要将为 Travis 编写的命名为 `.travis.yml` 的配置文件，翻译成 Github Actions 能识别的 workflow.yml 配置文件。

以[献给中文读者的设计模式教程](https://github.com/LolipopJ/design-patterns-for-humans-zh)这个项目为例，原有的 `.travis.yml` 内容如下：

```yml
language: node_js
node_js:
  - lts/*
branches:
  only:
    - main
install:
  - cd vuepress
  - yarn install
script:
  - yarn build
deploy:
  provider: pages
  skip_cleanup: true
  local_dir: vuepress/docs/.vuepress/dist
  github_token: $CI_DEPLOY_TOKEN
  keep_history: true
  on:
    branch: main
```

当检测到 main 分支代码更新后，启动持续集成工具。克隆项目，进入到项目的 `vuepress` 目录下执行安装依赖和生成静态文件操作，最后将 `/vuepress/docs/.vuepress/dist` 目录下的静态文件，上传到 `gh-pages` 分支，交给 Github 部署。

使用 Github Actions 实现上面的过程，首先在项目根目录创建 `.github/workflows` 文件夹，在文件夹内创建 workflow 配置文件，例如 `deploy.yml`，编写内容如下：

```yml
name: Vuepress Deployment

on:
  push:
    branches:
      - main

jobs:
  pages:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Use Node.js 14.x
        uses: actions/setup-node@v1
        with:
          node-version: &quot;14.x&quot;
      - name: Cache NPM dependencies
        uses: actions/cache@v2
        with:
          path: node_modules
          key: ${{ runner.OS }}-npm-cache
          restore-keys: |
            ${{ runner.OS }}-npm-cache
      - name: Install Dependencies
        run: |
          cd vuepress
          npm install
      - name: Build
        run: |
          cd vuepress
          npm run build
      - name: Deploy
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          deploy_key: ${{ secrets.ACCESS_TOKEN }}
          publish_dir: vuepress/docs/.vuepress/dist
          publish_branch: gh-pages
```

## 创建 SSH Deploy Key

这一步是为了给 Github Actions 远程服务器访问我的 Github 账号提供凭证。如果没有设置 Github 账户双重验证或其它安全验证，可以移除上面脚本中的 `deploy_key` 属性然后跳过这一步。但是假如以后设置了安全验证，回来改又会很麻烦，不如一步到位了吧 🤗。

您也可以生成 Personal access token 作为替代，不过前面的脚本中的 `deploy_key` 应该修改为 `personal_token`。

启动命令行工具，创建 SSH 部署密钥：

```bash
# 进入到当前用户的 .ssh 目录下
cd ~/.ssh
# 创建 SSH 密钥
ssh-keygen -t rsa -b 4096 -C &quot;$(git config user.email)&quot; -f design-patterns-for-humans-zh-gh-pages
```

其中，`design-patterns-for-humans-zh-gh-pages.pub` 为公钥，应上传到 [Github 账户 SSH keys 设置](https://github.com/settings/keys)中；不带后缀的为私钥，应作为 [Github 项目仓库的 Secret](https://github.com/LolipopJ/design-patterns-for-humans-zh/settings/secrets/actions)，根据前面的配置，这里命名为 `ACCESS_TOKEN`。

## 最后一步

最后，移除 Github 仓库中用于 Travis 的删除原有部署密钥，例如 `CI_DEPLOY_TOKEN`，删除项目中的 `.travis.yml` 文件，提交代码到 Github 即可。

![CI 部署成功](./switch-travis-to-github-workflow/deploy-finished.png)
</content:original-text><content:updated-at>2021-07-14T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[在浏览器中输入 URL 到显示网页，背后发生了什么]]></title><description><![CDATA[最近学习前端基础知识的时候，看到了这个问题和一个回答，非常生动有趣。遂抱着梳理的想法，将整个过程描述出来。 现在，假设您打开了浏览器，想要访问我的个人博客，您会在地址栏输入  这个 URL 然后敲下回车键。

从敲下回车键到最终顺利在浏览器显示我博客的主页，这个过程的背后发生了什么呢？

检查 URL 格式

别急，在正式驶入互联网的快车道之前，浏览器会首先检查输入的 URL 的格式是否正确。

例如…]]></description><link>https://blog.towind.fun/posts/browser-behind-visit-url</link><guid isPermaLink="false">browser-behind-visit-url</guid><category><![CDATA[技术琐事]]></category><pubDate>Thu, 08 Jul 2021 00:00:00 GMT</pubDate><content:original-text>
最近学习前端基础知识的时候，看到了这个问题和[一个回答](https://www.zhihu.com/question/34873227/answer/518086565)，非常生动有趣。遂抱着梳理的想法，将整个过程描述出来。

现在，假设您打开了浏览器，想要访问我的个人博客，您会在地址栏输入 `lolipopj.github.io` 这个 URL 然后敲下回车键。

从敲下回车键到最终顺利在浏览器显示我博客的主页，这个过程的背后发生了什么呢？

## 检查 URL 格式

别急，在正式驶入互联网的快车道之前，浏览器会首先检查输入的 URL 的格式是否正确。

例如，假如您输入的是 `lolipop j.github.io`，或是 `lolipopj.gith$ub.io`，浏览器将会判断它们为非 URL。在这种情况下，浏览器通常会将我们错误输入的 URL 作为搜索引擎的输入关键字，最终跳转到搜索结果界面。

### 什么是 URL

- [「标识互联网上的内容」](https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Basics_of_HTTP/Identifying_resources_on_the_Web)，MDN

HTTP 请求的内容非常宽泛，统称为“资源”。“资源”可以是一份文档，一张图片，或所有您可以想象到的格式。而这样的每个“资源”，都由一个**统一资源定位符 URL** 标识。

通俗地讲，URL 可以叫作“网络地址”或“链接”，它是对指定计算机网络上某位置的**网络资源**的引用，以及检索它的机制。一个典型的 URL 可以采用 `http://www.example.com/index.html` 形式表示，它表明了使用的协议（http），访问的主机名（www.example.com）以及文件名（index.html）。

此外，URL 也可以用于文件传输（FTP），发送邮件（SMTP）和数据库访问（JDBC）等。

URL 是**统一资源标志符 URI** 的子集，由于早期 RFC 文档撰写的[一些混乱](https://danielmiessler.com/study/difference-between-uri-url/)，在实际使用中可能会发生混用的情况。在以 HTTP 为上下文的语境中，大多数情况使用 URL 即可。

### URL 格式

- [「URL」](https://en.wikipedia.org/wiki/URL)，Wikipedia

URL 符合通用 URI 语法，格式如下：

```plaintext
URI = scheme:[//authority]path[?query][#fragment]
```

其中，`authority` 部分可以划分为三个子模块：

```plaintext
authority = [userinfo@]host[:port]
```

URI 具体包括如下部分：

- **非空的** `scheme` 标识，后面跟着一个 `:`。由字母开头，后跟字母、数字、`+`、`.` 或连字符 `-` 的任意组合，规范建议使用小写格式。常见的例子有 `http:`，`https:` 和 `ftp:` 等。
- 可选的以 `//` 开头的 `authority` 权限组件，包括：
  - 可选的 `userinfo` 用户信息，可能包括用户名和用户密码，两者使用 `:` 分开。在后面跟着 `@`。出于安全考虑，应用程序不应当将用户密码部分用明文表示。
  - **非空的** `host` 主机，由注册名称（例如主机名）或 IP 地址组成。对于后者，如果是 IPv4 地址，需要使用十进制表示法；如果是 IPv6 地址，需要包括在方括号 `[]` 中。
  - 可选的以 `:` 开头的 `port` 端口号。
- **非空的** `path` 路径，由一系列 `/` 分隔的路径段组成。路径将类似或完全映射到文件系统中。此外，如果存在 `authority` 权限组件，则必须为空或以 `/` 开头；如果不存在，则不能以空路径段开头，因为这样实际上就是以 `//` 开头，将被解释为 `authority` 权限组件。
- 可选的以 `?` 开头的 `query` 查询。语法没有明确要求，通常采用键值对的形式。
- 可选的以 `#` 开头的 `fragment` 片段。用于提供对次要资源的指向，例如在 HTML 文档中，将指向包含对应 `id` 属性的元素。

## 补齐 URL

前面我们在使用 URL 时，并没有添加它的前缀例如 `https://`。那么我们具体使用的是 HTTP 协议还是 HTTPS 协议呢？

针对这种情况，浏览器有自己的预案，即默认使用 HTTP 协议。假如您是**第一次**访问我的博客（更严谨地说是第一次访问 `github.io` 域名下的网站），除非在输入的最开始就使用了 `https://lolipopj.github.io` 这个 URL，否则均会被默认补齐为 `http://lolipopj.github.io`。对于启用了 [HSTS 保护](#什么是-hsts为什么我们需要它)的网站，从第二次的访问开始，浏览器将根据第一次访问时得到的响应结果，自动补齐协议。

随着 HSTS 的推广使用，现代浏览器中还内置了一个列表 [Preload List](#hsts-的-preload-list-机制)，记录常用网站所使用的协议。对于这些网站，输入的 URL 将自动在前面补上记录的协议，再由浏览器发送请求。因此在实际情况中，第一次访问时，我们输入的 URL 就会被补齐为 `https://lolipopj.github.io`。

### HTTP 严格传输安全 HSTS

以下内容主要参考此文章：

- [「HSTS 详解」](https://zhuanlan.zhihu.com/p/25537440)，2017-03-03

### 什么是 HSTS，为什么我们需要它

在过去，假如服务器使用的是 HTTPS 协议，当我们默认使用 `http://lolipopj.github.io` 发起请求时，也会在服务器端通过 301 重定向到 `https://lolipopj.github.io`。在这个过程中，浏览器首先使用了 HTTP 协议发起请求，得到重定向的响应后，浏览器会重新发起基于 HTTPS 协议的请求并最终与服务器建立通信。

这是一个存在风险的操作，因为在建立 HTTPS 通信之前，我们有一次明文的 HTTP 请求以及重定向操作。HTTP 主要有如下不足：

- 通信使用明文（不加密），内容可能会被窃听；
- 不验证通信方的身份，因此有可能遭遇伪装；
- 无法证明报文的完整性，所以有可能已遭篡改等。

中间人可以劫持 HTTP 请求并篡改响应，阻止建立 HTTPS 连接，跳转到钓鱼网站等。

可以想见，对于使用 HTTPS 协议的服务器，如果能从第一次开始就直接以 HTTPS 协议建立连接，跳过 HTTP + 301 重定向的步骤，便可以避免这个潜在风险了。那么，浏览器该如何知道对于哪些网站应该使用 HTTP 协议，哪些网站应该建立 HTTPS 请求呢？

这就不得不提到 **HSTS**（HTTP Strict-Transport-Security，即 HTTP 严格传输安全）了，它是一个 Web 安全策略机制。其最核心的实现，是一个 HTTP 响应头，正是它让浏览器得知，接下来的一段时间（通常设置为 1 年），对这个域名的访问都应基于 HTTPS 协议。

例如，当浏览器通过 HTTP/HTTPS 协议访问某网站，返回的响应头可能包括一项：

```plaintext
Strict-Transport-Security: max-age=31536000; includeSubDomains
```

浏览器就知道，在接下来的 31536000 秒（即 1 年）内，对该域名，以及子域名（includeSubDomains）的后续通信应该强制使用 HTTPS 进行，直到过了有效期（max-age）为止。每次相应都将刷新 HSTS 有效期；如果过了有效期，只要进行一次新的通信，又会开启一年的 HSTS 有效期。

### HSTS 加强浏览器连接保护

在 HSTS 出现以前，当浏览器发现当前网站的证书出现错误，或浏览器与服务器之间的通信不安全，或无法建立 HTTPS 连接时，浏览器会告警用户，但又允许用户继续不安全的访问。

从理论上来说，当出现此类告警时，用户应该提高警惕，终止后续的操作。然后现实是，绝大多数用户即使遇到这样的告警，也仍会继续访问。

HSTS 的出现使得事情出现了转机。对于启用了 HSTS 保护的网站，如果浏览器发现连接不安全，它将仅仅告警用户，**不再**提供继续访问的选择，进而避免后续的安全问题。

### HSTS 的 Preload List 机制

很容易发现，在第一次通过 URL 访问网站时（或浏览器没有当前网站的 HSTS 信息时），仍会默认使用明文的 HTTP 协议进行请求，然后重定向切换到 HTTPS，并刷新浏览器中的 HSTS 信息。这样，用户还是有受到中间人攻击的风险。

针对这种情况，HSTS 的应对方法是：在浏览器中内置一个列表，即 Preload List，在这个列表中的域名，无论何种情况，浏览器将**只使用** HTTPS 发起连接。

这个列表由 Google Chromium 维护。

### 加入到 HSTS Preload List

为了加入到此列表，您的站点必须满足以下需求：

1. 提供有效的证书。
2. 如果监听了 80 端口，则需要在同一主机上从 HTTP 重定向到 HTTPS。
3. 为所有子域名提供 HTTPS 服务。
4. 在根域名的 HTTP 响应头中添加 HSTS header。

您可以在 [HSTS Preload List 官网](https://hstspreload.org/)提交申请，或是了解更多相关内容。

## 通过 DNS 获取 IP 地址

TCP/IP 是能够在多个不同网络间实现信息传输的**协议簇**，它不仅仅指的是 TCP 和 IP 两个协议，而是指一个由 HTTP、HTTPS、FTP、SMTP、TCP、UDP 和 IP 等协议构成的协议簇。TCP/IP 定义了电子设备如何连入因特网，以及数据如何在它们之间传输的标准。

在 TCP/IP 概念层模型中，计算机网络体系结构自上而下分成了应用层、传输层、网络层和链路层。其中，HTTP 协议属于应用层。当客户端发出 HTTP 请求后，报文接下来将来到传输层和网络层进行处理。

浏览器会随机选用一个大于 1023（且 {&quot;&lt;&quot;}= 65535）的端口号，作为当前页面通讯使用的端口，以及传输层的 TCP 协议头的 Source Port 部分，这很容易实现。

但问题是，运输层的 IP 协议需要**目标网站的 IP 地址**才能工作，而现在浏览器只有 `https://lolipopj.github.io` 这个 URL 链接，它完全无法理解。

因此，浏览器将首先联系**域名系统 DNS**（Domain Name System），一个将域名和 IP 地址相互映射的分布式数据库。我们可以通过 DNS 来查询一串 URL 链接所对应的 IP 地址。

DNS 由客户端和服务端两部分组成，其中，客户端发起查询请求，例如查询域名的 IP 地址，服务端则负责回答域名的真正 IP 地址。那么现在，DNS 客户端发起查询 `github.io` 域名 IP 地址的请求，可能经过如下步骤：

1. 首先检查本机的 DNS 缓存，没有该域名的 IP 地址信息！
2. 再查看本地硬盘中的 Host 文件，也没有！
3. 请求本地域名服务器或公共域名服务器记录的信息。这里也没有！
4. 此时，本地域名服务器或公共域名服务器会将查询请求发送给**根域名服务器**（Root name server）。根域名服务器会根据请求的 URL，将其对应的**顶级域名服务器**（Top-level domain server）的地址返回给本地域名服务器或公共域名服务器。例如，这里我们查询网址的顶级域名是 `.io`，则将此域名对应的顶级域名服务器的 IP 地址返回回来。
5. 接着，本地域名服务器或公共域名服务器会将查询请求发送给刚刚得到的顶级域名服务器。顶级域名服务器将返回管理 `github.io` 的**权威域名服务器**的 IP 地址，例如 `185.199.110.153`。继续请求此权威域名服务器，如果得知 `lolipopj.github.io` 为此域名下的 A 记录，那么此 IP 地址即为所求。
6. 查询的 IP 地址结果将缓存到本机以及本地域名服务器或公共域名服务器，用户下次查询时可以直接使用。

通常情况下，大型网站都会返回 CNAME 记录，传递给[全局流量管理 GTM 服务](#全局流量管理-gtm)，递归解析器将执行全新的 DNS 查找。通过 GTM 服务的负载均衡机制等，可以帮助用户找到最适合自己的访问的服务器 IP 地址；此外，大多数网站会做 CDN 缓存，GTM 服务也可以帮助用户找到最适合自己的 CDN 缓存服务器。

但无论如何，老天爷，浏览器可算是知道我博客的 IP 地址了。

### 递归解析器

也称为 “DNS 解析器”。

递归解析器是 DNS 查询中的第一站。递归解析器作为客户端与 DNS 域名服务器的中间人。从 Web 客户端收到 DNS 查询后，递归解析器将使用缓存的数据进行响应，或者将向根域名服务器发送请求，接着向顶级域名服务器发送另一个请求，然后向权威域名服务器发送最后一个请求。收到来自包含已请求 IP 地址的权威域名服务器的响应后，递归解析器将向客户端发送响应。

大多数用户使用他们的 ISP 提供的递归解析器，即本地域名服务器。但也可以指定公共域名服务器作为递归解析器，如 Google 的 `8.8.8.8` 或 Cloudflare 的 `1.1.1.1` 等。

### 根域名服务器

- [「根域名的知识」](https://www.ruanyifeng.com/blog/2018/05/root-domain.html)，2018-05-09

根域名服务器中记录了各个顶级域名服务器的 IP 地址信息。

由于早期的 DNS 查询结果是一个 512 字节的 UDP 数据包，这个包最多容纳 13 个服务器的地址，因此规定全世界有 13 台根域名服务器，编号从 `a.root-servers.net` 到 `m.root-servers.net`。

为了保证根域名服务器的可用性，每台服务器又有多个节点。根据[此网站](https://root-servers.org/)的统计，截止 2021 年 07 月 12 日，全球一共有 1403 台根域名服务器实例。

当需要通过根域名服务器查询顶级域名服务器时，进行请求的 DNS 服务器会向这 13 台服务器**同时**发出请求，哪一个返回的信息先到达，则使用哪一个查询得到的结果。

### 顶级域名服务器

也称为 “TLD 域名服务器”。

顶级域名服务器负责管理在该顶级域名下注册的所有二级域名。或者说，顶级域名服务器中记录了属于它的各个权威域名服务器的 IP 地址。

### 权威域名服务器

权威域名服务器是保存 DNS 名称记录（包括 A、AAAA 和 CNAME）的服务器。

权威域名服务器包含特定于其服务域名的信息。它可为递归解析器提供在 DNS A 记录中找到的服务器的 IP 地址；或者如果该域具有 CNAME 记录，它将为递归解析器提供一个别名域，这时递归解析器将必须执行全新 DNS 查找，以便从权威域名服务器获取记录（通常为包含 IP 地址的 A 记录）。

### DNS 支持 TCP 和 UDP 协议

- [「为什么 DNS 使用 UDP 协议」](https://draveness.me/whys-the-design-dns-udp-tcp/)

&gt; 实际上，DNS 不仅使用了 UDP 协议，也使用了 TCP 协议，不过在具体介绍今天的问题之前，我们还是要对 DNS 协议进行简单的介绍：DNS 查询的类型不止包含 A 记录、CNAME 记录等常见查询，还包含 AXFR 类型的特殊查询，这种特殊查询主要用于 **DNS 区域传输**，它的作用就是在多个命名服务器之间快速迁移记录，由于查询返回的响应比较大，所以会使用 TCP 协议来传输数据包。
&gt; ……
&gt; 我们可以简单总结一下 DNS 的发展史，1987 年的 RFC1034 和 RFC1035 定义了最初版本的 DNS 协议，刚被设计出来的 DNS 就会同时使用 UDP 和 TCP 协议，对于绝大多数的 DNS 查询来说都会使用 UDP 数据报进行传输，TCP 协议只会在区域传输的场景中使用，其中 UDP 数据包只会传输最大 512 字节的数据，多余的会被截断；两年后发布的 RFC1123 预测了 DNS 记录中存储的数据会越来越多，同时也第一次显式的指出了**发现 UDP 包被截断时应该通过 TCP 协议重试**。
&gt; 过了将近 20 年的时间，由于互联网的发展，人们发现 IPv4 已经不够分配了，所以**引入了更长的 IPv6**，DNS 也在 2003 年发布的 RFC3596 中进行了协议上的支持；随后发布的 RFC5011 和 RFC6376 增加了在鉴权和安全方面的支持，但是也带来了巨大的 DNS 记录，UDP 数据包被截断变得非常常见。
&gt; RFC6891 提供的 DNS 扩展机制能够帮助我们在一定程度上解决大数据包被截断的问题，减少了使用 TCP 协议进行重试的需要，但是由于**最大传输单元的限制**，这并不能解决所有问题。
&gt; DNS 出现之后的 30 多年，RFC7766 才终于提出了使用 TCP 协议作为主要协议来解决 UDP 无法解决的问题，TCP 协议也不再只是一种重试时使用的机制，随后出现的 DNS over TLS 和 DNS over HTTP 也都是对 DNS 协议的一种补充。

### 全局流量管理 GTM

- [「全局流量管理产品原理」](https://help.aliyun.com/document_detail/189591.html?spm=a2c4g.11186623.6.640.3198229dgDSHXU)，2020-12-11，阿里云

&gt; 全局流量管理（GTM）支持用户就近接入、高并发负载均衡、健康检查与故障切换，可以帮助企业在短时间内构建同城多活与异地灾备的容灾架构。
&gt; GTM 属于 DNS 级别的服务，使用 DNS 向向用户返回特定的服务地址，然后客户端用户直接连接到服务地址。

当递归解析器接收到响应的 CNAME 结果后，会再执行一遍 DNS 查找过程，得到 GTM 服务器的 IP 地址。递归解析器向 GTM 服务器发送请求，GTM 收到请求后，会根据运行机制和预配置策略向递归解析器响应最终应用服务的 IP 地址。

递归解析器将最后一次查询得到的 IP 地址作为访问 URL 的最终地址，返回给浏览器，同时缓存到本地。浏览器使用此 IP 地址直接向应用服务器发起网络连接，开始进行业务通信。

## 通过 ARP 获取 MAC 地址

- [「地址解析协议」](https://zh.wikipedia.org/wiki/%E5%9C%B0%E5%9D%80%E8%A7%A3%E6%9E%90%E5%8D%8F%E8%AE%AE)，Wikipedia
- [「图解 ARP 协议（四）代理 ARP：善意的欺骗」](https://blog.51cto.com/chenxinjie/1961255)，2017-09-01

经过传输层和网络层封装后的包含有 HTTP 报文的数据包，现在来到了链路层。在这里，还将为它加上 MAC 头部。

&gt; 在**以太网协议**（在点对点协议 PPP 中，知道 IP 地址就可以进行通信；本文的讨论基于以太网协议）中规定，同一局域网中的一台主机要和另一台主机进行直接通信，必须要知道目标主机的 MAC 地址……另外，当发送主机和目的主机不在同一个局域网中时，即便知道对方的 MAC 地址，两者也不能直接通信，必须经过**路由转发**才可以。

如何将已知的目标主机的 IP 地址，转换为数据链路传输需要的 MAC 地址？在 IPv4 中，这通过地址解析协议 ARP（Address Resolution Protocol）实现；在 IPv6 中，使用邻居发现协议 NDP（Neighbor Discovery Protocol）代替 ARP。

以 ARP 协议为例，它通过 ARP 请求和 ARP 响应报文确定 MAC 地址。ARP 请求报文是一种广播报文，**局域网内**的所有主机都可以收到。当某一个主机发现请求报文中的 IP 地址为自己的 IP 地址，就会发送 ARP 响应报文给发出请求报文的主机。

大多数情况下，我们需要与不在同一个局域网的主机通信，但由于每个网段都是独立的广播域，没法直接向互联网上的其它主机发送广播报文，该怎么办？网关设备就在这里大显身手了。

当主机**已设置网关**时，主机设置的网关设备将接收到 ARP 请求报文，以路由器为例：路由器发现 ARP 请求报文中的 IP 地址不属于当前局域网，就把自己的 MAC 地址响应给请求主机。后续请求主机直接使用这个 MAC 地址，将数据包传输给路由器。而数据包通过路由器的**路由转发**功能，最终顺利抵达互联网上对应 IP 地址的主机。

当主机**未设置网关**时，可以使用 **代理 ARP**（ARP Proxy）机制。局域网内的所有网关设备将接收到 ARP 请求报文，还是以路由器为例：路由器发现 ARP 请求报文中的 IP 地址不属于当前局域网，而是属于自己**已知的**另一个网段上的某台主机，就将自己作为代理，把自己的 MAC 地址响应给请求主机。后续请求主机直接使用这个 MAC 地址，通过路由器代理，就可以访问到局域网外的目标主机了。与 ARP 相比，代理 ARP 有如下局限：

- 代理 ARP 需要有目标网关的信息；
- 代理 ARP 每次访问新的外网地址时，都需要发送一次 ARP 请求；
- 代理 ARP 受限于沿途网络设备。例如部分路由器可能不支持此功能，而支持此功能的路由器在默认情况下一般没有启用代理 ARP。

在跨网段通信中，无论使用 ARP 还是代理 ARP，发出 ARP 请求的主机总会收到网关的 MAC 地址作为响应。

因此，在实际网络中，无论是局域网内通信，还是跨网段通信，绝大多数情况下还是使用的是 ARP，而非代理 ARP。代理 ARP 是对 ARP 的补充，是 ARP 的拓展使用。

言归正传，假如我们使用的是 IPv4 网络，通过 ARP 协议，我们将收到主机设置的网关设备的 MAC 地址，这样我们就顺利地为数据包添加了 MAC 头部。现在，完整的数据包就可以从主机传输到网关设备上，再驶入互联网的快车道，最终抵达服务器了。

### RARP 和 IARP

- [「图解 ARP 协议（六）RARP 与 IARP：被遗忘的兄弟协议」](https://zhuanlan.zhihu.com/p/29081692)，2017-09-05

RARP 即反向 ARP（Reverse ARP），功能与 ARP 恰巧相反，用来实现 MAC 地址到 IP 地址的映射。

一个简单的例子是，当一台主机刚刚接入网络，这时它还没有局域网分配的内网 IP 地址。通过 RARP，它可以向局域网发送广播，广播包含自己的 MAC 地址，如果局域网内有 RARP 服务器且**记录有此 MAC 地址的映射 IP 地址**，那么它将接收到 RARP 响应，于是主机就拥有了自己的 IP 地址。

RARP 有这些特性：

- RARP 服务器必须提前绑定 MAC 地址和 IP 地址。如果没有提前绑定，则服务器不会发回响应；
- RARP 服务器只能给请求的主机分配 IP 地址，不包括网关、DNS 等其它信息。

在后来，有了启动协议 BOOTP，又有了现在最常用的[动态主机设置协议 DHCP](https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol)。

&gt; RARP 是一种逝去的地址分配技术，是 BOOTP 和 DHCP 的鼻祖，目前我们的电脑基本不会用到这个协议，只有部分无盘工作站等情况需要用到。

IARP 即逆向 ARP（Inverse ARP），在帧中继网络（广域网）中实现 DLCI 到 IP 地址的映射。在帧中继网络中，它的功能**类似于**以太网中 MAC 地址到 IP 地址的映射。

随着广域网技术的更迭，帧中继技术正慢慢被被其它技术所替代。因此 IARP 作为帧中继技术中的一环，在现实中的使用也愈来愈少。

## 使用 TLS 与服务器建立安全的 TCP 连接

在发送包含 HTTP 报文的数据包之前，客户端还要先通过三次握手与服务器建立 TCP 连接，这是为了保证数据传输的**可靠性**。在前面，我们已经得到了服务器的 MAC 地址，因此包含建立 TCP 连接请求报文的数据包可以顺利发送到服务器。

1. TCP 第一次握手，客户端主动向服务器发送 TCP 请求报文。设置其首部：`SYN = 1, seq = x`。其中，x 和下面步骤中的 y 为随机值。
2. TCP 第二次握手，服务器监听请求，当接收到客户端的请求报文时，若同意连接请求，则发回确认报文。设置其首部：`SYN = 1, ACK = 1, ack = x + 1, seq = y`。
3. TCP 第三次握手，客户端收到确认报文，通知上层应用（即我们的浏览器）连接已建立，并向服务器发送确认报文。设置其首部：`ACK = 1, ack = y + 1`。服务器接收到确认报文后，也通知其上层应用连接已建立。

由于前面我们提到的 HSTS 机制，我们的 HTTP 请求将基于 HTTPS 协议。严格来说，HTTPS 并非应用层的一种新协议，它只是将 HTTP 协议的**通信接口**部分使用**传输层安全性协议 TLS**（Transport Layer Security）代替罢了。通常，HTTP 直接与 TCP 通信。当使用 TLS 时，HTTP 先与 TLS 通信，再由 TLS 与 TCP 通信。

在这种情况下，为了保证数据传输的**安全性**，客户端还要与服务器协商 TLS 协议参数，这个过程通常称为 TLS 握手。在 TLS 1.0, 1.1 及 1.2 版本中，握手有四次；而在 TLS 1.3 版本中，握手只需要三次。TLS 握手是在 TCP 连接建立之后进行的。

以 [TLS 1.3 协议](https://datatracker.ietf.org/doc/html/rfc8446)为例，握手过程如下所示：

```plaintext
       Client                                           Server

Key  ^ ClientHello
Exch | + key_share*
     | + signature_algorithms*
     | + psk_key_exchange_modes*
     v + pre_shared_key*       --------&gt;
                                                  ServerHello  ^ Key
                                                 + key_share*  | Exch
                                            + pre_shared_key*  v
                                        {EncryptedExtensions}  ^  Server
                                        {CertificateRequest*}  v  Params
                                               {Certificate*}  ^
                                         {CertificateVerify*}  | Auth
                                                   {Finished}  v
                               &lt;--------  [Application Data*]
     ^ {Certificate*}
Auth | {CertificateVerify*}
     v {Finished}              --------&gt;
       [Application Data]      &lt;-------&gt;  [Application Data]

              +  Indicates noteworthy extensions sent in the
                 previously noted message.

              *  Indicates optional or situation-dependent
                 messages/extensions that are not always sent.

              {} Indicates messages protected using keys
                 derived from a [sender]_handshake_traffic_secret.

              [] Indicates messages protected using keys
                 derived from [sender]_application_traffic_secret_N.
```

这样，我们的客户端和服务器建立了基于 TLS 1.3 的安全 TCP 连接，是时候传输数据了！

### TLS 与 SSL

- [「传输层安全性协议」](https://zh.wikipedia.org/wiki/%E5%82%B3%E8%BC%B8%E5%B1%A4%E5%AE%89%E5%85%A8%E6%80%A7%E5%8D%94%E5%AE%9A)，Wikipedia

在日常使用中，我们经常会说 SSL 或 TLS/SSL，那么 TLS 和 SSL 之间有什么关系呢？

原来，**安全套接层 SSL**（Secure Sockets Layer） 是 TLS 的前身。TLS 基于 SSL 3.0 协议，是 SSL 协议标准化后的协议名。由于 SSL 3.0 设计中的缺陷，在 2015 年 6 月，[RFC 7568](https://datatracker.ietf.org/doc/html/rfc7568) 宣布弃用 SSL 3.0。

目前最新的 TLS 1.3 协议在 2018 年 8 月发表的 [RFC 8446](https://datatracker.ietf.org/doc/html/rfc8446) 中定义。而较老的 TLS 1.0 和 TLS 1.1 也已于 2021 年 3 月，在 [RFC 8996](https://datatracker.ietf.org/doc/html/rfc8996) 中宣告被弃用。

结论是，理论上我们现在用的加密协议大抵都是 TLS，在讨论中直接使用 TLS 即可。

### TLS 1.3 的进步

- [「A Detailed Look at RFC 8446 (a.k.a. TLS 1.3)」](https://blog.cloudflare.com/rfc-8446-aka-tls-1-3/)，2018-08-11

TLS 已经存在相当多的问题：例如代码缺乏测试，稳健性较低；存在许多设计缺陷，出现很多漏洞等。

在近些年来，互联网上一直存在一个主要趋势，即全面启用 HTTPS。这可以保护用户的安全，但会导致连接速度变慢。自 TLS 标准化以来，在发送加密数据之前，客户端到服务器的握手请求会进行两次往返（或者会话恢复连接时进行一次往返）。与单独的 HTTP 相比，HTTPS 中 TLS 握手的额外成本可能带来潜在的问题，并对以性能为中心的应用产生负面影响。在 [TLS 1.2 协议](https://datatracker.ietf.org/doc/html/rfc5246)中，握手过程如下所示：

```plaintext
Client                                                Server

ClientHello                   --------&gt;
                                                 ServerHello
                                                Certificate*
                                          ServerKeyExchange*
                                         CertificateRequest*
                              &lt;--------      ServerHelloDone
Certificate*
ClientKeyExchange
CertificateVerify*
[ChangeCipherSpec]
Finished                      --------&gt;
                                          [ChangeCipherSpec]
                              &lt;--------             Finished
Application Data              &lt;-------&gt;     Application Data

* Indicates optional or situation-dependent messages that are not
always sent.
```

IETF 对 TLS 1.2 的过时设计和两次往返开销不满意，着手定义新版本的 TLS，即 TLS 1.3，旨在解决如下的主要问题：

- 减少握手延迟；
- 加密更多的握手信息；
- 提高对跨协议攻击的恢复能力；
- 删除遗留的功能。

在过去的二十年里，对密码学的研究帮助人们学到更多关于如何编写**更安全的加密协议**的知识。TLS 1.3 的设计目标之一就是删除潜在的危险元素，纠正过去的错误设计。例如：

- **移除 RSA 密钥交换模式**，仅保留 Diffie-Hellman（下简称 DH）密钥协议。RSA 模式存在两个严重的问题，一是它不是前向加密（forward secret），意味着如果有人记录下加密的会话，如果在某天获取到服务器的私钥，就可以对会话进行破解。二是存在难以修复的漏洞，可以参见 [ROBOT 攻击](https://robotattack.org/)。删除 RSA 模式，只保留 DH 密钥协议，带来了一些性能优势，我们在后面进行讨论。
- **提供更少的可选项**。在密码学中，提供太多的选项可能导致更多的错误。这个原则在选择 DH 密钥协议的参数时尤为明显。该协议的安全性取决于选择的 DH 参数值，它一方面要为较大的值，另一方面需要具有某些[正确的数学属性](https://arstechnica.com/information-technology/2016/01/high-severity-bug-in-openssl-allows-attackers-to-decrypt-https-traffic/)。在以前版本的 TLS 中，DH 参数由参与者决定；而在 TLS 1.3 版本中，则将参数限制为已知安全的值，减少用户的可选项。

更多关于安全性的改进可以访问该小节开头的[参考博客](https://blog.cloudflare.com/rfc-8446-aka-tls-1-3/)。下面我们来看看 TLS 1.3 在**性能表现上的优势**。

在 DH 密钥协议中，客户端和服务器都从创建公钥-私钥对开始，然后交换各自的公钥，并根据自己的私钥和对方的公钥生成最终的密钥。最终的密钥自始至终都不会通过网络传输，DH 算法通过数学定律保证双方算出的结果一致。接下来，客户端和服务器就可以使用这个密钥对数据进行加密和解密。

TLS 1.3 使用这样一个更简单的密钥协商模式和一组更少的密钥协商选项，这意味着每个连接都将使用基于 DH 的密钥协议，服务器支持的 DH 参数更容易被猜到。有限的选择使得客户端可以在第一条消息中就发送自己的公钥，而无需等待服务器确认支持的类型。

在服务器不支持客户端使用协商选项的罕见情况下，服务器可以发送 `HelloRetryRequest` 的消息，让客户端知道自己支持哪些协商选项组。

作为小结：

&gt; TLS 1.3 is a modern security protocol built with modern tools like formal analysis that retains its backwards compatibility. It has been tested widely and iterated upon using real world deployment data. It&apos;s a cleaner, faster, and more secure protocol ready to become the de facto two-party encryption protocol online.
&gt; It is one the best recent examples of how it is possible to take 20 years of deployed legacy code and change it on the fly, resulting in a better internet for everyone. TLS 1.3 has been debated and analyzed for the last three years (2015 - 2018) and it&apos;s now ready for prime time.

### 对称密钥加密，非对称密钥加密与混合加密

简单来说，在**对称密钥加密**中，对数据的加密和解密都使用同一个密钥。相较于非对称密钥加密，它的速度更快；但是由于密钥在传输过程中容易被获取，因此其安全性较低。

在**非对称密钥加密**（或公开密钥加密）中，使用一对密钥进行加密和解密，分别为公开密钥和私有密钥。公开密钥所有人都可以获得，客户端使用公开密钥对数据进行加密，服务器使用私有密钥对数据进行解密。同样，服务器对响应的数据使用私有密钥加密，客户端则可以通过公开密钥进行解密。相较于对称密钥加密，只要保管好私有密钥，就能保证客户端传输的消息不被破解，因此它的安全性更高；由于算法和过程更为繁琐，因此其速度较慢。

HTTPS 采用的是**混合加密**机制——在 TLS 1.3 以前，客户端首先使用服务器提供的公钥，加密一个随机值，然后将它传输给服务器。服务器使用私钥解密，获得随机值，然后使用与客户端相同的密钥生成算法，基于这个随机值和之前握手中创建的另外两个随机值，生成与客户端相同的密钥，之后客户端和服务器就可以使用这把对称密钥进行通信了。在 TLS 1.3 中，基于 DH 密钥协议，不再赘述。这样，客户端和服务器之间的通信就兼顾了对称密钥加密的高效性和非对称密钥加密的安全性。

## 欢迎访问我的博客

现在，浏览器知道了已经与远方的服务器建立好了安全可靠的传输通道，于是将 HTTP 请求信息打包好，传输到服务器上的 443 端口。服务器使用密钥解密获得其中的信息，发现是请求我博客的 HTTP 报文，遂转发给相应的 HTTP 服务。最终将我们所需的 HTML, CSS, JS 以及相关的静态文件发送给浏览器，浏览器再把它们渲染出来。

Oh, Welcome to visit my blog！

## 参考文章

除了正文中特别罗列出来的网站文章，还包括：

- [「在浏览器地址栏输入一个 URL 后回车，背后会进行哪些技术步骤？」](https://www.zhihu.com/question/34873227/answer/518086565)，2020-03-28
- [「上古面试题——浏览器地址栏输入后回车会发生什么」](https://segmentfault.com/a/1190000021000934)，2019-11-14
- [「搞懂这 9 步，DNS 访问原理就明明白白了」](https://segmentfault.com/a/1190000039650564)，2021-03-17
- [「DNS 查询机制」](https://segmentfault.com/a/1190000039406281)，2021-03-13
- [「DNS 服务器有哪些不同类型？」](https://www.cloudflare.com/zh-cn/learning/dns/dns-server-types/)，Cloudflare
- [「36 张图详解 ARP ：网络世界没有我，你哪也别想去」](https://zhuanlan.zhihu.com/p/379015679)，2021-06-08
- [「图解 HTTP」](https://www.ituring.com.cn/book/1229)，\[日\]上野宣 著，于均良 译
- [「详解 TCP 三次握手以及 TLS/SSL 握手」](https://ocdman.github.io/2018/11/02/%E8%AF%A6%E8%A7%A3TCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E4%BB%A5%E5%8F%8ATLS-SSL%E6%8F%A1%E6%89%8B/)，2018-11-02
- [「HTTPS 详解二：SSL / TLS 工作原理和详细握手过程」](https://segmentfault.com/a/1190000021559557)，2020-01-19
</content:original-text><content:updated-at>2021-07-23T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[为 Archer 主题更换字体]]></title><description><![CDATA[看腻了原先的字体，亦或是想满足独树一帜的设计欲望？不妨更换一下博客的字体吧！本文将基于 Hexo 和主题 Hexo-Theme-Archer 展示如何更换博客的中文字体。 引入字体文件

这里提供两种引入的思路，一种是 CDN 引入，一种是本地引入。建议通过 CDN 的方式引入，可以大大提高加载效率。

引入 CDN 字体文件

以更换字体为思源黑体（Google 字体上叫 ，Adobe 版本叫 ）为例…]]></description><link>https://blog.towind.fun/posts/web-font-for-hexo-theme-archer</link><guid isPermaLink="false">web-font-for-hexo-theme-archer</guid><category><![CDATA[前端开发]]></category><pubDate>Sat, 26 Jun 2021 00:00:00 GMT</pubDate><content:original-text>
看腻了原先的字体，亦或是想满足独树一帜的设计欲望？不妨更换一下博客的字体吧！本文将基于 Hexo 和主题 [Hexo-Theme-Archer](https://github.com/fi3ework/hexo-theme-archer) 展示如何更换博客的中文字体。

## 引入字体文件

这里提供两种引入的思路，一种是 CDN 引入，一种是本地引入。建议通过 CDN 的方式引入，可以大大提高加载效率。

### 引入 CDN 字体文件

以更换字体为思源黑体（Google 字体上叫 `Noto Sans`，Adobe 版本叫 `Source Han Sans`）为例，考虑到中文站点面向的读者在国内，无法直接下载思源黑体这款 Google 字体，因此考虑通过 CDN 的方式引入它。据笔者测试，目前有这四个 CDN 站点可以提供稳定的服务：

- `https://fonts.googleapis.cnpmjs.org`
- `https://fonts.font.im`，可参考：http://www.googlefonts.cn/old
- `https://fonts.proxy.ustclug.org`
- `https://fonts.loli.net`，可参考：https://sb.sb/blog/css-cdn

使用方法非常简单，在 Google 字体上选择[思源黑体简体中文版本](https://fonts.google.com/specimen/Noto+Sans+SC?subset=chinese-simplified#standard-styles)，再选择需要的字重如 `Regular 400`，如果使用 `&lt;link&gt;` 的方式引入，则代码如下所示：

```html
&lt;link rel=&quot;preconnect&quot; href=&quot;https://fonts.googleapis.com&quot; /&gt;
&lt;link rel=&quot;preconnect&quot; href=&quot;https://fonts.gstatic.com&quot; crossorigin /&gt;
&lt;link
  href=&quot;https://fonts.googleapis.com/css2?family=Noto+Sans+SC&amp;display=swap&quot;
  rel=&quot;stylesheet&quot;
/&gt;
```

使用 CDN 加速的方式，只需要将上面代码中的 `https://fonts.googleapis.com` 部分更换为前边对应 CDN 链接即可。例如使用中国科学技术大学的镜像站加速，应修改代码如下：

```html
&lt;link rel=&quot;preconnect&quot; href=&quot;https://fonts.proxy.ustclug.org&quot; /&gt;
&lt;link rel=&quot;preconnect&quot; href=&quot;https://fonts.gstatic.com&quot; crossorigin /&gt;
&lt;link
  href=&quot;https://fonts.proxy.ustclug.org/css2?family=Noto+Sans+SC&amp;display=swap&quot;
  rel=&quot;stylesheet&quot;
/&gt;
```

然后将上面这段代码放到 Archer 主题目录下的 `layout/_partial/base-head.ejs` 代码片段中即可。

这段代码同时定义了一个名为 `Noto Sans SC` 的字体族，在后面就可以直接使用这个值引入字体了。

### 直接引入本地字体文件

以更换字体为未来荧黑为例，首先，去[开源的 Realease 页](https://github.com/welai/glow-sans/releases)下载字体文件。其中，以 `GlowSansSC` 开头的字体为未来荧黑的简体中文字体。这里我选择下载了 `GlowSansSC-Normal-v0.92.zip`。

解压之，可以很多不同**字重**的 `.otf` 字体文件，这里我选择了 `GlowSansSC-Normal-Book.otf`。将字体文件复制到 Archer 主题的 `source/font` 目录下。

编辑 Archer 主题目录下的 `src/scss/_variables.scss` 文件，添加新的 `@font-face` 如下：

```scss
@font-face {
  font-family: &quot;Glow Sans SC&quot;;
  src: url(&quot;../font/GlowSansSC-Normal-Book.otf&quot;);
}
```

我们定义了一个名为 `Glow Sans SC` 的字体族，在后面就可以直接使用这个值引入字体了。

其中，url 路径 `../font/GlowSansSC-Normal-Book.otf` 是如何得来的呢？我们知道，在执行 `hexo g` 时，会将主题目录下的 `source` 中的文件拷贝到博客根目录下的 `public` 目录中。而根据 Archer 主题的 gulp 生成规则，编译好的 `.css` 文件存放在主题目录下的 `source/css` 中。因此，为了最终正确指向博客根目录下 `public/font/GlowSansSC-Normal-Book.otf` 文件，应该设置 url 为上一级目录下的 `font` 目录。

这种方式的优点是：配置方便。

但缺点也很明显：慢！尤其对于大多数人建立个人博客时，会使用自己的“小水管”服务器，字体文件要加载老半天才能正常显示出来（包括中文的字体文件大约 9 MB 左右，而下载速度大约为 200 KB/s）。此外，由于 Archer 主题在开头处就会引入包含字体配置的 CSS 文件，要留待字体加载完成后才渲染后续内容，极大影响首次浏览体验。

## 更换博客字体

接下来，修改 Archer 主题 `src/scss/_variables.scss` 文件中的 `$base-font-family` 变量：

```scss
$base-font-family:
  &quot;Noto Sans SC&quot;,
  -apple-system,
  BlinkMacSystemFont,
  &quot;Helvetica Neue&quot;,
  Arial,
  &quot;PingFang SC&quot;,
  &quot;Hiragino Sans GB&quot;,
  STHeiti,
  &quot;Microsoft YaHei&quot;,
  &quot;Microsoft JhengHei&quot;,
  &quot;Source Han Sans SC&quot;,
  &quot;Noto Sans CJK SC&quot;,
  &quot;Source Han Sans CN&quot;,
  &quot;Noto Sans SC&quot;,
  &quot;Source Han Sans TC&quot;,
  &quot;Noto Sans CJK TC&quot;,
  &quot;WenQuanYi Micro Hei&quot;,
  SimSun,
  sans-serif;
```

如上所示，在 `$base-font-family` 最前面加上 `Noto Sans SC`（或 `Glow Sans SC`）就可以更换字体为思源黑体（或未来黑体）了。浏览器按顺序读取并使用字体，如果前面的字体没有，则依次使用后面的字体。

特别的，Archer 主题提供了另一个变量 `$feature-font-family`，渲染为主题中 Profile，Intro，Footer 等地方的字体，如果希望，也可以更换。

在上传到 Github 之前，别忘了在主题根目录执行 `npm run build`。

## 压缩中文字体

**笔者并未成功在 Archer 主题中实现**，这里只是给出实现思路和方法。

适用于在[**本地引入字体**](#直接引入本地字体文件)的方式。

因为中文字体的编码相比英文字体多很多，导致中文字体文件通常特别大，十分耗费网络资源，还会降低用户体验。以 Archer 主题自带的字体为例，英文字体 `Oswald-Regular.ttf` 只有 89 KB 大小，而刚刚本地引入示例中的中文字体 `GlowSansSC-Normal-Book.otf` 有 8971 KB 之大。如果能对中文字体进行压缩，将能大大提高用户体验。

一款国人开源的智能 WebFont 压缩工具 [font-spider](https://github.com/aui/font-spider) 能够满足我们的需求。

这款工具能够获取网页中使用到的中文文字，然后从源字体文件中提取出来这些文字，生成一个只包含这些文字的字体文件。而我们只需要引入这个生成的字体文件，就可以实现压缩中文字体了！

为了能使用 Github Actions 自动集成部署博客，这里我在博客根目录引入了 font-spider 作为项目的开发依赖。

```bash
yarn add -D font-spider
```

由于 font-spider 需要使用 `.ttf` 格式的字体进行转换，因此首先需要把之前下载的 `.otf` 格式转换为 `.ttf` 格式，这可以借助于这个[格式转换网站](https://convertio.co/zh/otf-ttf/)。将转换完成后的 `GlowSansSC-Normal-Book.ttf` 字体文件放在主题目录下的 `source/font` 中。

编辑主题目录下的 `src/scss/_variables.scss` 文件，修改刚刚的样式表属性：

```scss
@font-face {
  font-family: &quot;Glow Sans SC&quot;;
  src: url(&quot;../font/GlowSansSC-Normal-Book.eot&quot;);
  src:
    url(&quot;../font/GlowSansSC-Normal-Book.eot?#font-spider&quot;)
      format(&quot;embedded-opentype&quot;),
    url(&quot;../font/GlowSansSC-Normal-Book.woff2&quot;) format(&quot;woff2&quot;),
    url(&quot;../font/GlowSansSC-Normal-Book.woff&quot;) format(&quot;woff&quot;),
    url(&quot;../font/GlowSansSC-Normal-Book.ttf&quot;) format(&quot;truetype&quot;),
    url(&quot;../font/GlowSansSC-Normal-Book.svg&quot;) format(&quot;svg&quot;);
}
```

在主题根目录执行 `yarn build` 生成打包好的样式表文件。

回到博客根目录，编辑 `package.json`，添加脚本：

```json
&quot;scripts&quot;: {
  &quot;font-spider&quot;: &quot;font-spider public/*.html&quot;
}
```

其中，`public` 是 Hexo 生成静态博客文件的默认目录。

在博客根目录分别执行：

```bash
# 生成静态博客文件
hexo g
# 压缩中文字体文件
yarn font-spider
```

特别的，假如使用 Github Actions 一类的 CI 系统，别忘了将压缩中文字体文件的步骤加入到 CI 配置中。

## 文末碎碎念

笔者最开始使用的是 Github 上[开源的思源黑体](https://github.com/adobe-fonts/source-han-sans)，通过本地引入的方式加载字体。而后选择使用了 CDN 引入 Google 字体中的思源黑体。发现两者竟然有不小的区别，总的来说前者更好看一些……均使用 `Regular` 字重，页面效果如下：

Github 开源版本：

![Github 开源版本思源黑体](./web-font-for-hexo-theme-archer/Github-Open-Source-Version.png)

Google 字体版本：

![Google 字体版本思源黑体](./web-font-for-hexo-theme-archer/Google-Fonts-Version.png)

前者字形更高，颜色更淡雅一些。可能需要额外的配置？考虑到页面加载速度，暂且选用 CDN 加速版本。

## 参考文章

- [網頁是否安裝思源黑體、中文字型的考量﹍影響載入速度的因素及作法分析](https://www.wfublog.com/2019/01/noto-sans-serif-traditional-chinese-web-font_11.html)
</content:original-text><content:updated-at>2021-08-10T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[使用基于 Docker 的 Nginx 部署静态网页项目]]></title><description><![CDATA[现在，我已经安装了 Docker，并拉取了 Nginx 的镜像。除此之外，我也购买了域名，完成了备案，并且为域名配置了 SSL。一切准备就绪，那么我该怎么将我的静态网页项目在 Linux 主机上通过 Nginx 部署，最终实现域名访问呢？ 本文以部署我的个人博客页面为例，介绍如何使用基于 Docker 的 Nginx 部署静态网页项目。

准备静态网页项目

为了更方便管理网页项目…]]></description><link>https://blog.towind.fun/posts/website-deployment-docker-nginx</link><guid isPermaLink="false">website-deployment-docker-nginx</guid><category><![CDATA[技术琐事]]></category><pubDate>Fri, 25 Jun 2021 00:00:00 GMT</pubDate><content:original-text>
现在，我已经安装了 Docker，并拉取了 Nginx 的镜像。除此之外，我也购买了域名，完成了备案，并且为域名配置了 SSL。一切准备就绪，那么我该怎么将我的静态网页项目在 Linux 主机上通过 Nginx 部署，最终实现域名访问呢？

本文以部署我的个人博客页面为例，介绍如何使用基于 Docker 的 Nginx 部署静态网页项目。

## 准备静态网页项目

为了更方便管理网页项目，可以在主机根目录下新建一个目录，例如 `www`：

```bash
sudo -i # 切换为管理员用户
cd /
mkdir www
```

现在，我已经有了一个完整的静态网页项目——我的[个人博客](https://github.com/LolipopJ/LolipopJ.github.io)。我的个人博客基于 Hexo，其中代码放在 `source` 分支，生成的静态网页文件放在 `master` 分支。首先通过 `git` 命令将静态网页文件克隆下来：

```bash
cd /www
git clone https://github.com/LolipopJ/LolipopJ.github.io.git -b master
```

## 准备 SSL 证书

我使用了腾讯云执行了备案操作，并申请了免费的 SSL 证书。参考[腾讯云官方文档](https://cloud.tencent.com/document/product/400/35244)，下面执行安装 SSL 证书操作。

将下载的证书文件传入 Linux 主机中并解压。以 SSL 证书文件压缩包 `blog.towind.fun.zip` 为例：

```bash
# 将文件解压到当前目录下的 blog.towind.fun 目录中
unzip blog.towind.fun.zip -d blog.towind.fun
```

可以将 `blog.towind.fun/Nginx` 目录下的文件放置到 `/www/cert` 目录下：

```bash
mkdir /www/cert
cp blog.towind.fun/Nginx/* /www/cert
```

之后，只需要将 `/www/cert` 目录挂载到 Nginx 的容器中，再通过 Nginx 配置访问即可。

## 创建 Nginx 容器

从 Nginx 1.19 版本开始，允许在配置中自定义环境变量，只需要编写一个 `docker-compose.yml` 文件。`docker-compose` 是用来将 Docker 自动化的命令，如果还没有，需要先安装它：

```bash
sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
```

可以用 `docker-compose --version` 命令测试安装的结果。

在默认情况下，Nginx 会寻找容器的 `/usr/share/nginx/html` 目录下的网页文件，因此我们需要把网页文件放到这个目录下去。最简单的方式是通过挂载 volume 使得容器可以访问到我们的 `/www/LolipopJ.github.io` 目录。

在 `/www/LolipopJ.github.io` 目录下创建 `docker-compose.yml`，编辑如下：

```yml
version: &quot;1.0&quot;
services:
  blog:
    image: nginx:1.21.0
    container_name: blog-nginx
    restart: always
    volumes:
      - ./:/usr/share/nginx/html # 挂载当前静态网页文件目录
      - ./templates:/etc/nginx/templates # 挂载 Nginx 配置模板目录
      - /www/cert:/etc/nginx/cert # 挂载 SSL 证书目录
    ports:
      - 80:80
      - 443:443
    environment:
      - NGINX_HOST=blog.towind.fun
      - NGINX_HOST_SSL_CRT=cert/1_blog.towind.fun_bundle.crt
      - NGINX_HOST_SSL_KEY=cert/2_blog.towind.fun.key
```

由于 `nginx.conf` 读取文件时，默认以 `/etc/nginx` 为起始目录，因此当把 `/www/cert` 目录挂载到 `/etc/nginx/cert` 目录时，应当设置环境变量 `NGINX_HOST_SSL_CRT` 为 `cert/1_www.example.com_bundle.crt`。

在默认情况下，执行此文件后，将会读取 `templates`（对应容器中的 `/etc/nginx/templates`）目录下的模板文件，并将结果输出到容器中 `/etc/nginx/conf.d` 目录下。因此，可以在 `/www/LolipopJ.github.io` 目录下创建 `templates` 目录，并编写 `templates/default.conf.template` 文件如下：

```conf
server {
  listen 443 ssl;
  listen [::]:443 ssl;
  server_name ${NGINX_HOST};
  ssl_certificate ${NGINX_HOST_SSL_CRT};
  ssl_certificate_key ${NGINX_HOST_SSL_KEY};

  location / {
    root /usr/share/nginx/html;
    index index.html index.htm;
  }

  error_page 404 /404.html;

  error_page 500 502 503 504 /50x.html;
  location = /50x.html {
    root /usr/share/nginx/html;
  }
}

# 将 http 请求转为 https 请求
server {
  listen 80;
  listen [::]:80;
  server_name ${NGINX_HOST};
  return 301 https://$host$request_uri;
}
```

现在，在 `/www/LolipopJ.github.io` 目录下有我们编写好的 `docker-compose.yml` 和 `templates/default.conf.template` 文件；另外，为部署 https 服务所需的 ssl 证书文件在 `/www/cert` 目录下。那么最后只需要在 `/www/LolipopJ.github.io` 目录执行下面的命令即可：

```bash
docker-compose up -d
```

其中，`-d` 表示在后台运行容器。执行后，将拉起 Nginx 容器，并在容器中生成对应的 `/etc/nginx/conf.d/default.conf` 文件，供 `/etc/nginx/nginx.conf` 读取。

## 从浏览器访问

嘿！一切就绪，从浏览器访问我的博客吧！网址是：[https://blog.towind.fun](https://blog.towind.fun/)

当我的个人博客有更新时，可以通过 `git` 命令来拉取，然后重新执行 `docker-compose up -d` 即可：

```bash
cd /www/LolipopJ.github.io
git pull
docker-compose up -d
```

## 参考文档

- [How To Use the Official NGINX Docker Image](https://www.docker.com/blog/how-to-use-the-official-nginx-docker-image/)
</content:original-text><content:updated-at>2021-06-25T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[在 Linux 系统下启用 Project-V]]></title><description><![CDATA[下载 Release 在 Project-V 的 Github Releases 页面下载最新的二进制包。

本文以通用于 x86_64 机器的  为例。下载完成后传入 Linux 主机即可。

安装 Project-V

执行  命令：

文件将全部解压到当前目录。

创建软链接

执行  命令：

这里的  是压缩包里已编译好的二进制可执行文件。

配置 Project-V

编辑与  相同目录下的  文件…]]></description><link>https://blog.towind.fun/posts/linux-project-v</link><guid isPermaLink="false">linux-project-v</guid><category><![CDATA[后端开发]]></category><pubDate>Wed, 09 Jun 2021 00:00:00 GMT</pubDate><content:original-text>
## 下载 Release

在 Project-V 的 [Github Releases](https://github.com/v2fly/v2ray-core/releases) 页面下载最新的二进制包。

本文以通用于 x86_64 机器的 `v2ray-linux-64.zip` 为例。下载完成后传入 Linux 主机即可。

## 安装 Project-V

执行 `unzip` 命令：

```bash
unzip v2ray-linux-64.zip
```

文件将全部解压到当前目录。

## 创建软链接

执行 `ln` 命令：

```bash
ln -s /path/to/v2ray /usr/local/bin
```

这里的 `v2ray` 是压缩包里已编译好的二进制可执行文件。

## 配置 Project-V

编辑与 `v2ray` 相同目录下的 `config.json` 文件，配置如下：

```json
{
  // 前略
  // List of inbound proxy configurations.
  &quot;inbounds&quot;: [
    {
      // Port to listen on. You may need root access if the value is less than 1024.
      &quot;port&quot;: 1080, // 本机监听的端口，应为不加双引号的数字

      // IP address to listen on. Change to &quot;0.0.0.0&quot; to listen on all network interfaces.
      &quot;listen&quot;: &quot;127.0.0.1&quot;,

      // Tag of the inbound proxy. May be used for routing.
      &quot;tag&quot;: &quot;socks-inbound&quot;,

      // Protocol name of inbound proxy.
      &quot;protocol&quot;: &quot;socks&quot;,

      // Settings of the protocol. Varies based on protocol.
      &quot;settings&quot;: {
        &quot;auth&quot;: &quot;noauth&quot;,
        &quot;udp&quot;: false,
        &quot;ip&quot;: &quot;127.0.0.1&quot;
      },

      // Enable sniffing on TCP connection.
      &quot;sniffing&quot;: {
        &quot;enabled&quot;: true,
        // Target domain will be overriden to the one carried by the connection, if the connection is HTTP or HTTPS.
        &quot;destOverride&quot;: [&quot;http&quot;, &quot;tls&quot;]
      }
    }
  ],
  // List of outbound proxy configurations.
  &quot;outbounds&quot;: [
    {
      // Protocol name of the outbound proxy.
      &quot;protocol&quot;: &quot;vmess&quot;, // 使用的代理协议

      // Settings of the protocol. Varies based on protocol.
      &quot;settings&quot;: {
        &quot;vnext&quot;: [
          {
            &quot;address&quot;: &quot;V2RAY_SERVER_ADDRESS&quot;, // 代理的服务器地址
            &quot;port&quot;: 16823, // 代理服务器的端口，应为不加双引号的数字
            &quot;users&quot;: [
              {
                &quot;id&quot;: &quot;V2RAY_UUID&quot;, // 代理服务器的 UUID
                &quot;alterId&quot;: 64 // 代理服务器的 Alter Id，应为不加双引号的数字
              }
            ]
          },
          {
            // 添加更多的 VMESS 协议代理服务器
          }
        ]
      },

      // Tag of the outbound. May be used for routing.
      &quot;tag&quot;: &quot;vmess_serve&quot;
    }
  ]
  // 后略
}
```

对于新手，只需要掌握 `inbounds` 和 `outbounds` 的配置方式即可。

对于进阶使用，可以参考[官方文档](https://www.v2fly.org/)，或者更适合新手的[“白话文”文档](https://guide.v2fly.org/)。

## 启用 Project-V

保存修改后，在后台运行 `v2ray`：

```bash
v2ray &amp;
```

如果参考前面的默认配置，则它将监听在 `127.0.0.1:1080` 端口。

假如您想加速 `git` 克隆速度，则可以配置它的 `http(s).proxy`：

```bash
git config --global http.proxy socks5://127.0.0.1:1080
git config --global https.proxy socks5://127.0.0.1:1080
```

现在，使用 `git clone` 命令享受飞一般的速度吧。
</content:original-text><content:updated-at>2021-06-09T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[在 Euler 系统上离线安装 MySQL 5.7]]></title><description><![CDATA[查看系统 OS 及架构 以 Euler 系统为例，在终端上输入命令查看，可以通过  命令找到 rpm 包，再通过  查看系统 OS 及架构信息：

当然，也可以使用通用的  命令。

当前系统为 Euler 2.0 (SP5)，处理器架构为 x86_64。

下载 MySQL

Euler 2.0 系统基于 CentOS 7 开发，而 CentOS 7 由 Red Hat Enterprise…]]></description><link>https://blog.towind.fun/posts/euler-install-mysql</link><guid isPermaLink="false">euler-install-mysql</guid><category><![CDATA[后端开发]]></category><pubDate>Fri, 04 Jun 2021 00:00:00 GMT</pubDate><content:original-text>
## 查看系统 OS 及架构

以 Euler 系统为例，在终端上输入命令查看，可以通过 `rpm -qa | grep euleros-release` 命令找到 rpm 包，再通过 `rpm -qi ${包名}` 查看系统 OS 及架构信息：

```bash
[root@lolipop ~]# rpm -qa | grep euleros-release
euleros-release-2.0SP5-13.eulerosv2r7.x86_64
[root@lolipop ~]# rpm -qi euleros-release-2.0SP5-13.eulerosv2r7.x86_64
Name        : euleros-release
Version     : 2.0SP5
Release     : 13.eulerosv2r7
Architecture: x86_64
......
```

当然，也可以使用通用的 `uname -a` 命令。

当前系统为 Euler 2.0 \(SP5\)，处理器架构为 x86_64。

## 下载 MySQL

Euler 2.0 系统基于 CentOS 7 开发，而 CentOS 7 由 Red Hat Enterprise Linux 依照开放源代码规定发布的源代码所编译而成。因此在[此页面](https://downloads.mysql.com/archives/community/)下载 MySQL 的时候，其中的 Operating System 项应选择 `Red Hat Enterprise Linux / Oracle Linux`，OS Version 应选择 `Red Hat Enterprise Linux 7 / Oracle Linux 7 (x86, 64-bit)`。接下来，选择下载 `RPM Bundle` 即可。

例如，在浏览器访问 `https://downloads.mysql.com/archives/get/p/23/file/mysql-5.7.33-1.el7.x86_64.rpm-bundle.tar`，将自动开始下载 MySQL 5.7.33 适用于 Oracle Linux 7 的 x86_64 版本。

## 安装 MySQL

将下载好的档案包传输到 Linux 主机上或 Docker 容器里，解压之：

```bash
tar -xvf mysql-5.7.33-1.el7.x86_64.rpm-bundle.tar
```

按顺序安装这些 rpm 包：

```bash
rpm -ivh mysql-community-common-5.7.33-1.el7.x86_64.rpm
rpm -ivh mysql-community-libs-5.7.33-1.el7.x86_64.rpm
rpm -ivh mysql-community-client-5.7.33-1.el7.x86_64.rpm
rpm -ivh mysql-community-server-5.7.33-1.el7.x86_64.rpm
```

其它的包并非必须，而是开发时可能会用到的，暂时忽略即可。

## 初始化 MySQL

初始化 MySQL 数据库：

```bash
mysqld -I
```

该命令会初始化默认数据库并创建一个有随机密码的超级用户，密码会打印到 MySQL 的日志中。

如果初始化时出现 `Fatal error: Please read &quot;Security&quot; section of the manual to find out how to run mysqld as root!` 报错，可以强制使用 root 权限执行：

```bash
mysqld -I --user=root
```

接下来，为目录移除可读权限，这是因为 MySQL 为了安全考虑，会忽略到权限过高的文件：

```bash
chown -R mysql:mysql /var/lib/mysql
```

## 启动 MySQL 服务

配置完成后，就可以启动 MySQL 服务了：

```bash
service mysqld start
```

确保启动成功，您可以通过这个命令查看 MySQL 服务的状态：

```bash
service mysqld status
```

## 登录 MySQL

在之前的初始化过程中，我们生成了一个超级用户和它的随机登录密码。可以通过下面的命令查看这个随机密码：

```bash
grep -n &apos;password&apos; /var/log/mysqld.log
```

例如，打印结果如下：

```bash
Storage:~ # grep -n &apos;password&apos; /var/log/mysqld.log
7:2021-06-02T07:50:39.284449Z 1 [Note] A temporary password is generated for root@localhost: IN2Scm=ERki9
```

则默认的随机密码为：`IN2Scm=ERki9`，您应当保管好此密码不要泄露，或是修改为新的密码。

使用这个密码，我们就可以登录到 MySQL 中去了：

```bash
Storage:~ # mysql -uroot -pIN2Scm=ERki9
mysql: [Warning] Using a password on the command line interface can be insecure.
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 2
Server version: 5.7.33 MySQL Community Server (GPL)

Copyright (c) 2000, 2021, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.

mysql&gt;
```

此时已经可以正常执行 MySQL 数据库操作了。

假如需要修改密码，可以执行下面的语句：

```sql
mysql&gt; ALTER USER &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;${新的密码}&apos;;
```

&gt; 注意，新的密码默认情况下需要符合长度，且必须包括数字，小写或大写字母，以及特殊字符。
&gt; 尽管不推荐，您也可以设置密码复杂度属性，这样密码只需要满足长度要求即可使用：`SET GLOBAL validate_password_policy=0;`

## 授权 MySQL 远程连接

登录到 MySQL 中，执行下面的命令：

```sql
mysql&gt; GRANT all privileges ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;${您的密码}&apos;;
```

现在便可以使用其它设备远程连接 MySQL 数据库了。
</content:original-text><content:updated-at>2021-07-29T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[漫谈 JavaScript 类（Class）的使用]]></title><description><![CDATA[类（Class）是用于创建对象的模板，他们用代码封装数据以处理该数据，是面向对象编程方法的重要特性之一。JavaScript 中的  语法在 ES6 中引入，其底层实现基于原型（Prototype），系原型继承的语法糖（Syntactic Sugar）。 本博文将探讨 JavaScript 中如何使用类的相关知识，文章组织架构和内容基于 MDN 上关于类的章节。

定义类

类可以被看作一种…]]></description><link>https://blog.towind.fun/posts/js-use-class</link><guid isPermaLink="false">js-use-class</guid><category><![CDATA[技术琐事]]></category><pubDate>Thu, 20 May 2021 00:00:00 GMT</pubDate><content:original-text>
类（Class）是用于创建对象的模板，他们用代码封装数据以处理该数据，是面向对象编程方法的重要特性之一。JavaScript 中的 `class` 语法在 ES6 中引入，其底层实现基于原型（Prototype），系原型继承的语法糖（Syntactic Sugar）。

本博文将探讨 JavaScript 中**如何使用类**的相关知识，文章组织架构和内容基于 MDN 上关于类的[章节](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Classes)。

## 定义类

类可以被看作一种“特殊的函数”，和函数的定义方法一样，类的定义方法有两种：**类声明**和**类表达式**。

第一种方法是，直接使用 `class` 关键字声明类，即**类声明**的方法。

```js
class User {
  //
}
```

但是，与函数声明不同的是，使用类声明的方式**不会提升**。这意味着必须先声明类，再使用它。

```js
const u = new User(); // Uncaught ReferenceError: User is not defined

class User {
  //
}
```

另一种方法是，将 `class` 声明的类赋值给变量，即**类表达式**的方法。类表达式可以命名或匿名，其中，命名类表达式的名称（类的 `name` 属性）是该类体的局部名称。

```js
// 匿名类
let User = class {
  //
};
console.log(User.name); // User

// 命名类
let User = class Admin {
  //
};
console.log(User.name); // Admin
```

同样，使用类表达式的方式也**不会提升**。

定义类之后，就可以使用 `new` 关键字实例化类了。

```js
const u = new User();
```

## 构造函数

`constructor()` 方法或**构造函数**，是用于创建和初始化一个由 `class` 创建的对象的特殊方法，一个类只能拥有一个 `constructor()` 方法。

如果一个类中有构造函数，那么执行 `new` 创建实例时，将调用这个构造函数。

```js
class User {
  // 构造函数
  constructor(name, gender) {
    this.name = name;
    this.gender = gender;
  }
}

const u = new User(&quot;Ming&quot;, &quot;Male&quot;); // 初始化对象
console.log(u.name, u.gender); // Ming Male

const u2 = new User(&quot;Xiao&quot;); // 初始化赋值参数少于构造函数参数时
console.log(u2.name, u2.gender); // Xiao undefined
```

对于 `new` 创建实例时的每个参数，将依次赋值给构造函数。多余的参数将被忽略。

特别的，ES6 规定，子类的 `constructor()` 中必须使用 `super()` 调用父类的构造函数，否则会报错。一个合法的例子：

```js
class User {
  constructor(name, gender) {
    this.name = name;
    this.gender = gender;
  }
}

// 使用 extends 创建 User 的子类 Admin
class Admin extends User {
  constructor(name, gender, openId) {
    super(name, gender); // 调用父类的构造函数
    this.openId = openId;
  }
}

const a = new Admin(&quot;Ming&quot;, &quot;Male&quot;, &quot;xxx489&quot;);
console.log(a.name, a.gender, a.openId); // Ming Male xxx489
```

## 原型方法

在类体中可以声明函数方法。从底层实现来看，这些方法将会在对象的原型链上定义出来，故称作**原型方法**。

```js
class Rectangle {
  // Field declarations
  log = []; // 日志属性
  // Constructor
  constructor(height, width) {
    this.height = height;
    this.width = width;
  }
  // Getter 获取当前的面积
  get area() {
    return this.calcArea();
  }
  // Setter 修改 height 属性时添加日志
  set height(h) {
    this._height = h; // 如果为 this.height = h 会循环调用这个 Setter，发生堆栈溢出
    this.log.push(`set height: ${h}`);
  }
  // Setter 修改 width 属性时添加日志
  set width(w) {
    this._width = w;
    this.log.push(`set width: ${w}`);
  }
  // Method 计算当前的面积
  calcArea() {
    return this._height * this._width;
  }
}

const rec = new Rectangle(5, 10);
console.log(rec.log); // [&quot;set height: 5&quot;, &quot;set width: 10&quot;]
console.log(rec.area); // 50

rec.height = 10;
rec.width = 20;
console.log(rec.log); // [&quot;set height: 5&quot;, &quot;set width: 10&quot;, &quot;set height: 10&quot;, &quot;set width: 20&quot;]
console.log(rec.area); // 200
```

上面的类中定义计算当前面积的方法 `calcArea()` 时，使用了 ES6 引入的[更简短的定义语法](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Functions/Method_definitions)，这种语法与 Setter 和 Getter 的语法相似，它直接将方法名赋值给了函数。

此外，由于 Setter 的特性，当我们在构造函数执行赋值操作，以及之后修改实例的属性时，将调用 Setter 的方法（即 Hook 函数）。因此在上面代码中的 `rec` 实例中，并不存在 `height` 和 `width` 属性，取而代之的是 `_height` 和 `_width` 属性。

## 静态方法和属性

在类的方法前面添加关键字 `static` 可以定义**静态方法**或**静态属性**，它们可以通过类直接调用，但不能通过类的实例调用。静态方法和静态属性常用于为一个使用类的应用程序创建工具函数。

```js
class Point {
  constructor(x, y) {
    this.x = x;
    this.y = y;
  }

  // 定义 Point 类的静态属性
  static className = &quot;Point&quot;;

  // 定义 Point 类的静态方法
  static distance(a, b) {
    const dx = a.x - b.x;
    const dy = a.y - b.y;
    return Math.hypot(dx, dy); // Math.hypot() 返回所有参数的平方和的平方根，在此处用于求两点之间的距离
  }
}

const p1 = new Point(5, 5);
const p2 = new Point(10, 10);

console.log(p1.className); // undefined
console.log(p1.distance); // undefined

console.log(Point.className); // Point
console.log(Point.distance(p1, p2)); // 7.0710678118654755
```

上面的代码中，当我们使用实例访问静态方法和属性时，会显示 `undefined`。而当我们使用类来访问时，则能正常调用了。

## 原型方法和静态方法中的 `this`

当调用静态或原型方法时没有指定 `this` 所属的上下文，那么将返回 `undefined`。这是因为 `class` 内部的代码**总是在严格模式下执行**。

```js
class MyClass {
  getThis() {
    return this;
  }
  static getStaticThis() {
    return this;
  }
}

const getClassStaticThis = MyClass.getStaticThis;
console.log(MyClass.getStaticThis()); // MyClass 类
console.log(getClassStaticThis()); // undefined

const obj = new MyClass();
const getObjThis = obj.getThis;
console.log(obj.getThis()); // obj 实例对象
console.log(getObjThis()); // undefined
```

作为对比，将上面的代码使用传统的基于函数的语法实现。在**非严格模式**下，若没有指定 `this` 所属的上下文，那么将指向全局对象。

```js
function MyClass() {}
MyClass.prototype.getThis = function () {
  return this;
};
// 模拟 Class 的 static 方法
MyClass.getStaticThis = function () {
  return this;
};

const getClassStaticThis = MyClass.getStaticThis;
console.log(MyClass.getStaticThis()); // MyClass 函数
console.log(getClassStaticThis()); // global object

const obj = new MyClass();
const getObjThis = obj.getThis;
console.log(obj.getThis()); // obj 实例对象
console.log(getObjThis()); // global object
```

`this` 一直是 JavaScript 语言最令人困惑的特性之一，您可以阅读与之相关的文章进一步理解。

## 生成器方法

生成器是 ES6 新增的高级特性，允许定义一个非连续执行的函数作为迭代算法，是替代迭代器（Iterator）的选择。

生成器函数使用 `function*` 语法定义，例如 `function* anyGenerator() {}`。在类中对应更简短的语法，将符号 `*` 放在方法名前面即可，例如 `*anyGenerator() {}`。

```js
class Polygon {
  constructor(...sides) {
    this.sides = sides;
  }

  // 定义生成器方法
  *getSides() {
    for (const side of this.sides) {
      yield side;
    }
  }
}

const pentagon = new Polygon(1, 2, 3, 4, 5);
console.log([...pentagon.getSides()]); // [1,2,3,4,5]
```

关于生成器的更多介绍可参考[此页面](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Iterators_and_Generators)。

## 箭头函数定义方法

类中还有一种常见的定义方法的方式：使用**箭头函数**。

```js
class Rectangle {
  // 使用箭头函数定义原型方法
  calcArea = () =&gt; {
    return this.height * this.width;
  };
}
```

子类继承父类的箭头函数定义的方法时，会出现**属性遮蔽（Property Shadowing）**的现象。编写代码如下：

```js
class Father {
  sayHello = () =&gt; {
    console.log(&quot;I am your father.&quot;);
  };
}

class Child extends Father {
  sayHello() {
    console.log(&quot;I am a child.&quot;);
    super.sayHello();
  }
}

const child = new Child();
child.sayHello(); // I am your father.
```

上面的代码并没有像我们预想的那样，依次打印出 `I am a child.` 和 `I am your father.`，而是只打印出了 `I am your father.`。

简单解释原因的话就是，箭头函数定义的方法将挂载到**实例的属性**上，而普通函数定义的方法挂载到**原型链**上。这样，当我们实例化 `child` 对象时，会将原型 `Child` 从自己的父类 `Father` 继承的 `sayHello()` 方法则挂载到自身的属性上。

回忆一下过去学过的知识，当我们尝试调用实例的方法时，JavaScript 会首先在实例的属性上查找是否存在此方法，如果存在则直接调用，如果不存在再在原型链上查找。因此，当我们调用实例 `child` 的 `sayHello()` 方法时，JavaScript 找到了属性上的 `sayHello()` 方法，遂结束了查找并调用。

详细内容可以参考[这篇博客](https://github.com/dwqs/blog/issues/67#issue-327371697)。

在类中，直接使用 `=` 的声明从本质上而言就是 [Field Declarations](https://github.com/tc39/proposal-class-fields#field-declarations) 的语法，相当于**直接声明了一个实例的属性**。在接下来的[字段声明](#字段声明)小节中，也使用到了这个语法。

## 字段声明

### 公有字段声明

在类中可以声明公有字段，使得类定义具有自我记录性，且这些字段将始终存在。字段的声明可以设置初始值。

```js
class Point {
  x; // 公有字段 x
  y = 0; // 公有字段 y，初始值为 0
  constructor(x, y) {
    this.x = x;
    this.y = y;
  }
  get position() {
    return [this.x, this.y];
  }
}

console.log(new Point(5, 10).position); // [5, 10]
```

### 私有字段声明

在声明的字段前面加上 `#` 表明为私有字段。私有字段同样可以设置初始值。

```js
class Point {
  #x; // 私有字段 x
  #y = 0; // 私有字段 y，初始值为 0
  constructor(x, y) {
    this.#x = x;
    this.#y = y;
  }
  get position() {
    return [this.#x, this.#y];
  }
}

console.log(new Point(10, 5).position); // [10, 5]
```

与公有字段不同的是：

- 不能从类外部引用私有字段。或私有字段在类外部不可见。
- 私有字段仅能在字段声明中预先定义。
- 在实例创建之后，不能再通过赋值来创建私有字段。

```js
class Point {
  name = &quot;point&quot;;
  #x;
  #y = 0;
  // #z // 假如不在这里显式声明 #z
  constructor(x, y, z) {
    this.#x = x;
    this.#y = y;
    // this.#z = z // Uncaught SyntaxError: Private field &apos;#z&apos; must be declared in an enclosing class
  }
  get position() {
    return [this.#x, this.#y];
  }
  get position3D() {
    // return [this.#x, this.#y, this.#z] // Uncaught SyntaxError: Private field &apos;#z&apos; must be declared in an enclosing class
  }
}

const p = new Point(10, 5, 15);
p.name = &quot;point3D&quot;; // 实例可以通过赋值修改公有字段
console.log(p.name); // point3D
p.#x = 20; // 实例不可通过赋值修改私有字段，Uncaught SyntaxError: Private field &apos;#x&apos; must be declared in an enclosing class
console.log(p.#x); // 实例不可在外部访问私有字段，Uncaught SyntaxError: Private field &apos;#x&apos; must be declared in an enclosing class
```

在上面的代码中，我们尝试在类中不显式声明私有字段 `#z` 的情况下，访问 `#z`，结果会抛出 `SyntaxError`。此外，我们尝试在实例中直接对私有字段 `#x` 进行赋值和获取操作，也会抛出 `SyntaxError`。

## 使用 `extends` 拓展子类

`extends` 可以用来创建子类，父类可以是自己定义的普通类，也可以是内建对象。对于后者，以继承内建的 `Date` 对象为例：

```js
class MyDate extends Date {
  constructor() {
    // 必须调用父类的构造函数，否则会报错
    super();
  }

  // 定义子类的方法，该方法可以获取格式化后的日期
  getFormattedDate() {
    const months = [
      &quot;Jan&quot;,
      &quot;Feb&quot;,
      &quot;Mar&quot;,
      &quot;Apr&quot;,
      &quot;May&quot;,
      &quot;Jun&quot;,
      &quot;Jul&quot;,
      &quot;Aug&quot;,
      &quot;Sep&quot;,
      &quot;Oct&quot;,
      &quot;Nov&quot;,
      &quot;Dec&quot;,
    ];
    return (
      this.getDate() +
      &quot; - &quot; +
      months[this.getMonth()] +
      &quot; - &quot; +
      this.getFullYear()
    );
  }
}

console.log(new MyDate().getFormattedDate()); // 20 - May - 2021
```

类不过是一种语法糖，因此我们也可以用 `extends` 来继承传统的基于函数的“类”：

```js
// 定义 Animal “类”
function Animal(name) {
  this.name = name;
}
Animal.prototype.speak = function () {
  console.log(this.name + &quot; makes a noise.&quot;);
};

// 使用 extends 拓展 Animal “类”
class Dog extends Animal {
  speak() {
    super.speak();
    console.log(this.name + &quot; barks.&quot;);
  }
}

const d = new Dog(&quot;Mitzie&quot;);
d.speak();
// Mitzie makes a noise.
// Mitzie barks.
```

对于**不可构造**的常规对象，要实现继承的话，可以使用 `Object.setPrototypeOf()` 方法，它可以设置一个指定对象的原型到另一个对象：

```js
// 定义 Animal 对象
const Animal = {
  speak() {
    console.log(this.name + &quot; makes a noise.&quot;);
  },
};

class Dog {
  constructor(name) {
    this.name = name;
  }
  speak() {
    super.speak();
    console.log(this.name + &quot; barks.&quot;);
  }
}

Object.setPrototypeOf(Dog.prototype, Animal); // 如果不这样做，在调用 speak 时会返回 TypeError

const d = new Dog(&quot;Mitzie&quot;);
d.speak();
// Mitzie makes a noise.
// Mitzie barks.
```

出于性能考量，应避免使用 `Object.setPrototypeOf()` 方法来实现继承，在[这里](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/setPrototypeOf)了解它的更多。

## 使用 `super` 调用超类

使用 `super` 关键字可以调用对象的父对象上的函数。

```js
class Cat {
  constructor(name) {
    this.name = name;
  }
  speak() {
    console.log(`${this.name}: meo~~!`);
  }
}

class Lion extends Cat {
  speak() {
    super.speak(); // 调用 Cat 对象的 speak 方法
    console.log(`${this.name}: roars!!!`);
  }
}

const l = new Lion(&quot;Li&quot;);
l.speak();
// Li: meo~~!
// Li: roars!!!
```

假如我们将上面代码中 `Lion` 类里的 `speak()` 方法删去，那么打印的结果是 `Li: meo~~!`。如果认真看到这里的话，原因想必也已经了然于胸：子类继承了父类的属性和方法。那么当子类定义了与父类相同名字的方法时，根据原型链上的调用规则，会调用子类定义的方法。这就是为什么我们需要 `super` 关键字的原因之一，方法名相同的情况下，在子类方法中我们仍可以调用父类的方法。

在**构造函数**中，`super()` 需要在使用 `this` 前调用：

```js
class Rectangle {
  constructor(height, width) {
    this._name = &quot;Rectangle&quot;;
    this._height = height;
    this._width = width;
  }
  get name() {
    return `Hi, I am a ${this._name}.`;
  }
  get area() {
    return this._height * this._width;
  }
}

class Square extends Rectangle {
  constructor(length) {
    // this._height = length // Must call super constructor in derived class before accessing &apos;this&apos; or returning from derived constructor
    super(length, length); // 调用 Rectangle 的构造函数，length 分别作 height 和 width
    this._name = &quot;Square&quot;; // 修改 name 属性为 Square
  }
}

const s = new Square(15);
console.log(s.name); // Hi, I am a Square.
console.log(s.area); // 225
```

`super` 也可以用来调用父类的静态方法：

```js
class Rectangle {
  constructor(height, width) {
    this._height = height;
    this._width = width;
  }
  static help() {
    // 父类的静态方法
    return &quot;I have 4 sides.&quot;;
  }
}

class Square extends Rectangle {
  constructor(length) {
    super(length, length);
  }
  static help() {
    // 子类的静态方法，使用 super 调用父类的 help 方法
    return super.help() + &quot; They are all equal.&quot;;
  }
}

console.log(Square.help()); // I have 4 sides. They are all equal.

// 假如只去除子类 help 方法前面的 static 关键字
// console.log(new Square(10).help()) // Uncaught TypeError: (intermediate value).help is not a function

// 假如只去除父类 help 方法前面的 static 关键字
// console.log(Square.help()) // Uncaught TypeError: (intermediate value).help is not a function
```

在上面的代码中，`Square` 中的静态方法 `help()` 调用了父类的静态方法。静态方法中的 `super` 只能调用父类的静态方法，假如我们去除子类或父类方法前面的 `static` 关键字，会发生报错。

在本章节的例子中，似乎子类方法中的 `super` 都调用了父类中与之同名的方法，但实际上并没有这个限制，在编写的时候可以根据实际的需求自行调整命名或调用其它父类方法。

在[箭头函数的使用](#箭头函数定义方法)章节的例子中，既然箭头函数定义的方法挂载到了实例的属性上，那么还能用 `super` 来调用吗？答案是否定的。JavaScript 没能在父对象的原型链上找到这个方法，于是什么也没有发生。

更多补充可以查阅 MDN 上[关于 `super` 的介绍](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/super)。

## 使用 `Symbol.species` 覆盖构造函数

`Symbol.species` 访问器属性允许子类覆盖对象的默认构造函数。

读着很拗口，那就看两个实际的例子。当使用 `map()` 这样的方法会返回默认的构造函数，我们可能想在对拓展的数组类 `MyArray` 执行操作时返回 `Array` 对象，那么可以这样编写代码：

```js
class MyArray extends Array {
  // 设置 getter，当获取 MyArray 类的构造函数时，返回 Array 类的构造函数
  static get [Symbol.species]() {
    return Array;
  }
}

const a = new MyArray(1, 2, 3);
const mapped = a.map((x) =&gt; x * x);
console.log(mapped instanceof MyArray); // false
console.log(mapped instanceof Array); // true
```

又例如，我们拓展 `Promise` 类为 `TimeoutPromise` 类，但我们不希望某一个超时的 Promise 请求影响整个 Promise 链，就可以使用 `Symbol.species` 来告诉 `TimeoutPromise` 类返回一个 `Promise` 对象，方便我们执行异常处理操作：

```js
class TimeoutPromise extends Promise {
  static get [Symbol.species]() {
    return Promise;
  }
}
```

`Symbol.species` 允许自定义返回的类，不一定是子类继承实现的类。

`Symbol.species` 帮助我们在处理子类实例时，能够有一套标准的操作流程，方便了开发，在某些场景十分实用。

## 使用 Mix-ins 实现多重继承

在 ECMAScript 中，一个类只能有一个单超类，因此想通过工具类的方法实现多重继承行为是不可能的。为了实现多重继承，我们可以使用 Mixin 的方法。

什么是 Mixin？简单来说，Mixin 也是一个类，包括了一些方法，这些方法可以被其它类使用。但在其它类中使用这些方法**不需要继承** Mixin。举一个简单的例子：

```js
let sayHiMixin = {
  // Mixin
  // Methods that useful
  sayHi() {
    alert(`Hello, ${this.name}`);
  },
  sayBye() {
    alert(`Bye, ${this.name}`);
  },
};

class User {
  constructor(name) {
    this.name = name;
  }
}

Object.assign(User.prototype, sayHiMixin); // 将 Mixin 中的方法复制到 Class 类中

new User(&quot;Dude&quot;).sayHi(); // Hello, Dude!
```

我们又知道，创建类的[两种声明方式](#定义类)是等价的：

```js
class Mixin1 {
  //
}

// 等价于
const Mixin2 = class {
  //
};
```

其中，第二种方式，或者说使用类表达式声明类的方式，允许我们**动态生成自定义的类**。根据这个特性，我们就可以编写 Mixin 代码来实现多重继承了：

```js
class Animal { // 共同工具类
    //
}
class CatMixin = (superClass) =&gt; class extends superClass { // 猫猫工具类
    //
}
class DogMixin = (superClass) =&gt; class extends superClass { // 狗狗工具类
    //
}

class MyMixin extends CatMixin(DogMixin(Animal)) { // 实现多重继承
    //
}
```

在上面的代码中，我们首先定义了一个通用的工具类 `Animal`，其它 Mixin 类可能会用到这个工具类。接着我们定义了猫猫和狗狗使用的工具类 `CatMixin` 与 `DogMixin` 的创建规则，它们将传入的参数作为自己的父类，并创建一个新的类。最后，我们定义了想要的 `MyMixin` 类，它继承了 `CatMixin(DogMixin(Animal))` 类。从实现的角度来看，**相当于**执行了下面的操作：

```js
class DogMixin extends Animal {
  //
}
class CatMixin extends DogMixin {
  // 这显然是不合理的，猫猫工具类怎么能继承狗狗工具类
  //
}

class MyMixin extends CatMixin {
  //
}
```

实际上，我们并没有让 `CatMixin` 类去继承 `DogMixin` 类，而是使用了 Mixin 的思想，让 `MyMixin` 继承了我们基于类表达式创建的一个新的类，实现了多重继承。

## 参考资料

本博文仅且记录了 JavaScript 中类在语法上的知识和运用，辅以少量的实现原理。关于底层的具体实现，就放到以后再深入探讨学习吧。

### 技术博文

- [JavaScript 或 ES6 如何实现多继承总结【Mixin 混合继承模式】](https://cloud.tencent.com/developer/article/1700017), 2020-09-18
- [ES6 Class Methods 定义方式的差异](https://github.com/dwqs/blog/issues/67), 2018-06-25
- [[学习 es6]setter/getter 探究](https://segmentfault.com/a/1190000007356931), 2016-11-02
- [Metaprogramming in ES6: Symbols and why they&apos;re awesome](https://www.keithcirkel.co.uk/metaprogramming-in-es6-symbols/#symbolspecies), 2015-06-18

### 其它资料

主要参考了 MDN 上关于类和相关内容的[描述](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Classes)。

- [Mixins - JAVASCRIPT.INFO](https://javascript.info/mixins)
</content:original-text><content:updated-at>2024-08-14T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[漫谈 JavaScript 闭包]]></title><description><![CDATA[JavaScript 中有一个叫作闭包（Closure）的概念，非常有趣且适用，值得学习并整理为一篇博客。 为了更好理解闭包的作用，不妨看看我的关于 JS 变量提升（Hoisting）和函数提升现象的阐述。

作用域

在 JavaScript 中，作用域（Scope）是当前代码执行的上下文，也即是值和表达式在其中可访问到的上下文。

如果一个变量或其它表达式不在当前作用域中，就会沿作用域链…]]></description><link>https://blog.towind.fun/posts/js-closure</link><guid isPermaLink="false">js-closure</guid><category><![CDATA[技术琐事]]></category><pubDate>Tue, 18 May 2021 00:00:00 GMT</pubDate><content:original-text>
JavaScript 中有一个叫作闭包（Closure）的概念，非常有趣且适用，值得学习并整理为一篇博客。

为了更好理解闭包的作用，不妨看看我的&lt;Link to=&quot;/posts/js-hoisting&quot;&gt;这一篇博客&lt;/Link&gt;关于 JS 变量提升（Hoisting）和函数提升现象的阐述。

## 作用域

在 JavaScript 中，**作用域**（Scope）是当前代码执行的上下文，也即是值和表达式在其中可访问到的上下文。

- 如果一个变量或其它表达式不在当前作用域中，就会沿**作用域链**（Scope Chain）往父作用域搜索。如果也仍未找到它的话，那么它就是不可用的。
- 最顶级的父作用域是全局对象。
- 父作用域不能引用子作用域中的变量和定义。

目前，作用域有三种：**全局作用域**和**函数作用域**，以及 ES6 新增的**块级作用域**。

### 作用域与执行上下文

作用域与**执行上下文**（Context）是两个不同的概念。JavaScript 系解释型语言，执行分为解释阶段和执行阶段两个阶段，两个阶段所完成的行为大抵如下：

- 解释阶段：
  1. 词法分析；
  2. 语法分析；
  3. **确定作用域规则**。
- 执行阶段：
  1. **创建执行上下文**；
  2. 执行函数代码；
  3. 垃圾回收。

可以看见，在解释阶段就已经确定了作用域规则，而在执行阶段才创建了执行上下文。因而作用域在定义时就确定，不会发生改变；执行上下文在运行时确定，可以发生改变。

### 全局作用域和函数作用域

最外层函数和在最外层函数外边定义的变量拥有全局作用域，而函数内部定义的其他函数和变量拥有函数作用域。如：

```js
var outVar = &quot;outVar&quot;;
function outFunc() {
  var inVar = &quot;inVar&quot;;
  function inFunc() {
    console.log(outVar, inVar);
  }
  inFunc();
}
console.log(outVar); // outVar
console.log(inVar); // Uncaught ReferenceError: inVar is not defined
outFunc(); // outVar inVar
inFunc(); // Uncaught ReferenceError: inFunc is not defined
```

在最外层，我们可以正常打印 `outVar` 和调用 `outFunc()` 方法，但是在尝试直接调用 `outFunc()` 方法中所定义的 `inVar` 和 `inFunc()` 方法时，发生报错。此外，在 `inFunc()` 方法中，成功在父作用域找到并打印出了 `outVar` 的值。

所有未定义而直接赋值的变量会自动声明为全局变量，拥有全局作用域。如：

```js
function outFunc() {
  globalInVar = &quot;globalInVar&quot;;
  var invar = &quot;inVar&quot;;
}
// 执行这个函数以赋值
outFunc();
console.log(globalInVar); // globalInVar
console.log(invar); // Uncaught ReferenceError: invar is not defined
```

我们在 `outFunc()` 方法中未使用 `var` 声明而直接给 `globalInVar` 变量进行赋值，它将声明为全局变量，并能在最外层直接打印出来。应当避免此类声明的存在，在 `ESLint` 等代码质量检查工具中，会标注此类错误。

接下来看一段非常经典的代码案例：

```js
function getArr() {
  var arr = [];
  for (var i = 0; i &lt; 5; i++) {
    arr.push(function () {
      return i;
    });
  }
  return arr;
}
var testArr = getArr();
console.log(testArr[2]()); // 5
```

我们将方法传入到数组中，期望调用方法返回的值为当前数组的索引值。在调用 `testArr[2]()` 时，期望得到的返回值为 `2`，但实际返回的值是 `5`，为什么？

这是由于在 `for` 循环中我们使用 `var` 声明的变量 `i` 会发生变量提升，其作用域为 `getArr()` 这个函数作用域。在调用数组中存储的函数时，我们已经完成了循环，此时 `i` 的值变成了 `5`，则无论调用数组的哪个函数都会打印出现在的值 `5`。上面的代码使用简化的方式编写，相当于：

```js
var arr = [];
var i; // 变量提升，我们在 for 循环中声明的变量在全局可访问
for (i = 0; i &lt; 5; i++) {
  arr.push(function () {
    return i;
  });
}
console.log(arr[2]()); // 5
// console.log(i) // 5
```

那么，现在的问题是，要如何在函数内部保存（或记住）一个从外部传入的值，在调用的时候能正确打印出我们想要的结果呢？

ES6 中提出了块级作用域，可以顺利解决这个问题。

### 块级作用域

与声明的变量只能是全局或整个函数块的 `var` 命令不同，`let` 和 `const` 命令声明的变量、语句和表达式作用域可以限制在块级以内。例如：

```js
{
  var varVar = &quot;varVar&quot;;
  let letVar = &quot;letVar&quot;;
}
console.log(varVar); // varVar
console.log(letVar); // Uncaught ReferenceError: letVar is not defined
```

在 ES6 以前，不存在块级作用域，使用 `var` 命令声明的在 `for`, `while` 等内部的变量都会提升为外部作用域的变量。

现在，我们就可以使用块级作用域替换刚刚的函数作用域了：

```js
function getArr() {
  const arr = [];
  for (let i = 0; i &lt; 5; i++) {
    // 使用 let 替换 var
    arr.push(function () {
      return i;
    });
  }
  return arr;
}
const testArr = getArr();
console.log(testArr[2]()); // 2
```

使用 `let` 命令声明的变量 `i` 在循环中拥有块级作用域，每次循环时每个返回的函数中引用的都是其对应块级作用域的变量。上面的代码使用简化的方式编写，相当于：

```js
const arr = [];
for (let i = 0; i &lt; 5; i++) {
  const n = i; // 声明的变量仅在 for 循环的块作用域可访问
  arr.push(function () {
    return n;
  });
}
console.log(arr[2]()); // 2
// console.log(i) // Uncaught ReferenceError: i is not defined
```

而在 ES6 之前，就需要用到了这篇博文真正的主角——闭包。

## 什么是闭包

由于 JavaScript 的链式作用域（Chain Scope）结构，父对象的所有变量都对子变量可见，反之则不成立。出于某种原因，我们有时候需要得到函数内的局部变量，就需要使用变通的方法实现：

```js
// 子对象的变量对父对象不可见
function outerFunc() {
  var value = 100;
  function innerFunc() {
    console.log(value);
  }
}
innerFunc(); // Uncaught ReferenceError: innerFunc is not defined

// 变通的方法
function outerFunc() {
  var value = 100;
  function innerFunc() {
    console.log(value);
  }
  return innerFunc; // 将内部定义的方法返回
}
var visitValue = outerFunc();
visitValue(); // 100
```

在一些编程语言中，一个函数的局部变量仅存在于此函数的执行期间。那么一旦 `outerFunc()` 执行完毕，您可能会认为函数内部定义的变量 `value` 将不能够再访问。然而，在 JavaScript 中这段代码能够顺利执行并打印出结果。

这是由于 JavaScript 中的函数会形成**闭包**。

&gt; 一个函数和对其周围状态（lexical environment，词法环境）的引用捆绑在一起（或者说函数被引用包围），这样的组合就是闭包（closure）。也就是说，闭包让你可以在一个内层函数中访问到其外层函数的作用域。在 JavaScript 中，每当创建一个函数，闭包就会在函数创建的同时被创建出来。
&gt; A closure is the combination of a function bundled together (enclosed) with references to its surrounding state (the lexical environment). In other words, a closure gives you access to an outer function’s scope from an inner function. In JavaScript, closures are created every time a function is created, at function creation time.

闭包是由函数以及声明该函数的词法环境组合而成的。该环境包含了这个闭包创建时作用域内的所有局部变量。从本质上来说，闭包可以看作将一个函数的内部和外部连接起来的桥梁。

在上面的代码中，变量 `visitValue` 是执行 `outerFunc()` 时创建的对 `innerFunc` 函数实例的引用，而 `innerFunc` 实例维持了一个对它的词法环境的引用，在这个词法环境中存在着变量 `value`。因此，当我们执行 `visitValue()` 时，变量 `value` 是可用的，最后我们成功在控制台打印出了它的值。

那么，为了解决在前文提出的不存在块级作用域的问题，我们可以像这样编写代码：

```js
function getArr() {
  var arr = [];
  for (var i = 0; i &lt; 5; i++) {
    arr.push(
      (function (n) {
        // n 的作用域为函数作用域
        return function () {
          // 返回一个函数
          return n; // 调用函数返回的值为传入的 n 的值
        };
      })(i),
    ); // 传入当前的 i 值
  }
  return arr;
}
var testArr = getArr();
console.log(testArr[2]()); // 2
```

对于上面的 `for` 循环，相当于执行了下述代码：

```js
arr[0] = (function (n) {
  return function () {
    return n;
  };
})(0);
arr[1] = (function (n) {
  return function () {
    return n;
  };
})(1);
arr[2] = (function (n) {
  return function () {
    return n;
  };
})(2);
// 下略

console.log(arr[2]()); // 2
```

这样一来，数组中的每个函数分别处于一个立即执行函数的**函数作用域**中，这个立即执行的函数传入了每次循环时变量 `i` 的值。于是，当我们调用数组中的函数时，将返回**传入时**的 `i` 值，而不是循环结束后的 `i` 值。

&gt; &quot;JavaScript 中闭包无处不在，你只需要能够识别并拥抱它。&quot;
&gt; &quot;最后你恍然大悟：原来在我的代码中已经到处都是闭包了，现在我终于能理解他们了。
&gt; &quot;理解闭包就好像 Neo 第一次见到矩阵一样。&quot;

_You Don&apos;t Know Javascript_ 中如是写道。

## 如何使用闭包

如果不是某些特定任务需要使用到闭包，那么在函数中创建另一个函数是不明智的。闭包会使得函数中的变量保存在**内存**中，可能造成性能问题。

### 函数防抖和节流

函数防抖和函数节流就是典型的闭包用例，我在&lt;Link to=&quot;/posts/js-debounce-throttle&quot;&gt;这一篇博客&lt;/Link&gt;里对它们进行了编写。

### 函数工厂

这是一个函数工厂的示例：

```js
function makeAdder(x) {
  return function (y) {
    return x + y;
  };
}

var add5 = makeAdder(5);
var add10 = makeAdder(10);

console.log(add5(2)); // 7
console.log(add10(2)); // 12
```

我们定义了一个函数 `makeAdder(x)`，它接受一个参数 `x`，并返回一个新的函数。返回的这个函数接受参数 `y`，并返回 `x + y` 的值。接着，我们创建了两个新函数 `add5` 和 `add10`，一个将它的参数与 `5` 求和，另一个与 `10` 求和。

`add5` 和 `add10` 都是闭包，它们共享相同的函数定义，但是保存了不同的词法环境。在 `add5` 的词法环境中，`x` 的值为 `5`；而在 `add10` 中，`x` 为 `10`。

### 面向对象编程

我们可以用闭包来模拟**私有**属性和方法，就像面向对象编程语言中类的私有属性和方法的编写一样。以构建 `Rectangle` 矩形类为例：

```js
var Rectangle = function (height, width) {
  var height = height; // 私有的高属性
  var width = width; // 私有的宽属性
  function calcArea() {
    // 私有的计算面积方法
    return height * width;
  }
  function setHeight(h) {
    // 私有的设置高方法
    height = h;
  }
  function setWidth(w) {
    // 私有的设置宽方法
    width = w;
  }
  return {
    // 返回一个对象，对象可以访问到闭包的作用域
    get area() {
      return calcArea();
    },
    setHeight: function (h) {
      setHeight(h);
    },
    setWidth: function (w) {
      setWidth(w);
    },
  };
};

var square = Rectangle(5, 5);
console.log(square.area); // 25

square.setHeight(10);
square.setWidth(10);
console.log(square.area); // 100

console.log(square.height); // undefined
```

在上面的代码中，我们使用了闭包来定义公共函数，并令这些公共函数访问到私有函数和变量。这个方式又称模块模式（Module Pattern）。

在 ES6 中，可以用 `class` 语法糖来声明类。上面的代码相当于：

```js
class Rectangle {
  #height;
  #width;
  // Constructor
  constructor(height, width) {
    this.#height = height;
    this.#width = width;
  }
  // Getter
  get area() {
    return this.calcArea();
  }
  // Method
  calcArea() {
    return this.#height * this.#width;
  }
  setHeight(h) {
    this.#height = h;
  }
  setWidth(w) {
    this.#width = w;
  }
}

const square = new Rectangle(5, 5); // 使用 new 关键字来创建对象
console.log(square.area); // 25

square.setHeight(10);
square.setWidth(10);
console.log(square.area); // 100

console.log(square.height); // undefined
```

在 `class` 内，私有属性 `height` 和 `width` 需要在前面加上 `#` 并在开头显示声明出来。

当然，相比闭包的方式，使用 `class` 的声明更加直观，值得推广使用。

值得补充的是，假如不需要在对象中使用私有声明，而是使用公用声明，应当避免使用闭包。同样以构建 `PublicRectangle` 矩形类为例：

```js
var PublicRectangle = function (height, width) {
  return {
    // 将矩形的高和宽作为返回对象的可访问属性
    height: height,
    width: width,
    get area() {
      return this.height * this.width;
    },
    setHeight: function (h) {
      this.height = h;
    },
    setWidth: function (w) {
      this.width = w;
    },
  };
};

var square = PublicRectangle(5, 5);
console.log(square.area); // 25

square.setHeight(10);
square.setWidth(10);
console.log(square.area); // 100

console.log(square.height); // 10
```

上面的代码中我们并没有利用到闭包的好处，反而在每次调用构造器时都重新赋值一遍方法。因此在这里不妨变为添加**原型方法**的方式：

```js
var PublicRectangle = function (height, width) {
  this.height = height;
  this.width = width;
};
Object.defineProperty(PublicRectangle.prototype, &quot;area&quot;, {
  // 为 PublicRectangle 原型添加 area 的 getter
  get() {
    return this.height * this.width;
  },
});
PublicRectangle.prototype.setHeight = function (h) {
  this.height = h;
};
PublicRectangle.prototype.setWidth = function (w) {
  this.width = w;
};

var square = new PublicRectangle(5, 5); // 应使用 new 关键字
console.log(square.area); // 25

square.setHeight(10);
square.setWidth(10);
console.log(square.area); // 100

console.log(square.height); // 10
```

## 参考资料

### 技术博客（或问答）

- [闭包以及其 ES6 下的使用](https://www.jianshu.com/p/ebb4eccb6625), 2020-01-13
- [深入理解 JavaScript 作用域和作用域链](https://blog.fundebug.com/2019/03/15/understand-javascript-scope/), 2019-03-15
- [深入解析 ES6 中 let 和闭包](https://juejin.cn/post/6844903747106111501), 2018-12-25
- [如何给 js 内建对象构造器添加 getter 和 setter](https://segmentfault.com/q/1010000016598692), 2018-10-06
- [学习 Javascript 闭包（Closure）](http://www.ruanyifeng.com/blog/2009/08/learning_javascript_closures.html), 2009-08-30

### 其它资料

- [闭包 - MDN](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Closures)
- [类 - MDN](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Classes)
- [You Don&apos;t Know Javascript](https://github.com/getify/You-Dont-Know-JS)
</content:original-text><content:updated-at>2021-05-18T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[漫谈 JavaScript 变量提升和函数提升]]></title><description><![CDATA[在 ES6 规范出现之前，使用 JavaScript 声明变量只有 ,  以及隐式声明三种方式。 按照一般编程的思维，我们会通过“先声明，后调用”的方式去使用变量，例如：

但假如反过来，我们“先调用，后声明”，会发生什么呢？

如上所示，在声明变量  之前尝试将它的值打印出来，控制台输出的结果是 ，而不是预期中的报错 。这就是变量提升。

而对于函数的声明与使用，也出现了相似的情况：

在声明函数…]]></description><link>https://blog.towind.fun/posts/js-hoisting</link><guid isPermaLink="false">js-hoisting</guid><category><![CDATA[技术琐事]]></category><pubDate>Tue, 11 May 2021 00:00:00 GMT</pubDate><content:original-text>
在 ES6 规范出现之前，使用 JavaScript 声明变量只有 `var`, `function` 以及隐式声明三种方式。

按照一般编程的思维，我们会通过“先声明，后调用”的方式去使用变量，例如：

```js
var a = 3;
console.log(a); // 3
```

但假如反过来，我们“先调用，后声明”，会发生什么呢？

```js
console.log(a); // undefined
var a = 3;
console.log(a); // 3

console.log(b); // Uncaught ReferenceError: b is not defined
```

如上所示，在声明变量 `a` 之前尝试将它的值打印出来，控制台输出的结果是 `undefined`，而不是预期中的报错 `Uncaught ReferenceError: a is not defined`。这就是**变量提升**。

而对于函数的声明与使用，也出现了相似的情况：

```js
sayHello(); // Hello there!

function sayHello() {
  console.log(&quot;Hello there!&quot;);
}
```

在声明函数之前，我们已经可以调用函数方法并正确输出。这便是**函数提升**。

在 JavaScript 中奇怪的一点是，我们可以在声明变量（使用 `var`）和声明函数之前使用它们，就好像变量和函数的声明被提升到了代码的顶部一样：

```js
console.log(a); // undefined
var a = 3;
console.log(a); // 3

// 好像等于下面的代码
var a;
console.log(a); // undefined
a = 3;
console.log(a); // 3
```

实际上，JavaScript 并不会移动代码，变量提升和函数提升并不是真正意义上的“提升”，而是解释执行 JavaScript 代码过程所带来的“特性”。

## 发生了什么

以现在最主流的 `V8` 引擎为例，其解释执行 JavaScript 代码的过程大致分为生成抽象语法树（AST），生成字节码和生成机器码三个阶段。

在生成抽象语法树阶段，又分为了词法分析和语法分析两个阶段。其中，在词法分析阶段，JavaScript 会检测到当前**作用域**使用到的所有变量和函数声明，并将这些变量和函数声明添加到一个名为**词法环境**（Lexical Environment）的内存空间当中。

在词法分析阶段，对于变量声明和函数声明，词法环境的处理是不一样的：

- 对于变量声明如 `var a = 3`，会为变量分配内存并初始化为 `undefined`，赋值语句在生成机器码阶段真正执行代码的时候才进行。
- 对于函数声明如 `function sayHello() { console.log(&apos;Hello there!&apos;) }`，会在内存里创建函数对象，并且直接初始化为该函数对象。

因此，对于变量声明，在真正执行到赋值语句之前，我们就已经可以使用此变量，但是初值为 `undefined`；而对于函数声明，在执行到函数声明之前，函数对象就已经存在在内存当中，并可以直接调用了。

应当注意的是，函数声明的处理优先级要高于变量声明（意味着函数会“提升”到更靠前的位置）：

```js
console.log(foo);
foo = 3;
console.log(foo);
function foo() {}

// 相当于下面的代码
var foo;
foo = function () {};
console.log(foo);
foo = 3;
console.log(foo);

// 依次打印出：
// function foo() {}
// 3
```

另外，变量提升和函数提升都是将声明“提升”到当前**作用域**的顶端：

```js
var foo = 5;

function hoist() {
  console.log(foo);
  foo = 3;
  console.log(foo);
  function foo() {}
}

hoist();
console.log(foo);

// 相当于下面的代码
var hoist;
var foo;

hoist = function () {
  var foo;
  foo = function () {};
  console.log(foo);
  foo = 3;
  console.log(foo);
};
foo = 5;

hoist();
console.log(foo);

// 依次打印出：
// function foo() {}
// 3
// 5
```

`hoist()` 方法中执行的 `console.log(foo)` 优先从当前作用域中寻找变量 `foo`，如果找不到才在父级作用域寻找。

## 匿名函数声明

如果将函数赋值给一个变量会怎样：

```js
sayHi(); // Uncaught TypeError: sayHi is not a function
console.log(sayHi); // undefined
var sayHi = function () {
  console.log(&quot;Hi there!&quot;);
};
sayHi(); // Hi there!
```

使用匿名函数声明时，`sayHi` 发生变量提升，但赋值为 `undefined`，因此执行 `sayHi()` 时会报错 `Uncaught TypeError: sayHi is not a function`。随后执行完赋值语句后，才成为一个可以执行的函数变量。

## 防止变量提升

在 ES6 中，提供了两个声明变量的新的命令，即现在我们最常用的 `let` 和 `const`。使用 `let` 声明的变量可以修改，而使用 `const` 声明的变量不可更改。使用 `const` 声明必须指定初始值。

使用 `var`, `let` 和 `const` 声明的变量都会被“提升”，不同的是：

- `var` 命令在变量的定义被执行之前就初始化变量，并拥有一个默认的 `undefined` 值。
- `let` 与 `const` 命令会形成**暂时性死区**，在变量的定义被执行之前都**不会初始化变量**，避免在声明语句之前的不正确调用。如果定义时没有给定值的话，`let` 声明的变量会赋值为 `undefined`，而 `const` 声明的变量会报错。

关于暂时性死区的概念，援引阮一峰老师在 [ES6 入门书](https://es6.ruanyifeng.com/#docs/let)中的话：

&gt; ES6 明确规定，如果区块中存在 `let` 和 `const` 命令，这个区块对这些命令声明的变量，从一开始就形成了封闭作用域。凡是在声明之前就使用这些变量，就会报错。
&gt; 总之，在代码块内，使用 `let` 命令声明变量之前，该变量都是不可用的。这在语法上，称为“暂时性死区”（temporal dead zone）。

下面是一个使用 `const` 声明函数的例子：

```js
test(); // Uncaught ReferenceError: Cannot access &apos;test&apos; before initialization
console.log(test); // Uncaught ReferenceError: Cannot access &apos;test&apos; before initialization
const test = function () {
  console.log(&quot;test&quot;);
};
test(); // test
```

在这里，我们使用了 `const` 命令声明函数，只要一进入当前作用域，所要使用的 `test` 变量就已经存在了，但是不可获取，如果获取则会抛出特别的错误 `Uncaught ReferenceError: Cannot access &apos;test&apos; before initialization`（一般情况下，获取未声明的变量抛出的错误为 `Uncaught ReferenceError: test is not defined`）。只有等到声明变量的那一行代码出现，才可以获取和使用该变量。使用 `let` 命令也有同样的效果。

_[Google JavaScript Style Guide](https://google.github.io/styleguide/jsguide.html#features-use-const-and-let)_ 建议使用 ES6 规范的 `const` 和 `let` 命令声明变量，舍弃容易造成错误的 `var` 命令。

## 为什么有变量提升和函数提升

布兰登·艾克（Brendan Eich）是 JavaScript 的主要创造者与架构师，关于“为什么要变量提升”这个问题，他给出了这样的回答：

&gt; Q: So what is one of the main (if not the main) reasons of variables &quot;hoisting&quot;? / 那么，变量“提升”的主要（或者并非主要）的原因之一是什么呢？
&gt; Brendan Eich: An &quot;abstraction leak&quot; from the first JavaScript VM. Compiler indexes vars to stack slots, binds names to slots on entry. / 这源于第一批 JavaScript 虚拟机的“抽象泄漏”问题。编译器在入口处将变量索引到了堆栈插槽，并把变量名绑定给了插槽。

**抽象泄漏**是一个计算机术语，指“本应隐藏实现细节的抽象化不可避免地暴露出底层细节与局限性”。这是一个非常有意思的概念，以 TCP 协议举例：

&gt; 当我说 TCP 协议可以保证消息一定能够到达，事实上并非如此。如果你的宠物蛇把网线给咬坏了，那即便是 TCP 协议也无法传输数据；如果你和网络管理员闹了矛盾，他将你的网口接到了一台负载很高的交换机上，那即便你的数据包可以传输，速度也会奇慢无比。
&gt; 这就是我所说的“抽象泄漏”。TCP 协议试图提供一个完整的抽象，将底层不可靠的数据传输包装起来，但是，底层的传输有时也会发生问题，即便是 TCP 协议也无法解决，这时你会发现，它也不是万能的。TCP 协议就是“抽象泄漏定律”的示例之一，其实，几乎所有的抽象都是泄漏的。

言归正传，正是由于第一批 JavaScript 虚拟机编译器上代码的设计失误，导致变量在声明之前就被赋予了 `undefined` 的初始值，而又由于这个失误产生的影响（无论好坏）过于广泛，因此在现在的 JavaScript 编译器中仍保留了变量提升的“特性”。

至于“为什么要函数提升”，有人提出可能是为了解决函数相互递归调用的问题，布兰登·艾克给予了肯定并补充道：

&gt; Brendan Eich: Yes, function declaration hoisting is for mutual recursion &amp; generally to avoid painful bottom-up ML-like order. / 是的，函数提升是为了解决函数相互递归调用的问题，并在总体上避免了像 ML 语言那样痛苦地自下而上调用的问题。

**函数相互递归调用**，可以理解为 A 函数内会调用到 B 函数，而 B 函数也会调用到 A 函数。ML 语言是 20 世纪 70 年代早期开发出来的通用的函数式编程语言。

举一个经典的例子：

```js
function isEven(n) {
  if (n === 0) {
    return true;
  }
  return isOdd(n - 1);
}

console.log(isEven(4)); // true

function isOdd(n) {
  if (n === 0) {
    return false;
  }
  return isEven(n - 1);
}
```

如果没有函数提升，在调用 `isEven` 方法时，由于 `isOdd` 方法还未声明，那么理论上执行就会出现错误，反之亦然。通过函数提升，`isEven` 和 `isOdd` 方法之间的相互递归调用也就可以实现了。

## 参考资料

对变量提升和函数提升的进一步学习探讨可以参考这篇博文[《我知道你懂 hoisting，可是你了解到多深？》](https://blog.techbridge.cc/2018/11/10/javascript-hoisting/)，非常之棒！

### 技术博客

- [JavaScript 中的 Var，Let 和 Const 有什么区别](https://chinese.freecodecamp.org/news/javascript-var-let-and-const), 2020-12-08
- [从本质上理解 JavaScript 中的变量提升](https://juejin.cn/post/6844903895341219854), 2019-07-23
- [JavaScript：深入理解 JavaScript-词法环境](https://limeii.github.io/2019/05/js-lexical-environment/), 2019-05-06
- [变量声明系列之 ES5(变量提升)](https://blog.csdn.net/weixin_38080573/article/details/79372448), 2018-02-25
- [JavaScript: 变量提升和函数提升](https://www.cnblogs.com/liuhe688/p/5891273.html), 2016-10-18
- [抽象泄漏定律](http://shzhangji.com/cnblogs/2013/12/17/the-law-of-leaky-abstractions/), 2013-12-17, 英文[原文链接](https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/)

### 其它资料

- [Hoisting（变量提升）- MDN](https://developer.mozilla.org/zh-CN/docs/Glossary/Hoisting)
- [Google JavaScript Style Guide](https://google.github.io/styleguide/jsguide.html)
- [let 和 const 命令 - 《ECMAScript 6 入门》](https://es6.ruanyifeng.com)
- [Leaky abstraction - Wikipedia](https://en.wikipedia.org/wiki/Leaky_abstraction)

## 关于抽象泄漏的补充

艾林·约耳·斯波尔斯基（Avram Joel Spolsky）是程序员必备的问答网站 Stack Overflow 的创始人之一，于 2002 年 11 月 11 日在博文 [_The Law of Leaky Abstractions_](https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/) 中对抽象泄漏定律做了非常详尽的描述，其中一些看法让我再赞同不过了，遂记录在这里（中文翻译来自[此博客](http://shzhangji.com/cnblogs/2013/12/17/the-law-of-leaky-abstractions/)）：

&gt; ......
&gt; One reason the law of leaky abstractions is problematic is that it means that abstractions do not really simplify our lives as much as they were meant to. When I’m training someone to be a C++ programmer, it would be nice if I never had to teach them about char*’s and pointer arithmetic. It would be nice if I could go straight to STL strings. But one day they’ll write the code “foo” + “bar”, and truly bizarre things will happen, and then I’ll have to stop and teach them all about char*’s anyway. Or one day they’ll be trying to call a Windows API function that is documented as having an OUT LPTSTR argument and they won’t be able to understand how to call it until they learn about char\*’s, and pointers, and Unicode, and wchar_t’s, and the TCHAR header files, and all that stuff that leaks up.
&gt; 抽象泄漏引发的麻烦之一是，它并没有完全简化我们的工作。当我指导别人学习 C++ 时，我当然希望可以跳过 char \* 和指针运算，直接讲解 STL 字符串类库的使用。但是，当某一天他写出了 “foo” + “bar” 这样的代码，并询问我为什么编译错误时，我还是需要告诉他 char \* 的存在。或者说，当他需要调用一个 Windows API，需要指定 OUT LPTSTR 参数，这时他就必须学习 char \*、指针、Unicode、wchar_t、TCHAR 头文件等一系列知识，这些都是抽象泄漏。
&gt; ......
&gt; In teaching someone about ASP.NET programming, it would be nice if I could just teach them that they can double-click on things and then write code that runs on the server when the user clicks on those things. Indeed ASP.NET abstracts away the difference between writing the HTML code to handle clicking on a hyperlink (\&lt;a&gt;) and the code to handle clicking on a button. Problem: the ASP.NET designers needed to hide the fact that in HTML, there’s no way to submit a form from a hyperlink. They do this by generating a few lines of JavaScript and attaching an onclick handler to the hyperlink. The abstraction leaks, though. If the end-user has JavaScript disabled, the ASP.NET application doesn’t work correctly, and if the programmer doesn’t understand what ASP.NET was abstracting away, they simply won’t have any clue what is wrong.
&gt; 在指导 ASP.NET 编程时，我希望可以直接告诉大家双击页面上的控件，在弹出的代码框中输入点击响应事件。的确，ASP.NET 将处理点击的 HTML 代码抽象掉了，但问题在于，ASP.NET 的设计者需要动用 JavaScript 来模拟表单的提交，因为 HTML 中的 \&lt;a&gt; 标签是没有这一功能的。这样一来，如果终端用户将 JavaScript 禁止了，这个程序将无法运行。初学者会不知所措，直至他了解 ASP.NET 的运作方式，了解它究竟将什么样的工作封装起来了，才能进一步排查。
&gt; The law of leaky abstractions means that whenever somebody comes up with a wizzy new code-generation tool that is supposed to make us all ever-so-efficient, you hear a lot of people saying “learn how to do it manually first, then use the wizzy tool to save time.” Code generation tools which pretend to abstract out something, like all abstractions, leak, and the only way to deal with the leaks competently is to learn about how the abstractions work and what they are abstracting. So the abstractions save us time working, but they don’t save us time learning.
&gt; 由于抽象定律的存在，每当有人说自己发现了一款新的代码生成工具，能够大大提高我们的编程效率时，你会听很多人说“先学习手工编写，再去用工具生成”。代码生成工具是一种抽象，同样也会泄漏，唯一的解决方法是学习它的实现原理，即它抽象了什么。所以说抽象只是用于提高我们的工作效率的，而不会节省我们的学习时间。
&gt; And all this means that paradoxically, even as we have higher and higher level programming tools with better and better abstractions, becoming a proficient programmer is getting harder and harder.
&gt; 这就形成了一个悖论：当我们拥有越来越高级的开发工具，越来越好的“抽象”，要成为一个高水平的程序员反而越来越困难了。
&gt; ......
&gt; Ten years ago, we might have imagined that new programming paradigms would have made programming easier by now. Indeed, the abstractions we’ve created over the years do allow us to deal with new orders of complexity in software development that we didn’t have to deal with ten or fifteen years ago, like GUI programming and network programming. And while these great tools, like modern OO forms-based languages, let us get a lot of work done incredibly quickly, suddenly one day we need to figure out a problem where the abstraction leaked, and it takes 2 weeks. And when you need to hire a programmer to do mostly VB programming, it’s not good enough to hire a VB programmer, because they will get completely stuck in tar every time the VB abstraction leaks.
&gt; 十年前，我们会想象未来能够出现各种新式的编程范型，简化我们的工作。的确，这些年我们创造的各类抽象使得开发复杂的大型软件变得比十五年前要简单得多，就像 GUI 和网络编程。现代的面向对象编程语言让我们的工作变得高效快速。但突然有一天，这种抽象泄漏出一个问题，解决它需要耗费两星期。如果你需要招录一个 VB 程序员，那不是一个好主意，因为当他碰到 VB 语言泄漏的问题时，他会变得寸步难行。
&gt; The Law of Leaky Abstractions is dragging us down.
&gt; 抽象泄漏定律正在阻碍我们前进。
</content:original-text><content:updated-at>2022-10-21T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[函数防抖和节流，以及在 Vue 中的运用]]></title><description><![CDATA[在前端性能优化中存在一个老生常谈的问题：如何优化高频率执行的 JS 代码？例如： 我们为浏览器滚动 scroll 绑定了监听事件，当滚动到某位置之下后，会在浏览器右下方显示一个点击后能快速回到页面顶部的浮动按钮；而滚动回该位置之上时，浮动按钮消失。现在我们发现，用户每次使用滚轮滑动页面，都会触发很多次该事件，判断当前在该位置之上还是之下，这在一定程度上降低了前端的性能。 我们为网页添加了搜索功能…]]></description><link>https://blog.towind.fun/posts/js-debounce-throttle</link><guid isPermaLink="false">js-debounce-throttle</guid><category><![CDATA[前端开发]]></category><pubDate>Sat, 08 May 2021 00:00:00 GMT</pubDate><content:original-text>
在前端性能优化中存在一个老生常谈的问题：如何优化**高频率执行**的 JS 代码？例如：

1. 我们为浏览器滚动 scroll 绑定了监听事件，当滚动到某位置之下后，会在浏览器右下方显示一个点击后能快速回到页面顶部的浮动按钮；而滚动回该位置之上时，浮动按钮消失。现在我们发现，用户每次使用滚轮滑动页面，都会触发很多次该事件，判断当前在该位置之上还是之下，这在一定程度上降低了前端的性能。
2. 我们为网页添加了搜索功能，当用户输入搜索关键字后，会自动显示出搜索的结果。但是，用户每次更改输入都立即调用后端进行了搜索，彼时用户可能尚未输入完关键字，亦或是关键字输入错误需要修改。这样搜索出来的结果并非用户希望看到的，同时还降低了前端性能，浪费了大量的服务器资源。

针对上述列举的问题，我们应该怎么做，才能在优化前端性能的同时不至于影响到用户的体验，便是本文探讨的内容。

## 函数节流

函数节流（Throttle），指在触发事件后的一定时间内绑定的函数只能执行一次。

函数节流的实现思路比较简单，例如使用 `setTimeout` 方法实现：由于 `setTimeout` 方法的返回值是一个正整数，表示定时器的编号，所以可以利用闭包的方法维护一个定时器编号。每次触发事件时都通过定时器编号判断当前是否有尚未到期的定时器，如果有则结束，如果没有则启用一个定时器。定时器到期后调用绑定的需要节流的函数，并设置定时器编号为空，表示可以启用一个新的定时器。代码如下：

```js
/**
 * 函数节流
 * 连续触发事件但是在 wait 毫秒中只执行一次函数
 * @param {Function} func 执行的函数
 * @param {Number} wait 函数节流等待的时间，单位为 ms
 * @returns 节流执行的函数
 */
function throttle1(func, wait) {
  let timer; // 维护的定时器编号
  return function () {
    // 返回节流执行的函数，可以绑定给事件
    const args = arguments; // 执行函数的参数
    if (!timer) {
      // 当定时器不存在或已到期时
      timer = setTimeout(() =&gt; {
        // 启用一个新的定时器
        timer = undefined; // 到期后设置定时器编号为空
        func.apply(this, args); // 到期后执行函数
      }, wait); // 定时器等待 wait 毫秒后执行
    }
  };
}
```

`setTimeout` 的方法，可以在触发事件后的 wait 毫秒自动后执行需要节流的函数。

需要特别留意的是上述代码有这样一个细节：`setTimeout(() =&gt; { func.apply(this, args) }, wait)`。

我们使用了箭头函数，使得 `setTimeout` 中方法内 `this` 的作用于指向绑定此节流函数的对象，而非全局 `window` 对象。

此外，如果不使用 `apply()` 方法而是直接调用函数的话，节流执行函数内的 `this` 对象仍指向的是全局的 `window` 对象，而非我们期望的绑定此节流函数的对象，因此应使用 `apply()` 传入 `this` 上下文对象。

对于实现传入上下文对象，`call()` 方法的作用和 `apply()` 相同，只是前者需要将传入的参数列举出来，而后者需要将传入的参数放在一个数组中。由于我们使用 `const args = arguments` 获取了函数传入的参数，而 `args` 为一个数组，因此选择使用 `apply()` 的方法。

假如不使用箭头函数，应该在 `setTimeout` 方法前获取 `this` 上下文对象，再调用 `apply()` 方法，如：

```js
/**
 * 函数节流非箭头函数版本
 * 连续触发事件但是在 wait 毫秒中只执行一次函数
 * @param {Function} func 执行的函数
 * @param {Number} wait 函数节流等待的时间，单位为 ms
 * @returns 节流执行的函数
 */
function throttle2(func, wait) {
  let timer;
  return function () {
    const args = arguments;
    const that = this; // 获取作用域上下文
    if (!timer) {
      timer = setTimeout(function () {
        // 使用 function () {} 的方式
        timer = undefined;
        func.apply(that, args); // 使用绑定的上下文对象
      }, wait);
    }
  };
}
```

如果不喜欢 `setTimeout` 方法，也可以使用时间戳的方法实现函数节流：利用闭包的方法维护一个时间戳，每次触发事件时通过当前的时间戳和维护的时间戳之间的差值获取间隔的时间。若间隔时间大于预设的等待时间，则执行函数，并设置维护的时间戳为当前的时间戳。

```js
/**
 * 函数节流时间戳版本
 * 连续触发事件但是在 wait 毫秒中只执行一次函数
 * @param {Function} func 执行的函数
 * @param {Number} wait 函数节流等待的时间，单位为 ms
 * @returns 节流执行的函数
 */
function throttle3(func, wait) {
  let previous = new Date();
  return function () {
    const args = arguments;
    const now = new Date(); // 获取当前的时间
    if (now - previous &gt; wait) {
      // Date 对象在计算时会隐式转换为时间戳，当间隔时间大于等待时间时
      previous = now; // 设置维护的时间为当前的时间
      func.apply(this, args); // 执行函数
    }
  };
}
```

时间戳的方法不会在等待时间后自动执行需要节流的函数，而是在下一次触发事件后才执行。应根据具体需求在 `setTimeout` 和时间戳的方法之间进行选择。

特别的，我们可以设置当触发事件后立即执行需要节流的函数，再等待一定时间后才能再次执行此函数。基于 `setTimeout` 的方法，改良代码如下：

```js
/**
 * 函数节流 setTimeout 改良版本
 * 连续触发事件但是在 wait 毫秒中只执行一次函数
 * @param {Function} func 执行的函数
 * @param {Number} wait 函数节流等待的时间，单位为 ms
 * @param {Boolean} immediate 触发后立即执行函数
 * @returns 节流执行的函数
 */
function throttle4(func, wait, immediate = false) {
  let timer;
  return function () {
    const args = arguments;
    if (!timer) {
      if (immediate) {
        // 设置立即执行函数
        timer = setTimeout(() =&gt; {
          // 启用一个新的定时器
          timer = undefined; // 到期后设置定时器编号为空
        }, wait); // 定时器等待 wait 毫秒后执行
        func.apply(this, args); // 立即执行函数
      } else {
        timer = setTimeout(() =&gt; {
          timer = undefined;
          func.apply(this, args);
        }, wait);
      }
    }
  };
}
```

## 函数防抖

函数防抖（Debounce），指在触发事件后的一定时间内绑定的函数只能执行一次，如果在这段时间内又触发了事件，则会重新计算时间。

从定义上来看，函数防抖像是函数节流的“强化版”：函数节流保证在一定时间内只执行一次事件绑定的函数，而函数防抖确保了事件在**一定时间内稳定不变**后才执行绑定的函数。

函数防抖的实现思路更加简单：同样适用闭包的方法维护一个定时器编号，每次触发事件时都通过此编号取消之前的定时器，并启用一个新的定时器。定时器到期后执行需要防抖的函数，并设置定时器编号为空。

特别的，我们也可以设置当触发事件后立即执行需要防抖的函数。触发事件时，若维护的定时器编号为空，表示可以立即执行函数。此时启用一个定时器，定时器到期后设置编号为空。当存在定时器编号时，表示仍在等待时间内，不会执行需要防抖的函数，此时我们清除前一个定时器，并启用一个新的定时器。

代码如下：

```js
/**
 * 函数防抖
 * 触发事件后在 wait 毫秒内函数只执行一次；如果在 wait 毫秒内又触发了事件，则会重新计算函数执行时间
 * @param {Function} func 需要防抖的函数
 * @param {Number} wait 防抖的等待时间，单位为 ms
 * @param {Boolean} immediate 触发事件后立即执行函数
 * @returns 防抖执行的函数
 */
function debounce(func, wait, immediate = false) {
  let timer;
  return function () {
    const args = arguments;

    timer &amp;&amp; clearTimeout(timer); // 如果定时器编号不为空，则清除定时器。此处只是清除定时器，并未清除定时器编号

    if (immediate) {
      // 设置立即执行函数
      !timer &amp;&amp; func.apply(this, args); // 如果定时器编号不为空，即在等待时间内，不执行函数；若为空，则执行函数
      timer = setTimeout(() =&gt; {
        // 启用新的定时器
        timer = undefined; // 定时器到期后清空定时器编号
      }, wait); // 定时器等待 wait 毫秒后执行
    } else {
      // 不立即执行函数
      timer = setTimeout(() =&gt; {
        // 启用新的定时器
        func.apply(this, args); // 定时器到期后执行函数
      }, wait); // 定时器等待 wait 毫秒后执行
    }
  };
}
```

## 在 Nuxt.js 中引入函数节流和防抖

在项目的 `plugins` 目录下创建一个新的文件，例如 `main.js`。将函数节流和防抖添加为 Vue 的实例方法。如：

```js
// plugins/main.js
import Vue from &quot;vue&quot;;

function throttle() {
  //
}

function debounce() {
  //
}

const main = {
  install(Vue) {
    // 注册到 Vue.prototype.$Main 中
    Vue.prototype.$Main = {
      throttle,
      debounce,
    };
  },
};

Vue.use(main);
```

接下来在 `nuxt.config.js` 中引入：

```js
export default {
  plugins: [&quot;~/plugins/main.js&quot;],
};
```

就可以在组件中通过 `this.$Main.throttle()` 调用函数了。其中 `this` 指向了全局的 Vue 对象。

## 简单的使用示例

### 浏览器滚动事件

对于本博客开头提出的第一种情况，我们可以使用函数节流的方案优化前端性能。

为什么不用函数防抖？假如用户一直在滚动浏览器，那么直到用户停止滚动前，都不会执行函数判断当前滚动位置。而使用函数节流，无论用户是否一直在滚动浏览器，都会在一定时间后再次执行函数判断当前滚动位置。

基于 Vuetify UI 组件库编写 Vue 代码如下：

```vue
&lt;template&gt;
  &lt;v-fab-transition&gt;
    &lt;!-- 当窗口滚动值大于 300 时显示按钮 --&gt;
    &lt;v-btn
      v-show=&quot;scrollVal &gt; 300&quot;
      fixed
      fab
      dark
      bottom
      right
      color=&quot;white&quot;
      elevation=&quot;2&quot;
      class=&quot;mb-12&quot;
      @click=&quot;backToTop&quot;
    &gt;
      &lt;v-icon color=&quot;primary&quot;&gt;mdi-arrow-up&lt;/v-icon&gt;
    &lt;/v-btn&gt;
  &lt;/v-fab-transition&gt;
&lt;/template&gt;

&lt;script&gt;
export default {
  data: () =&gt; ({
    // 当前的窗口滚动值
    scrollVal: 0,
  }),
  mounted() {
    // 每 500 毫秒获取当前的 scrollVal 值
    const throttleOnScroll = this.$Main.throttle(this.onScroll, 500);
    // 为 window 添加滚动事件
    window.addEventListener(&quot;scroll&quot;, throttleOnScroll);
  },
  methods: {
    // 获取 window.pageYOffset 值并赋值给 scrollVal
    onScroll() {
      this.scrollVal = window.pageYOffset;
    },
    // 回到顶端
    backToTop() {
      window.scroll({
        top: 0,
        left: 0,
        behavior: &quot;smooth&quot;,
      });
    },
  },
};
&lt;/script&gt;
```

`window.pageYOffset` 是 `window.scrollY` 的别名，前者的浏览器兼容性较好，调用时将返回文档在垂直方向已滚动的像素值。

`onScroll()` 方法可以获取当前文档在垂直方向已滚动的像素值并赋值给 `scrollVal`，而浮动按钮根据此值判断是否显示。上述代码设定当该值大于 300 时显示浮动按钮。

上述代码将 `onScroll()` 方法封装成了一个等待时间为 500 毫秒的节流函数 `throttleOnScroll()`，并将该节流函数绑定给浏览器滚动事件。

当用户滚动浏览器时，每隔 500 毫秒会获取当前已滚动的像素值，浮动按钮再根据此值判断是否显示，性能优化完成！

## Easy ride

不想自己手撸函数节流和防抖？

那就用封装好的吧：[Lodash](https://lodash.com/)，你值得拥有。

## 参考资料

- [终于搞懂：防抖和节流](https://juejin.cn/post/6914591853882900488), 2021-01-06
- [彻底弄懂函数防抖和函数节流](https://segmentfault.com/a/1190000018445196), 2019-03-09
- [什么是防抖和节流？有什么区别？如何实现](https://github.com/Advanced-Frontend/Daily-Interview-Question/issues/5), 2019-01-23
- [浅析函数防抖与函数节流](https://www.jianshu.com/p/f9f6b637fd6c), 2018-08-12
</content:original-text><content:updated-at>2021-05-08T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[Windows 系统编译安装基于 C++ 的 gRPC]]></title><description><![CDATA[本博客基于 CMake 实现编译  版本。 本博客的 Windows 端使用的命令提示符界面为 Powershell。

安装编译依赖软件

在 Windows 系统上编译 gRPC 需要首先准备下述软件：

Visual Studio 2015（或 2017），将使用到 Visual C++ compiler Git CMake nasm ninja（可选）
Visual Studio 2015…]]></description><link>https://blog.towind.fun/posts/windows-install-grpc</link><guid isPermaLink="false">windows-install-grpc</guid><category><![CDATA[后端开发]]></category><pubDate>Mon, 26 Apr 2021 00:00:00 GMT</pubDate><content:original-text>
本博客基于 CMake 实现编译 `gRPC 1.28.1` 版本。

本博客的 Windows 端使用的命令提示符界面为 Powershell。

## 安装编译依赖软件

在 Windows 系统上编译 gRPC 需要首先准备下述软件：

- Visual Studio 2015（或 2017），将使用到 Visual C++ compiler
- Git
- CMake
- nasm
- ninja（可选）

### Visual Studio 2015 (或 2017)

用于编译 gRPC。下简称 VS。

在微软的 [VS 官网](https://visualstudio.microsoft.com/zh-hans/)下载安装即可。

### Git

用于拉取 gRPC 库并下载所需的第三方依赖。

在 [Git 官网](https://git-scm.com/)下载并安装即可。

```powershell
PS C:\Users\lolipop&gt; git --version
git version 2.21.0.windows.1
```

### CMake

用于生成编译 gRPC 的 Makefile 文件。

在 [CMake 官网](https://cmake.org/download/)下载，可以选择下载 `.msi` 文件直接安装。

例如对于 64 位的 Windows 电脑安装 `CMake 3.20.1`，找到：

| Platform                                                                               | Files                           |
| -------------------------------------------------------------------------------------- | ------------------------------- |
| Windows x64 Installer: Installer tool has changed. Uninstall CMake 3.4 or lower first! | cmake-3.20.1-windows-x86_64.msi |

下载并运行 `.msi` 文件安装即可。

```powershell
PS C:\Users\lolipop&gt; cmake --version
cmake version 3.20.1
```

### nasm

gRPC 的第三方依赖 `boringssl` 需要此软件。

在 [nasm 官网](https://www.nasm.us/)下载，可以选择下载 `.exe` 文件直接安装。

例如对于 64 位的 Windows 电脑安装 `nasm 2.15.05`，可以进入 `/pub/nasm/releasebuilds/2.15.05/win64` 目录下载 `nasm-2.15.05-installer-x64.exe` 文件并执行安装操作。

nasm 默认安装目录为 `C:\Users\${您的工号}\AppData\Local\bin\NASM`（若非此目录，请在安装界面确认安装的路径），将该目录添加到环境变量中即可。

```powershell
PS C:\Users\lolipop&gt; nasm --version
NASM version 2.15.05 compiled on Aug 28 2020
```

### ninja（可选）

您可以使用 Ninja 来加速编译。

假如您希望使用它，后续的编译操作可参考[官方文档](https://github.com/grpc/grpc/blob/master/BUILDING.md#windows-using-ninja-faster-build)，本博客**不使用** Ninja 加速编译。

## 拉取 gRPC 库

建议在能够连接 Github 的机器环境利用 Git 克隆 gRPC 库并获取第三方依赖，再打包出来给 windows 系统编译使用。

```bash
# 克隆 gRPC 仓库
# 对于特定的分支，例如 gRPC 1.28.1 版本，可以使用此命令：
# git clone https://github.com/grpc/grpc.git -b v1.28.1
git clone https://github.com/grpc/grpc.git
# 获取 gRPC 第三方依赖
cd grpc
git submodule update --init
```

## 编译安装 gRPC

特别的，如果您使用的 CMake 版本低于 3.13，或编译的 gRPC 版本低于 1.27，在执行 `cmake` 命令之前，需要自行手动编译安装 gRPC 的依赖库，且在执行 `cmake` 命令时指定这些库的路径，这里是[官方的说明](https://github.com/grpc/grpc/blob/master/BUILDING.md#install-after-build)。

下面的内容基于的 CMake 版本不低于 3.13，且编译的 gRPC 版本不低于 1.27。

首先创建文件夹存储编译结果，并执行 `cmake` 命令生成 Makefile 文件。使用**管理员权限**打开命令行界面，执行下面的命令：

```powershell
# 在 grpc 目录下创建 .build 目录并进入
md .build
cd .build
# 生成 Makefile 文件
# 其中 Visual Studio 15 2017 为当前的 VS 版本
cmake .. -DgRPC_INSTALL=ON -G &quot;Visual Studio 15 2017&quot;
```

&gt; 尽管[不推荐](https://github.com/grpc/grpc/blob/master/BUILDING.md#windows-a-note-on-building-shared-libs-dlls)，在上面执行 `cmake` 命令时，您可以指定 `-DBUILD_SHARED_LIBS=ON` 以编译生成 gRPC C++ 的 DLL 文件。

接下来对 gRPC 进行编译安装操作，包括两种方式，这里更建议使用 VS。

### 使用 VS 编译并安装 gRPC

首先，使用**管理员权限**打开 VS，否则在安装 gRPC 时会报错。

接着使用 VS 打开此目录下的 `grpc.sln` 解决方案，找到**解决方案资源管理器**（默认情况下在 VS 的右侧）中的 `ALL_BUILD` 项，右键并选择**生成**按钮，开始执行编译操作。

编译结束后，在**解决方案资源管理器**中找到 `INSTALL` 项，右键并选择**生成**按钮，开始执行安装操作。

gRPC 默认安装在 `C:\Program Files (x86)\grpc` 目录。

如果报错，请确保您已使用管理员权限打开 VS。使用管理员权限重新打开 VS 后，右键点击 `ALL_BUILD` 并选择**重新生成**按钮即可。

### 使用命令行编译并安装 gRPC

您也可以直接使用命令行界面，执行 `cmake` 命令来编译安装 gRPC。

```powershell
# 编译并安装 gRPC
cmake --build . --target install --config Release
```

编译结束后，会自动进行安装操作。

gRPC 默认安装在 `C:\Program Files (x86)\grpc` 目录。

如果没有使用管理员权限打开命令行界面，安装时会发生报错。请重新使用管理员权限打开命令行界面，并执行上面的命令。

## 测试 gRPC 编译安装结果

接下来的步骤基于 VS 编译安装 `gRPC 1.28.1` 的结果，测试 Windows 系统下的 gRPC 环境是否安装成功。

移动到 git 克隆的 gRPC 源码目录下的 `grpc\examples\cpp\helloworld` 目录，创建存放编译结果的文件夹 `cmake\build`，进入到该目录，执行下面的命令：

```powershell
# 生成 Makefile 文件
# 其中 C:\Program Files (x86)\grpc 是 gRPC 默认的安装目录
cmake -DCMAKE_PREFIX_PATH=&apos;C:\Program Files (x86)\grpc&apos; ../..
```

使用管理员权限打开 VS，并打开当前目录下的 `HelloWorld.sln` 解决方案，右键分别选择解决方案资源管理器中的 `greeter_client.cc` 和 `greeter_server.cc` 并点击生成按钮。

编译完成后，会在 `cmake\build\Debug` 目录下生成我们需要的可执行文件 `greeter_server.exe` 和 `greeter_client.exe`。

使用 Powershell 移动到该目录，启动服务端 `./greeter_server.exe`：

```powershell
PS grpc\examples\cpp\helloworld\cmake\build\Debug&gt; ./greeter_server.exe
Server listening on 0.0.0.0:50051
```

服务端默认监听 50051 端口。

再启动一个 Powershell 移动到该目录，启动客户端 `./greeter_client.exe`：

```powershell
PS grpc\examples\cpp\helloworld\cmake\build\Debug&gt; ./greeter_client.exe
Greeter received: Hello world
```

成功打印出 `Greeter received: Hello world` 字段，测试成功！

作为对照组，您可以关闭掉服务端，再执行客户端，观察打印的结果。

Hello, gRPC world!

## 参考资料

- [编译 gRPC(windows)和测试 demo](https://blog.csdn.net/xiaoyafang123/article/details/76529917) - 2017.08.01 - 注：博主填的转载，暂未找到原文链接
</content:original-text><content:updated-at>2021-04-26T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[Linux 系统编译安装基于 C++ 的 gRPC]]></title><description><![CDATA[本文适用于 C++ 版本 gRPC 的离线编译安装，但对于下载 gRPC 步骤强烈建议使用 git 进行。 如果在能直接连接外网的机器上编译，可直接按照 gRPC 官网文档的指引快速执行编译操作。

安装基本依赖

确保机器上包括这些基本依赖：, ,  与 C++ 编译环境。

gRPC 的编译需要  版本在  及以上。假如版本低于此，应当在 Docker 容器中安装较新版本的 GCC 再执行编译操作…]]></description><link>https://blog.towind.fun/posts/linux-docker-install-grpc</link><guid isPermaLink="false">linux-docker-install-grpc</guid><category><![CDATA[后端开发]]></category><pubDate>Thu, 22 Apr 2021 00:00:00 GMT</pubDate><content:original-text>
本文适用于 C++ 版本 gRPC 的离线编译安装，但对于[下载 gRPC](#下载-gRPC) 步骤强烈建议使用 git 进行。

如果在能直接连接外网的机器上编译，可直接按照 [gRPC 官网文档](https://github.com/grpc/grpc)的指引快速执行编译操作。

## 安装基本依赖

确保机器上包括这些基本依赖：`autoconf`, `libtool`, `pkg-config` 与 C++ 编译环境。

```bash
# 检查是否有 autoconf
which autoconf

# 如果没有，则安装
# CentOS
yum install autoconf
# Ubuntu
apt-get install autoconf
```

gRPC 的编译需要 `gcc` 版本在 `4.9` 及以上。假如版本低于此，应当在 Docker 容器中安装较新版本的 GCC 再执行编译操作。

我使用 `gcc 4.9.4` 成功编译 `gRPC 1.28.x`，另外[有人测试](https://github.com/grpc/grpc/issues/24932#issuecomment-754344093)在 `4.9.2`, `5.3.1` 以及 `7.3.1` 版本编译成功；而我使用撰写此文时使用最新版本 `10.3.0` 编译报错，请读者加以选择。

更新 GCC 的方法可以参考我的&lt;Link to=&quot;/posts/linux-docker-gcc-update&quot;&gt;这一篇博客&lt;/Link&gt;。

```bash
# 查看 gcc 版本
gcc -v
```

如果机器上没有 `gcc` 或 `g++` 等，可以安装 `Development Tools` 或 `build-essential` 软件包。

```bash
# CentOS
yum groupinstall &quot;Development Tools&quot;
# Ubuntu
apt-get install -y build-essential
```

## 安装 CMake

&gt; `make` 是 gRPC 以前使用的构建命令，但是官方文档不再建议使用它。应使用 `bazel` 或 `cmake` 代替。此处我们选择使用 `cmake` 执行编译。

执行下述命令，如果没有找到命令则需要安装 CMake。目前编译 gRPC 需要的 CMake 最低版本为 `3.5.1`，建议使用的 CMake 版本为 `3.13` 及以上。

```bash
# 查看当前 CMake 版本
cmake --version
```

在 [CMake 官网](https://cmake.org/download/)下载需要版本的 CMake 源码或二进制文件。

例如下载适用于 x86_64 的 Linux 系统的二进制文件，可以选择下载 `cmake-3.20.1-linux-x86_64.tar.gz`，其中 `3.20.1` 为版本号。

解压，可以将 `/path/to/cmake-3.20.1-linux-x86_64/bin/` 目录下的二进制文件复制粘贴到 `/usr/bin/` 目录下；或是为它们创建软链接，创建软链接应使用绝对路径。

```bash
# 解压
tar -zxvf cmake-3.20.1-linux-x86_64.tar.gz
# 为二进制文件创建软链接
sudo ln -sf /path/to/cmake-3.20.1-linux-x86_64/bin/* /usr/bin/
# 再次执行，确保安装成功
cmake --version
```

查看版本号时如果提示 `CMake Error: Could not find CMAKE_ROOT !!!`，可能是原本调用的 CMake 二进制文件存放在其它目录下。例如，原来的 CMake 二进制文件存放在 `/usr/local/bin/` 目录下，而调用命令时系统又优先从该目录搜索命令。因此应在创建软链接时应执行：

```bash
# 创建 cmake 二进制文件软链接
sudo ln -sf /path/to/cmake-3.20.1-linux-x86_64/bin/* /usr/local/bin/
```

对于其它路径，可以通过 `find / -name &quot;cmake&quot;` 来寻找。

## 下载 gRPC

建议在能够直接访问外网的环境利用 git 克隆 gRPC 库并获取第三方依赖，再打包出来给其它环境编译使用。

手动下载 gRPC 及第三方依赖耗时耗力，还有可能像我一样“赔了夫人又折兵”依然编译不了。

```bash
# 克隆 gRPC 仓库
git clone https://github.com/grpc/grpc.git
cd grpc
# 获取 gRPC 第三方依赖
git submodule update --init
```

## 编译安装 gRPC

官方文档[建议](https://grpc.io/docs/languages/cpp/quickstart/#build-and-install-grpc-protocol-buffers-and-abseil)用户选择本地路径安装 gRPC，因为全局安装后想要卸载 gRPC 会十分复杂。因此，在编译安装之前，可以首先选择一个用户本地的路径。

```bash
# 安装到 $HOME/.local 中
export MY_INSTALL_DIR=$HOME/.local
# 确保目录存在
mkdir -p $MY_INSTALL_DIR
# 添加该路径下的 bin 目录到环境变量
export PATH=&quot;$PATH:$MY_INSTALL_DIR/bin&quot;
```

在 gRPC 根目录下执行[下述操作](https://github.com/grpc/grpc/blob/master/BUILDING.md#building-with-cmake)：

```bash
# 创建存放编译 gRPC 结果的目录
mkdir -p cmake/build
# 进入到该目录
pushd cmake/build
# 生成编译 gRPC 的 Makefile 文件
# 其中 DCMAKE_INSTALL_PREFIX 指定了 gRPC 的安装路径
cmake -DgRPC_INSTALL=ON \
    -DgRPC_BUILD_TESTS=OFF \
    -DCMAKE_INSTALL_PREFIX=$MY_INSTALL_DIR \
    ../..
# 执行编译
# ${JOBS_NUM} 为同时执行的线程数，应替换为数字，下同
make -j ${JOBS_NUM}
# 安装 gRPC
make install
```

如果想要编译动态库 `.so` 文件，可以在上一步执行 `cmake` 命令时设置 `-DBUILD_SHARED_LIBS=ON`，如：

```bash
# 生成编译 gRPC 的 Makefile 文件
cmake -DBUILD_SHARED_LIBS=ON ../..
```

假如编译失败，可以参考笔者遇到的[错误和解决方案](#可能遇见的错误)。

C++ 版本的 gRPC 还依赖于 Abseil C++ 库，因此需要单独编译安装它：

```bash
# 回到 gRPC 根目录
popd
# 创建存放 Abseil C++ 编译结果的目录
mkdir -p third_party/abseil-cpp/cmake/build
# 进入到编译目录
pushd third_party/abseil-cpp/cmake/build
# 生成编译 abseil-cpp 的 Makefile 文件
cmake -DCMAKE_INSTALL_PREFIX=$MY_INSTALL_DIR \
    -DCMAKE_POSITION_INDEPENDENT_CODE=TRUE \
    ../..
# 执行编译
make -j ${JOBS_NUM}
# 安装 Abseil C++
make install
```

哈！大功告成。最后我们来测试一下 gRPC 是否安装成功。

## 测试编译安装 gRPC 成功

首先编译 gRPC 提供的示例：

```bash
# 回到 gRPC 根目录
popd
# 进入 example 目录
cd examples/cpp/helloworld
# 创建存放 example 编译结果的目录
mkdir -p cmake/build
# 进入到编译目录
pushd cmake/build
# 生成编译 example 的 Makefile 文件
# 其中 DCMAKE_PREFIX_PATH 指定我们使用的 gRPC 路径，即 gRPC 的安装路径
cmake -DCMAKE_PREFIX_PATH=$MY_INSTALL_DIR ../..
# 执行编译
make -j ${JOBS_NUM}
```

这样，在当前目录就会生成编译好的二进制文件。试试看吧！

在当前终端启用 gRPC 示例的服务端，它会默认监听当前主机的 `50051` 端口：

```bash
./greeter_server

# 显示内容如下
Server listening on 0.0.0.0:50051
```

打开一个新终端，进入到此目录，运行客户端，就可以看到访问的结果啦：

```bash
./greeter_client

# 显示内容如下
Greeter received: Hello world
```

假如退出了服务端，再运行客户端，则会打印：

```bash
# 关闭服务端，然后执行
./greeter_client

# 显示内容如下
14: failed to connect to all addresses
Greeter received: RPC failed
```

开始愉快地编写 gRPC 程序吧！

## 可能遇见的错误

### 编译 gRPC 执行 make 后提示 error

```bash
error: no matching function for call to ‘StrFormat(const char [22], const char*, char [64], int32_t&amp;, long int&amp;, const char*&amp;, int&amp;)’
```

提示报错没有找到 `StrFormat` 函数，请确保 `gcc` 版本在 `4.9` 及以上，可以执行 `gcc -v` 命令查看当前版本。

建议[更新](http://3ms.huawei.com/km/blogs/details/10193429)到 `gcc 4.9.4` 版本，笔者在该版本下顺利编译 gRPC。
</content:original-text><content:updated-at>2021-05-11T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[Linux 容器更新或降级 GCC 版本]]></title><description><![CDATA[如果软件源可用，可以使用 CentOS 的 yum 包管理器或 Ubuntu 的 apt 包管理器等一键安装 GCC，例如： 本文适用于系统中包含有其它版本的 GCC 编译器情况下，手动更新或降级 GCC 编译器。编译 GCC 的过程十分耗时，如果能使用包管理器尽量还是使用包管理器吧。

NOTE: 如果仅使用 GCC 进行编译操作或不确定当前系统能否兼容新版本的 GCC，建议在 Docker…]]></description><link>https://blog.towind.fun/posts/linux-docker-gcc-update</link><guid isPermaLink="false">linux-docker-gcc-update</guid><category><![CDATA[后端开发]]></category><pubDate>Tue, 20 Apr 2021 00:00:00 GMT</pubDate><content:original-text>
如果软件源可用，可以使用 CentOS 的 yum 包管理器或 Ubuntu 的 apt 包管理器等一键安装 GCC，例如：

```bash
yum -y install gcc
yum -y install gcc-c++

# 或是一键安装开发工具软件包，包括 gcc, g++ 等
yum groupinstall &quot;Development Tools&quot;
```

本文适用于系统中**包含有**其它版本的 GCC 编译器情况下，手动更新或降级 GCC 编译器。编译 GCC 的过程十分耗时，如果能使用包管理器尽量还是使用包管理器吧。

NOTE: 如果仅使用 GCC 进行编译操作或不确定当前系统能否兼容新版本的 GCC，建议在 **Docker 容器环境**中执行编译和安装操作，并在容器中使用 GCC 编译器进行编译源码。

```bash
# 查看当前系统中 GCC 的版本
gcc -v
```

## 下载 GCC 并解压

在 [GCC 官网](https://gcc.gnu.org/mirrors.html)选择下载 GCC 的镜像站点，选择进入 `release/` 目录，选择需要的 GCC 版本下载即可。

本文以安装 `GCC 10.3.0` 为例，进入 `release/gcc-10.3.0/` 目录，选择 `gcc-10.3.0.tar.gz` 进行下载。

将压缩包传入 Docker 容器环境，并解压到容器的 `/usr/local/` 目录下。

```bash
tar -xf gcc-10.3.0.tar.gz -C /usr/local/
```

## 下载 GCC 依赖包

执行以下命令，下载 GCC 编译所需的依赖包：

```bash
cd /usr/local/gcc-10.3.0
# 执行脚本下载依赖包
./contrib/download_prerequisites
```

对于较老版本的 GCC（例如 `4.9.4` 版本），执行脚本时可能会无法连接服务器，可以更换代理进行下载：

```bash
# 编辑 contrib/download_prerequisites 文件
vim ./contrib/download_prerequisites
```

将文件中 `ftp://gcc.gnu.org/pub/gcc/infrastructure` 字段更换为 `http://www.mirrorservice.org/sites/sourceware.org/pub/gcc/infrastructure`，然后再在根目录执行 `./contrib/download_prerequisites` 命令下载依赖包即可。

对于较新版本的 GCC（例如 `10.3.0` 版本），依赖包包括 `gmp`, `mpfr`, `mpc` 以及 `isl`。提示如下，表示依赖下载成功：

```bash
2021-04-19 15:32:27 URL:http://gcc.gnu.org/pub/gcc/infrastructure/gmp-6.1.0.tar.bz2 [2383840/2383840] -&gt; &quot;./gmp-6.1.0.tar.bz2&quot; [1]
2021-04-19 15:32:30 URL:http://gcc.gnu.org/pub/gcc/infrastructure/mpfr-3.1.4.tar.bz2 [1279284/1279284] -&gt; &quot;./mpfr-3.1.4.tar.bz2&quot; [1]
2021-04-19 15:32:34 URL:http://gcc.gnu.org/pub/gcc/infrastructure/mpc-1.0.3.tar.gz [669925/669925] -&gt; &quot;./mpc-1.0.3.tar.gz&quot; [1]
2021-04-19 15:32:38 URL:http://gcc.gnu.org/pub/gcc/infrastructure/isl-0.18.tar.bz2 [1658291/1658291] -&gt; &quot;./isl-0.18.tar.bz2&quot; [1]
gmp-6.1.0.tar.bz2: OK
mpfr-3.1.4.tar.bz2: OK
mpc-1.0.3.tar.gz: OK
isl-0.18.tar.bz2: OK
All prerequisites downloaded successfully.
```

## 编译 GCC

回到上一级目录即 `/usr/local`，手动创建一个目录，存放编译 GCC 源码生成的文件：

```bash
cd /usr/local
mkdir gcc-build-10.3.0
cd gcc-build-10.3.0
```

现在我们在 `/usr/local/` 路径下创建了一个名为 `gcc-build-10.3.0` 的目录，并进入到此目录中。

GCC 编译器支持多种编程语言的编译，但我们一般使用它来编译 C 和 C++ 语言程序的源码。因此在编译 GCC 之前可以配置只启用 C 和 C++ 语言支持。这一步为**可选**操作：

```bash
/usr/local/gcc-10.3.0/configure --enable-checking=release --enable-languages=c,c++ --disable-multilib
```

如果不需要禁用多语言编译支持，则直接运行 `configure` 即可：

```bash
/usr/local/gcc-10.3.0/configure
```

执行完成后，会在当前路径创建 `Makefile` 文件，执行 `make` 命令编译 GCC 源程序即可。

编译过程会占用很大的空间，请确保当前目录的空间足够使用。

考虑到单作业执行编译操作十分之慢（参考文档的作者在他的机器上花费了 6 小时才完成编译），可以设置 `-j` 选项执行[并行作业](https://stackoverflow.com/questions/414714/compiling-with-g-using-multiple-cores)，选项后边的数字建议为 CPU 内核数量的 1.5 倍甚至 2 倍。如下述命令同时启用 8 个作业并行编译 GCC：

```bash
make -j 8
```

现在，忘记这边的事情，去做一些其它的事儿吧！

---

Tue Apr 20 9:52 CST 2021 - Tue Apr 20 12:33:29 CST 2021.

好久不见！我的机器在 8 个作业并行编译的情况下，大约花费了 2.5 小时完成了编译。

现在执行下述命令安装 GCC 即可：

```bash
make install
```

此时直接执行 `gcc -v` 仍会显示以前安装的版本，在**重启系统**之后就会显示为当前安装的版本。

```bash
# 重启容器环境
docker restart ${CONTAINER}

# 检查安装是否成功
gcc -v

# 显示如下内容表示安装成功
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/local/libexec/gcc/x86_64-pc-linux-gnu/10.3.0/lto-wrapper
Target: x86_64-pc-linux-gnu
Configured with: /usr/local/gcc-10.3.0/configure --enable-checking=release --enable-languages=c,c++ --disable-multilib
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 10.3.0 (GCC)
```

## 文末碎碎念

我没有在 Docker 容器环境里执行 `make install` 操作，接着直接重启了当前主机，导致主机出现问题没法登陆。

一般建议编译和安装操作都在 Docker 容器环境里进行，不要直接操作宿主机环境！

## 参考文档

- [GCC 编译器下载和安装教程（针对 Linux 发行版）](http://c.biancheng.net/view/7933.html)
</content:original-text><content:updated-at>2021-04-20T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[Protobuf 学习笔记]]></title><description><![CDATA[实习中学习一下 Protobuf 的功能和语法等，整理为此笔记。主要为翻译官方文档而来。 什么是 Protobuf

Protobuf 是 Google 公司研发的一种用于序列化结构数据的机制，全称为 Protocol Buffers，具有语言无关、平台无关以及可拓展的特性。

我们常常把 Protobuf 与 XML (Extensible Markup Language) 相比较…]]></description><link>https://blog.towind.fun/posts/protobuf-learning</link><guid isPermaLink="false">protobuf-learning</guid><category><![CDATA[技术琐事]]></category><pubDate>Mon, 29 Mar 2021 00:00:00 GMT</pubDate><content:original-text>
实习中学习一下 Protobuf 的功能和语法等，整理为此笔记。主要为翻译官方文档而来。

## 什么是 Protobuf

Protobuf 是 Google 公司研发的一种用于序列化结构数据的机制，全称为 Protocol Buffers，具有语言无关、平台无关以及可拓展的特性。

我们常常把 Protobuf 与 XML (Extensible Markup Language) 相比较，它们二者都被设计来传输和存储结构化数据。相比于 XML，Protobuf 有如下优势与缺点：

- **Protobuf 占用的空间更小**。Protobuf 采用二进制格式存储数据，适合网络传输和高性能场景；而 XML 采用文本格式存储数据，数据冗余度较高。
- **Protobuf 编码和解码更快**。测试 Protobuf 库和 tinyxml2 库执行序列化和反序列化操作（[相关链接](https://zhuanlan.zhihu.com/p/91313277)），Protobuf 序列化速度大约是 XML 的 5 - 9 倍，反序列化速度大约是 XML 的 9 - 12 倍，更加适合高性能场景。
- **Protobuf 不具有可读性**。Protobuf 传输的值为二进制数据，需要专用工具生成和解析；而 XML 自身的标签和文本内容具有一定的可读性。

&lt;details&gt;
&lt;summary&gt;使用 Protobuf，只需要编写 &lt;code&gt;.proto&lt;/code&gt; 文件来描述需要传输和存储的结构数据，随后编译器会为之创建一个类，实现结构数据的自动编码和解码。&lt;/summary&gt;

With protocol buffers, you write a .proto description of the data structure you wish to store. From that, the protocol buffer compiler creates a class that implements automatic encoding and parsing of the protocol buffer data with an efficient binary format. The generated class provides getters and setters for the fields that make up a protocol buffer and takes care of the details of reading and writing the protocol buffer as a unit.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;此外，Protobuf 支持使用特定的方式来拓展格式，使代码能够解析以前格式编码得到的数据。&lt;/summary&gt;

Importantly, the protocol buffer format supports the idea of extending the format over time in such a way that the code can still read data encoded with the old format.

&lt;/details&gt;

## 定义协议格式

为了创建基于 Protobuf 的应用程序，我们需要首先创建一个 `.proto` 文件并且给出定义：为需要序列化的每个结构数据添加一条 **message** ，然后为 message 的每个字段指定名称和类型。下面是一个来自官网的基于 C++ 语言的例子，可以让您对 `.proto` 文件有一个更加直观的了解：

```proto
syntax = &quot;proto2&quot;; // 协议版本

package tutorial; // 程序包声明

message Person {
  optional string name = 1;
  optional int32 id = 2;
  optional string email = 3;

  enum PhoneType {
    MOBILE = 0;
    HOME = 1;
    WORK = 2;
  }

  message PhoneNumber {
    optional string number = 1;
    optional PhoneType type = 2 [default = HOME];
  }

  repeated PhoneNumber phones = 4;
}

message AddressBook {
  repeated Person people = 1;
}
```

在开头，首先指定了协议的版本，`syntax = &quot;proto2&quot;;` 表示应使用 proto2 进行编码和解码。同理，如果指定为 `syntax = &quot;proto3&quot;;` 则表示应使用 proto3 进行编码和解码。如果不指定协议版本，在默认情况下，编译器会使用 proto2 进行编码和解码。

使用程序包声明 `package tutorial;` 有助于防止不同项目之间的命名发生冲突。在 C++ 中，生成的类将放置在与程序包名称匹配的命名空间中。

接下来就是最重要的 message 定义了。message 是包含一组字段类型的总合。我们将基于 **proto3** 版本对 message 语法进行讲解与描述。

## proto3 基础语法

### [定义消息类型](https://developers.google.com/protocol-buffers/docs/proto3#simple)

下面是一个非常简单的 `.proto` 例子：

```proto
syntax = &quot;proto3&quot;;

message SearchRequest {
  string query = 1;
  int32 page_number = 2;
  int32 result_per_page = 3;
}
```

其中第一行需要指定正在使用 `proto3` 语法，否则编译器将假定正在使用 `proto2` 语法。指定语法版本必须在文件的第一个非空、非注释行。

例子定义了一个名为 `SearchRequest` 的 message，存储了三个字段，包括字符串类型的 `query` 和整数类型的 `page_number` 与 `result_per_page`. 三个字段均为标量值类型，所有可用的标量值类型可参考[此链接](https://developers.google.com/protocol-buffers/docs/proto3#scalar)。除了标量值类型，字段还可以使用枚举和其它 message 类型。

每个定义的字段都有一个**唯一**的编号，用来标识二进制格式下的字段。例如 `query` 字段的唯一编号为 `1`. 对字段编号的补充可参考[此链接](https://developers.google.com/protocol-buffers/docs/proto3#assigning_field_numbers)。

### [字段规则](https://developers.google.com/protocol-buffers/docs/proto3#specifying_field_rules)

与 proto2 不同的是，proto3 只包括两种字段规则：

- singular. 一则 message 只能拥有不超过一个该字段。是**默认**的字段规则，不需要特别指定；
- `repeated`. 一则 message 可以拥有任意个该字段。重复值的顺序将被保留。

```proto
message SearchRequest {
  string query = 1;
  int32 page_number = 2;
  int32 result_per_page = 3;
  repeated string query_extras = 4;
}
```

上例中定义了三个 singular 字段 `query`, `page_number` 和 `result_per_page`，以及一个 `repeated` 字段 `query_extras`.

&lt;details&gt;
&lt;summary&gt;
作为补充，proto2 包括三种字段规则：&lt;code&gt;required&lt;/code&gt;, &lt;code&gt;optional&lt;/code&gt; 和 &lt;code&gt;repeated&lt;/code&gt;.
&lt;/summary&gt;

- `required`. 一则 message 中必须且只能拥有一个该字段；
- `optional`. 一则 message 中只能拥有不超过一个该字段，相当于 proto3 的 singular.
- `repeated`. 一则 message 可以拥有任意个该字段，相当于 proto3 的 `repeated`.

[相关链接](&lt;(https://developers.google.com/protocol-buffers/docs/proto#specifying_field_rules)&gt;)

&lt;/details&gt;

### [保留字段](https://developers.google.com/protocol-buffers/docs/proto3#reserved)

当更新 message 定义需要完全移除一个字段时，则将来的用户在自己对该类型进行更新时可以重用该字段号。为了保证在读取旧版本的 `.proto` 时不引发问题，需要将已删除字段的字段编号（或名称）指定为 `reserved`，这样将来任何用户在更新 message 时尝试使用这些字段号（或名称）时，编译器会报错。

```proto
message Foo {
  reserved 2, 15, 9 to 11;
  reserved &quot;foo&quot;, &quot;bar&quot;;
}
```

上述内容指定了 2, 9, 10, 11, 15 为保留字段号，指定了 foo, bar 为保留字段名。在以后的编写中不应当被使用。

需要注意的是，不能在一条 `reserved` 语句中同时使用字段号和字段名。

```proto
reserved 2, 15, &quot;foo&quot;; // wrong!
```

### [使用枚举类型](https://developers.google.com/protocol-buffers/docs/proto3#enum)

当只希望某一个字段的取值为预定义值的某一个时，可以使用枚举。

```proto
message SearchRequest {
  string query = 1;
  int32 page_number = 2;
  int32 result_per_page = 3;
  enum Corpus {
    UNIVERSAL = 0;
    WEB = 1;
    IMAGES = 2;
    LOCAL = 3;
    NEWS = 4;
    PRODUCTS = 5;
    VIDEO = 6;
  }
  Corpus corpus = 4;
}
```

上例中我们定义了名为 `Corpus` 的枚举，其中包括 7 种可能的取值。接下来，我们就可以添加使用 `Corpus` 枚举的字段 `corpus`.

为了定义枚举常量的别名，我们可以将相同的值分配给不同的枚举常量名。为此，首先需要将 `allow_alias` 选项设置为 `true`，否则将会报错。

```proto
message MyMessage {
  enum EnumAllowingAlias {
    option allow_alias = true;
    UNKNOWN = 0;
    STARTED = 1;
    RUNNING = 1; // It works.
  }
  EnumAllowingAlias enum_allowing_alias = 1;
}
```

上例中，`STARTED` 和 `RUNNING` 为同一枚举常量的不同别名。

### [使用 Message 类型](https://developers.google.com/protocol-buffers/docs/proto3#other)

为了使消息结构更加清晰，我们可以指定其他 message 类型作为字段类型，实现嵌套。

```proto
message SearchResponse {
  repeated Result results = 1;
}

message Result {
  string url = 1;
  string title = 2;
  repeated string snippets = 3;
}
```

上例中定义了两种不同的 message 类型，`SearchResponse` 和 `Result`. 其中 `SearchResponse` 拥有一个 `results` 字段，其字段类型为 message 类型 `Result`.

### [使用嵌套类型](https://developers.google.com/protocol-buffers/docs/proto3#nested)

也许您不需要复用一些 message 类型，我们也可以将 message 类型放在 message 当中。

```proto
message SearchResponse {
  message Result {
    string url = 1;
    string title = 2;
    repeated string snippets = 3;
  }
  repeated Result results = 1;
}
```

上例与前一小段的例子有相同的效果。

嵌套类型不限定层数，可以根据需要进行深层嵌套。

```proto
message Outer { // Level 0
  message MiddleAA { // Level 1
    message Inner { // Level 2
      int64 ival = 1;
      bool  booly = 2;
    }
  }
  message MiddleBB { // Level 1
    message Inner { // Level 2
      int32 ival = 1;
      bool  booly = 2;
    }
  }
}
```

其中，`MiddleAA` 中的 `Inner` 与 `MiddleBB` 中的 `Inner` 虽然有相同的字段名，但存储的是不同的内容。

### 其它字段类型

- [Any](https://developers.google.com/protocol-buffers/docs/proto3#any).
- [Oneof](https://developers.google.com/protocol-buffers/docs/proto3#oneof).
- [Maps](https://developers.google.com/protocol-buffers/docs/proto3#maps).

## 定义为 RPC 服务

如果想要将 message 类型用于 RPC 系统，可以在 `.proto` 文件中定义 RPC 服务接口，编译器将根据使用的语言生成 RPC 服务接口并打桩。

```proto
service SearchService {
  rpc Search(SearchRequest) returns (SearchResponse);
}
```

上例定义了一个名为 `SearchService` 的 RPC 服务，其中包含了一个 `Search` 方法，其参数为 `SearchRequest` 类型的 message，返回值为 `SearchResponse` 类型的 message.

能够与 Protobuf 最直接对接的 RPC 系统是 gRPC，同样由 Google 公司开发的语言无关、平台无关的开源 RPC 系统。如果使用 gRPC，只需要使用一个特殊的 [gRPC 插件](https://grpc.io/docs/protoc-installation/)，就可以根据 `.proto` 文件里的内容自动生成 RPC 代码。

## 编译 .proto 文件

首先编译并配置好 Protoc，并且安装了 Go 语言插件 protoc-gen-go.

参考官网给出的例子，我分别编写了 Go 和 C++ 版本的 `.proto` 文件：

```proto
// addressbook-go.proto
syntax = &quot;proto3&quot;;
package tutorial;

import &quot;google/protobuf/timestamp.proto&quot;;

// go_package 选项定义了软件包的导入路径
// 对于 go 版本，包含 go_package 设置的内容；cpp 版本应注释掉
option go_package = &quot;github.com/protocolbuffers/protobuf/examples/go/tutorialpb&quot;;

message Person {
  string name = 1;
  int32 id = 2;
  string email = 3;

  enum PhoneType {
    MOBILE = 0;
    HOME = 1;
    WORK = 2;
  }

  message PhoneNumber {
    string number = 1;
    PhoneType type = 2;
  }

  repeated PhoneNumber phones = 4;

  google.protobuf.Timestamp last_updated = 5;
}

message AddressBook {
  repeated Person people = 1;
}
```

使用 `protoc` 命令对编写的 `.protoc` 文件进行编译。

### Go 版本编译

```bash
# 编译 Go 版本的 .protoc 文件
protoc -I=$SRC_DIR --go_out=$DST_DIR $SRC_DIR/addressbook-go.proto
```

编译 Go 版本，会在 `$DST_DIR/github.com/protocolbuffers/protobuf/examples/go/tutorialpb` 目录下生成 `addressbook-go.pb.go` 文件。

示例代码 [`list_people.go`](https://github.com/protocolbuffers/protobuf/blob/master/examples/list_people.go) 展示了如何打印出 AddressBook 中所有的 Person 信息：

```go
func writePerson(w io.Writer, p *pb.Person) {
    fmt.Fprintln(w, &quot;Person ID:&quot;, p.Id)
    fmt.Fprintln(w, &quot;  Name:&quot;, p.Name)
    if p.Email != &quot;&quot; {
        fmt.Fprintln(w, &quot;  E-mail address:&quot;, p.Email)
    }

    for _, pn := range p.Phones {
        switch pn.Type {
        case pb.Person_MOBILE:
            fmt.Fprint(w, &quot;  Mobile phone #: &quot;)
        case pb.Person_HOME:
            fmt.Fprint(w, &quot;  Home phone #: &quot;)
        case pb.Person_WORK:
            fmt.Fprint(w, &quot;  Work phone #: &quot;)
        }
        fmt.Fprintln(w, pn.Number)
    }
}

func listPeople(w io.Writer, book *pb.AddressBook) {
    for _, p := range book.People {
        writePerson(w, p)
    }
}
```

### C++ 版本编译

```bash
# 编译 C++ 版本的 .protoc 文件
protoc -I=$SRC_DIR --cpp_out=$DST_DIR $SRC_DIR/addressbook-cpp.proto
```

编译 C++ 版本，会在 `$DST_DIR` 目录下生成 `addressbook-cpp.pb.cc` 和 `addressbook-cpp.pb.h` 两个文件。

示例代码 [`list_people.cc`](https://github.com/protocolbuffers/protobuf/blob/master/examples/list_people.cc) 展示了如何打印出 AddressBook 中所有的 Person 信息：

```cpp
void ListPeople(const tutorial::AddressBook&amp; address_book) {
  for (int i = 0; i &lt; address_book.people_size(); i++) {
    const tutorial::Person&amp; person = address_book.people(i);

    cout &lt;&lt; &quot;Person ID: &quot; &lt;&lt; person.id() &lt;&lt; endl;
    cout &lt;&lt; &quot;  Name: &quot; &lt;&lt; person.name() &lt;&lt; endl;
    if (person.email() != &quot;&quot;) {
      cout &lt;&lt; &quot;  E-mail address: &quot; &lt;&lt; person.email() &lt;&lt; endl;
    }

    for (int j = 0; j &lt; person.phones_size(); j++) {
      const tutorial::Person::PhoneNumber&amp; phone_number = person.phones(j);

      switch (phone_number.type()) {
        case tutorial::Person::MOBILE:
          cout &lt;&lt; &quot;  Mobile phone #: &quot;;
          break;
        case tutorial::Person::HOME:
          cout &lt;&lt; &quot;  Home phone #: &quot;;
          break;
        case tutorial::Person::WORK:
          cout &lt;&lt; &quot;  Work phone #: &quot;;
          break;
        default:
          cout &lt;&lt; &quot;  Unknown phone #: &quot;;
          break;
      }
      cout &lt;&lt; phone_number.number() &lt;&lt; endl;
    }
    if (person.has_last_updated()) {
      cout &lt;&lt; &quot;  Updated: &quot; &lt;&lt; TimeUtil::ToString(person.last_updated()) &lt;&lt; endl;
    }
  }
}
```
</content:original-text><content:updated-at>2021-03-29T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[在 Nuxt.js 中引入高德地图并实现定位及逆地理编码]]></title><description><![CDATA[迷途知反！腾讯地图的 JS API 文档实在过于简陋，且库很久没有更新，转身投入高德地图的怀抱，享受 this moment 的美好！ 高德地图与腾讯地图定位功能区别

高德地图将定位功能和逆地理编码功能分开为两个操作，而腾讯地图将二者合并。

这意味着使用高德地图实现逆地理编码，首先需要执行定位操作，再将得到的结果传给逆地理编码插件获得最后的结果。

此外…]]></description><link>https://blog.towind.fun/posts/amap-import-vue</link><guid isPermaLink="false">amap-import-vue</guid><category><![CDATA[前端开发]]></category><pubDate>Wed, 17 Mar 2021 00:00:00 GMT</pubDate><content:original-text>
迷途知反！腾讯地图的 JS API 文档实在过于简陋，且库很久没有更新，转身投入高德地图的怀抱，享受 this moment 的美好！

## 高德地图与腾讯地图定位功能区别

高德地图将定位功能和逆地理编码功能分开为两个操作，而腾讯地图将二者合并。

这意味着使用高德地图实现逆地理编码，首先需要执行定位操作，再将得到的结果传给逆地理编码插件获得最后的结果。

此外，高德地图的逆地理编码无法解析中国以外的地理坐标，只能解析中国境内省市区等地理坐标。

## 引入高德地图 JS API 库

这里我们通过顺序同步加载的方式引入第三方库。

编辑 `nuxt.config.js` 中 `head` 项以引入 JS API 库。

```js
// nuxt.config.js
script: [
  {
    type: &apos;text/javascript&apos;,
    // 引入高德地图 JavaScript API：https://developer.amap.com/api/jsapi-v2/guide/abc/load
    // YOUR-APP-KEY 即高德位置服务应用的 key
    // AMap.Geolocation 为定位插件
    // AMap.Geocoder 为逆地理编码插件
    src:
      &apos;https://webapi.amap.com/maps?v=2.0&amp;key=YOUR-APP-KEY&amp;plugin=AMap.Geolocation,AMap.Geocoder&apos;,
  },
],
```

如果获取脚本失败，请关闭浏览器中拦截广告的插件等。

此外要注意：

- 为避免地图数据协议和前端资源不匹配导致页面运行报错，只允许在线加载 JS API，禁止进行本地转存、与其它代码混合打包等用法。——[JS API 的加载](https://developer.amap.com/api/javascript-api/guide/abc/load)
- 由于 Chrome、IOS10 等已不再支持非安全域的浏览器定位请求，为保证定位成功率和精度，请尽快升级您的站点到 HTTPS。——[AMap.Geolocation 插件](https://developer.amap.com/api/javascript-api/reference/location)

## 高德地图定位功能实现

现在已经引入了需要的库及插件 `AMap.Geolocation`，首先要构建一个浏览器定位实例 `geolocation`：

```js
// 仅进行定位，不与地图交互
const geolocation = new AMap.Geolocation({
  timeout: 10000, // 超过 10 秒后停止定位
  noIpLocate: 1, // 禁止移动端使用 IP 定位
  useNative: true, // 使用安卓定位 sdk 用来进行定位
});
```

更多的构造选项可参考 [AMap.Geolocation 插件官方文档](https://developer.amap.com/api/javascript-api/reference/location)。

对于定位方法 `getCurrentPosition(callback:function(status,result){})`，可以用 callback 的方式或事件监听的方式实现取得返回值。以 callback 的方式为例，可编写：

```js
geolocation.getCurrentPosition((status, result) =&gt; {
  if (status === &quot;complete&quot;) {
    // 定位成功
    console.log(result);
  } else {
    // 定位失败
    console.log(result.message);
  }
});
```

为了封装函数，无法在回调函数里添加 `return` 进行返回，因此选择使用 `Promise` 的方法：

```js
function getCurrentPosition() {
  return new Promise((resolve, reject) =&gt; {
    geolocation.getCurrentPosition((status, positionResult) =&gt; {
      if (status === &quot;complete&quot;) {
        // 定位成功
        resolve({
          status: 1,
          msg: &quot;获取地理位置成功&quot;,
          result: positionResult,
        });
      } else {
        // 定位失败
        reject(new Error(`获取地理位置失败：${positionResult.message}`));
      }
    });
  });
}
```

接下来只需要进行标准的 `then()` 或 `catch()` 处理就可以了：

```js
getCurrentPosition()
  .then((res) =&gt; {
    console.log(res);
  })
  .catch((err) =&gt; {
    console.log(err);
  });
```

## 高德地图逆地理编码功能实现

[高德地图逆地理编码](https://developer.amap.com/api/javascript-api/reference/lnglat-to-address)由 `AMap.Geocoder` 插件实现，我们也已经引入。

因此只需要将上一步得到的地理经纬度坐标信息作为参数传入即可：

```js
function getCurrentAddress() {
  return new Promise((resolve, reject) =&gt; {
    // 调用定位方法
    getCurrentPosition()
      .then((positionResult) =&gt; {
        // 定位成功
        // 获得经纬度坐标数组，元素顺序不可变
        const lnglat = [
          positionResult.result.position.lng,
          positionResult.result.position.lat,
        ];
        // 构造地理编码或逆地理编码功能实例
        const geocoder = new AMap.Geocoder();
        // 获得逆编码信息
        geocoder.getAddress(lnglat, (addressStatus, addressResult) =&gt; {
          if (addressStatus === &quot;complete&quot; &amp;&amp; addressResult.regeocode) {
            // 获取成功
            resolve({
              status: 1,
              msg: &quot;获取地理位置和地区信息成功&quot;,
              result: { positionResult: positionResult.result, addressResult },
            });
          } else {
            // 获取失败
            resolve({
              status: 2,
              msg: &quot;获取地理位置成功，但获取地区信息失败&quot;,
              result: { positionResult: positionResult.result },
            });
          }
        });
      })
      .catch((err) =&gt; {
        // 定位失败
        reject(err);
      });
  });
}
```

调用方式同上所述，不再赘述。可以添加对 `status` 的判断验证结果并进行相应处理。

## 完整源码

编写封装的基于 Nuxt.js + Promise 实现高德地图定位及逆地理编码的代码如下所示：

```js
// plugins/amapGeolocation.js
import Vue from &quot;vue&quot;;

const geolocation = new AMap.Geolocation({
  timeout: 10000, // 超过 10 秒后停止定位
  noIpLocate: 1, // 禁止移动端使用 IP 定位
  useNative: true, // 使用安卓定位 sdk 用来进行定位
});

/**
 * 获取当前的经纬度坐标
 * https://developer.amap.com/api/javascript-api/reference/location
 */
function getCurrentPosition() {
  return new Promise((resolve, reject) =&gt; {
    geolocation.getCurrentPosition((status, positionResult) =&gt; {
      if (status === &quot;complete&quot;) {
        // 定位成功
        resolve({
          status: 1,
          msg: &quot;获取地理位置成功&quot;,
          result: positionResult,
        });
      } else {
        // 定位失败
        reject(new Error(`获取地理位置失败：${positionResult.message}`));
      }
    });
  });
}

/**
 * 获取当前地理坐标的逆编码结果
 * https://developer.amap.com/api/javascript-api/reference/lnglat-to-address
 */
function getCurrentAddress() {
  return new Promise((resolve, reject) =&gt; {
    getCurrentPosition()
      .then((positionResult) =&gt; {
        const lnglat = [
          positionResult.result.position.lng,
          positionResult.result.position.lat,
        ];
        const geocoder = new AMap.Geocoder();
        geocoder.getAddress(lnglat, (addressStatus, addressResult) =&gt; {
          if (addressStatus === &quot;complete&quot; &amp;&amp; addressResult.regeocode) {
            // 逆编码成功
            resolve({
              status: 1,
              msg: &quot;获取地理位置和地区信息成功&quot;,
              result: { positionResult: positionResult.result, addressResult },
            });
          } else {
            // 逆编码失败
            resolve({
              status: 2,
              msg: &quot;获取地理位置成功，但获取地区信息失败&quot;,
              result: { positionResult: positionResult.result },
            });
          }
        });
      })
      .catch((err) =&gt; {
        // 定位失败
        reject(err);
      });
  });
}

const amapGeolocation = {
  install(Vue) {
    Vue.prototype.$Geolocation = {
      getCurrentPosition,
      getCurrentAddress,
    };
  },
};

Vue.use(amapGeolocation);
```

将此文件作为插件引入 Nuxt.js 后，可以在 Vue 文件中通过如下代码轻松调用：

```js
// .vue
// 仅获取地理坐标
this.$Geolocation
  .getCurrentPosition()
  .then((res) =&gt; {
    console.log(res);
  })
  .catch((error) =&gt; {
    console.log(error);
  });

// 获取地理坐标及所在地址
this.$Geolocation
  .getCurrentAddress()
  .then((res) =&gt; {
    console.log(res);
  })
  .catch((error) =&gt; {
    console.log(error);
  });
```

## 相关链接

- &lt;Link to=&quot;/posts/tencent-map-api-get-current-location&quot;&gt;
    使用腾讯位置服务进行 Web 前端定位
  &lt;/Link&gt;
</content:original-text><content:updated-at>2021-03-18T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[使用腾讯位置服务进行 Web 前端定位]]></title><description><![CDATA[正在开发的 Web 项目需要获取使用者的位置信息，而使用者主要通过移动端访问此 Web 服务。位置信息需要精确到区。在腾讯位置服务的定位解决方案里想要搜索可用的 JavaScript 库，只看到了服务端的 IP 定位和移动端的几个 SDK 包，甚异之。 终于在不起眼的地方找到了前端定位组件，适用于浏览器进行定位操作。

本文基于 Nuxt.js 实现前端定位功能。

它能做什么

组件旨在优化纯 HTM…]]></description><link>https://blog.towind.fun/posts/tencent-map-api-get-current-location</link><guid isPermaLink="false">tencent-map-api-get-current-location</guid><category><![CDATA[前端开发]]></category><pubDate>Tue, 16 Mar 2021 00:00:00 GMT</pubDate><content:original-text>
正在开发的 Web 项目需要获取使用者的位置信息，而使用者主要通过移动端访问此 Web 服务。位置信息需要精确到区。在腾讯位置服务的[定位解决方案](https://lbs.qq.com/location/#anchor)里想要搜索可用的 JavaScript 库，只看到了服务端的 IP 定位和移动端的几个 SDK 包，甚异之。

终于在不起眼的地方找到了[前端定位组件](https://lbs.qq.com/webApi/component/componentGuide/componentGeolocation)，适用于浏览器进行定位操作。

本文基于 Nuxt.js 实现前端定位功能。

## 它能做什么

组件旨在优化纯 [HTML5 Geolocation](https://w3c.github.io/geolocation-api) 定位能力弱，定位成功率不高的问题，提供简单、易用的接口帮助业务层获取用户当前的位置信息（需用户授权），以降低开发成本，提升定位精准度。

除了常规的经纬度坐标以外，它返回的结果里还包含了 `city` 和 `district` 项，非常方面。

```js
{
  &quot;module&quot;: &quot;geolocation&quot;,
  &quot;type&quot;: &quot;h5_watch&quot;,
  &quot;adcode&quot;: &quot;&quot;, //行政区 ID，六位数字, 前两位是省，中间是市，后面两位是区，比如深圳市 ID 为 440300
  &quot;nation&quot;: &quot;美国&quot;,
  &quot;province&quot;: &quot;&quot;,
  &quot;city&quot;: &quot;加利福尼亚州&quot;,
  &quot;district&quot;: &quot;洛杉矶县&quot;,
  &quot;addr&quot;: &quot;&quot;,
  &quot;lat&quot;: 34.035244, //火星坐标 (gcj02)，腾讯、Google、高德等地图通用
  &quot;lng&quot;: -118.252207,
  &quot;accuracy&quot;: 1441 //误差范围，以米为单位
}
```

可惜相关页面的最后一次更新最近可能是在 `2016-02-23`，文档撰写简陋不堪，只能在项目中慢慢试错使用。

## 引入前端定位组件库

首先需要在腾讯位置服务的控制台添加应用，获得应用的 `key`。

这里选择使用前端定位组件的**调用方式三**。

在 HTML 文件的中引入脚本，对应 Nuxt.js 应用中的 `nuxt.config.js`：

```js
// nuxt.config.js
export default {
  head: {
    script: [
      {
        type: &quot;text/javascript&quot;,
        // 引入腾讯地图前端定位组件
        // YOUR-APP-KEY 即腾讯位置服务应用的 key
        // YOUR-APP-NAME 即腾讯位置服务应用的名称
        src: &quot;https://apis.map.qq.com/tools/geolocation/min?key=YOUR-APP-KEY&amp;referer=YOUR-APP-NAME&quot;,
      },
    ],
  },
};
```

如果获取脚本失败，请关闭浏览器中拦截广告的插件等。

此外，由于用户的位置信息属于敏感信息，因此需要使用 `HTTPS` 的网页发送请求；如果在本机开发，请使用 `localhost:port` 的形式开发和访问。

## 实现 Web 前端定位

编写代码如下：

```js
const geolocation = new qq.maps.Geolocation();

geolocation.getLocation(
  (position) =&gt; {
    // 成功获取位置信息
    console.log(position);
    // 将位置信息序列化存储在 localStorage 中
    const currentLocation = {
      city: position.city,
      district: position.district,
    };
    localStorage.currentLocation = JSON.stringify(currentLocation);
  },
  (error) =&gt; {
    // 获取位置信息失败
    console.log(error);
  },
);
```

执行代码，得到结果如下（其中 `*` 为我手动打码）：

```js
{
  &quot;module&quot;: &quot;geolocation&quot;,
  &quot;type&quot;: &quot;h5&quot;,
  &quot;adcode&quot;: &quot;510117&quot;,
  &quot;nation&quot;: &quot;中国&quot;,
  &quot;province&quot;: &quot;四川省&quot;,
  &quot;city&quot;: &quot;成都市&quot;,
  &quot;district&quot;: &quot;郫都区&quot;,
  &quot;addr&quot;: &quot;***&quot;,
  &quot;lat&quot;: 30.75****,
  &quot;lng&quot;: 103.92****,
  &quot;accuracy&quot;: 113
}
```

定位成功精确到当前我所处的区和地址，满足项目开发要求。

其它相关方法和说明敬请参考[官方文档](https://lbs.qq.com/webApi/component/componentGuide/componentGeolocation)的“暗示”。
</content:original-text><content:updated-at>2021-03-16T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[提示 *** is not a symbolic link 解决方案]]></title><description><![CDATA[问题描述 在 CentOS 环境下执行  和  命令时都出现提示警告，节选内容如下所示：

错误分析

进入到对应目录下查找可以发现，这里的  与  实际上是相同的动态库文件，而非我们期望的符号链接和动态库文件。

这个错误的产生原因是， 在正常情况下应该是一个指向  文件的软链接，但却变成了一个动态库文件。

在一般情况下，这个错误并不会导致严重的问题；但假如目录下有多个不同版本的动态库文件…]]></description><link>https://blog.towind.fun/posts/xxx-is-not-a-symbolic-link</link><guid isPermaLink="false">xxx-is-not-a-symbolic-link</guid><category><![CDATA[技术琐事]]></category><pubDate>Wed, 10 Mar 2021 00:00:00 GMT</pubDate><content:original-text>
## 问题描述

在 CentOS 环境下执行 `yum update` 和 `ldconfig` 命令时都出现提示警告，节选内容如下所示：

```bash
ldconfig: /OSM/lib/librdmacm.so.1 is not a symbolic link
ldconfig: /OSM/lib/libgrpc++_reflection.so.1 is not a symbolic link
ldconfig: /OSM/lib/libupb.so.9 is not a symbolic link
```

## 错误分析

进入到对应目录下查找可以发现，这里的 `librdmacm.so.1` 与 `librdmacm.so.1.1.17.4` 实际上是相同的动态库文件，而非我们期望的符号链接和动态库文件。

```bash
[root@xxx ~]# cd /OSM/lib
[root@xxx lib]# find librdmacm.so.1* | xargs ls -l
-rwx------. 1 root root 442208 Mar  9 16:13 librdmacm.so.1
-rwx------. 1 root root 442208 Mar  9 16:13 librdmacm.so.1.1.17.4
```

这个错误的产生原因是，`librdmacm.so.1` 在正常情况下应该是一个指向 `librdmacm.so.1.1.17.4` 文件的软链接，但却变成了一个动态库文件。

在一般情况下，这个错误并不会导致严重的问题；但假如目录下有多个不同版本的动态库文件，软链接可能无法正确获取到最新版本，产生隐患。

解决这个问题只需要将 `librdmacm.so.1` 修改为正常的软链接文件，重新链接两个文件就可以了。

## 解决方案

执行 `ln -sf [动态库文件或源文件] [符号链接或目标文件]` 即可，其中 `-s` 指创建软链接，`-f` 指强制执行。例如：

```bash
[root@xxx lib]# ln -sf librdmacm.so.1.1.17.4 librdmacm.so.1
[root@xxx lib]# find librdmacm.so.1* | xargs ls -l
lrwxrwxrwx. 1 root root     21 Mar 10 16:26 librdmacm.so.1 -&gt; librdmacm.so.1.1.17.4
-rwx------. 1 root root 442208 Mar  9 16:13 librdmacm.so.1.1.17.4
```

现在正如我们预期的，`librdmacm.so.1` 正确指向了 `librdmacm.so.1.1.17.4`。

### 更好的方式

假如同时报了很多个类似的提示，应当如何处理呢？

我们知道执行 `ldconfig` 命令时，会自动为关联目录下的所有动态库文件创建对应的软链接。因此我们只需要删除掉这些重复的文件，再执行命令就可以了。

将 `ldconfig` 命令的错误输出重定向到临时文件中，读取内容，组合使用 `cut` 和 `rm` 命令即可实现删除重复文件，接下来删除存储错误信息的临时文件。最后再次执行 `ldconfig` 创建软链接。

```bash
# 标准错误输出到 dupNote 临时文件中
ldconfig 2&gt; dupNote
# 删除重复的动态库（非软链接）文件
cat dupNote | cut -c 11- | rev | cut -c 23- | rev | xargs rm -rf
# 删除刚刚创建的临时文件
rm -rf dupNote
# 创建软链接
ldconfig
```

当然可以合起来变成一条命令使用：

```bash
ldconfig 2&gt; dupNote ; cat dupNote | cut -c 11- | rev | cut -c 23- | rev | xargs rm -rf ; rm -rf dupNote ; ldconfig
```

现在执行 `ldconfig` 不再提示 `*** is not a symbolic link` 错误，问题顺利解决！

## 相关链接

- [/usr/lib/\*\*\* is not a symbolic link 问题解决 - CSDN](https://blog.csdn.net/qq_34213260/article/details/107399507)
- [Linux ln 命令 - 菜鸟教程](https://www.runoob.com/linux/linux-comm-ln.html)
</content:original-text><content:updated-at>2021-03-10T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[前端工程配置 ESLint 和 Prettier 检查并规范代码质量与格式]]></title><description><![CDATA[哪位代码人不希望自己的代码总有统一优美的风格，不会因为合作开发项目而杂乱呢？ 在最开始写项目代码的时候我就用起了 ESLint 和 Prettier，再装一堆预设的配置，便跑了起来。令人沮丧的是，用 ESLint 修复了代码质量问题，还是会在编译器里看到红色波浪线，提醒还有些代码风格需要修复。直到这一次，我才忽然意识到 ESLint 和 Prettier 其实分工了不同领域，协同使用体验极好。

本…]]></description><link>https://blog.towind.fun/posts/nodejs-eslint-prettier</link><guid isPermaLink="false">nodejs-eslint-prettier</guid><category><![CDATA[前端开发]]></category><pubDate>Wed, 03 Mar 2021 00:00:00 GMT</pubDate><content:original-text>
哪位代码人不希望自己的代码总有统一优美的风格，不会因为合作开发项目而杂乱呢？

在最开始写项目代码的时候我就用起了 ESLint 和 Prettier，再装一堆预设的配置，便跑了起来。令人沮丧的是，用 ESLint 修复了代码**质量**问题，还是会在编译器里看到红色波浪线，提醒还有些代码**风格**需要修复。直到这一次，我才忽然意识到 ESLint 和 Prettier 其实分工了不同领域，协同使用体验极好。

本文将阐述笔者如何配置 ESLint + Prettier，实现前端项目检查并修复代码质量与格式问题的能力。

## 介绍 ESLint 与 Prettier

ESLint 是一个开源的 JavaScript 代码检查工具，Prettier 是一款代码格式工具。它们的功能侧重如下所示：

- ESLint：主要负责代码**质量**的校验，其次包含部分的**风格**检验。
- Prettier：主要负责代码**风格**的校验。

ESLint 认为代码风格并没有那么重要，因此并未完全解决代码风格问题。

&gt; Rules are &quot;agenda free&quot; - ESLint does not promote any particular coding style.

而 Prettier 则认为自己是固执己见的代码格式化工具。

&gt; An opinionated code formatter.

目前公认的一个最佳实践是结合二者的强项：让 Prettier 专注处理代码格式问题，让 ESLint 专注处理代码质量问题。

## 引入 ESLint 与 Prettier

首先，自然是安装 ESLint 和 Prettier 作为项目依赖。

```bash
yarn add --dev eslint prettier
```

接着，让 ESLint 一并接管 Prettier 的工作，这可以通过以下两个库实现：

- [`eslint-plugin-prettier`](https://github.com/prettier/eslint-plugin-prettier)：ESLint 插件，包括了 ESLint 需要检查的一些额外代码格式规则。在幕后，它使用到了 Prettier，相当于将 Prettier 作为 ESLint 的一部分运行。
- [`eslint-config-prettier`](https://github.com/prettier/eslint-config-prettier)：ESLint 配置，可以关闭 ESLint 里所有不必要或者可能与 Prettier 产生冲突的代码格式规则。

在项目里安装它们：

```bash
yarn add --dev eslint-plugin-prettier eslint-config-prettier
```

将 `eslint-plugin-prettier` 的推荐配置 `plugin:prettier/recommended` 放到 ESLint 配置里 `extends` 的最后一项即可，它将自动配置并启用 `eslint-config-prettier`。如下所示：

```js
// .eslintrc.js
module.exports = {
  extends: [
    // ... 您使用的其它 ESLint 拓展
    &quot;plugin:prettier/recommended&quot;,
  ],
};
```

对于新版本的 ESLint 配置文件 `eslint.config.js`，可以像下面这样配置：

```js
const eslintPluginPrettierRecommended = require(&quot;eslint-plugin-prettier/recommended&quot;);

module.exports = [
  // ... 您使用的其它 ESLint 拓展
  eslintPluginPrettierRecommended,
];
```

参考 Prettier 的官方[配置文档](https://prettier.io/docs/options.html)，您可以自由地配置项目代码的风格。在项目目录创建 `.prettierrc.json` 文件，添加风格配置项如：

```json
{
  // 注释只是便于理解，您应当删除 JSON 文件里的注释
  &quot;semi&quot;: false, // 句末是否添加分号
  &quot;singleQuote&quot;: true // 是否使用单引号（而非双引号）
}
```

当然，笔者通常保持 Prettier 的默认风格，不单独引入配置文件。

## 现在就格式化代码吧

修改 `package.json` 文件，添加脚本：

```json
// package.json
{
  &quot;scripts&quot;: {
    &quot;lint&quot;: &quot;eslint .&quot;
  }
}
```

根据上面的配置，可以在项目根目录下执行如下脚本：

```bash
# 检查代码质量问题
yarn lint

# 检查并修复代码质量问题
yarn lint --fix
```

由于我们将 Prettier 作为了 ESLint 的插件运行，所以无需手动执行 Prettier 的格式化命令了。

在 VSCode 里，也应当将 ESLint 作为默认的格式化工具。如果正确地安装并启用了 VSCode 的 ESLint 拓展，编辑器就能正确的高亮 ESLint 与 Prettier 检查出来的问题了。

此外，VSCode 还可以设置**保存时自动修复代码问题**，如下所示。这样执行 `Ctrl + S` 保存时会自动格式化代码文件。

```json
// settings.json
&quot;editor.codeActionsOnSave&quot;: {
  &quot;source.fixAll.eslint&quot;: true
},
```

## 参考资料

- [What&apos;s the difference between prettier-eslint, eslint-plugin-prettier and eslint-config-prettier?](https://stackoverflow.com/questions/44690308/whats-the-difference-between-prettier-eslint-eslint-plugin-prettier-and-eslint) - stackoverflow
- [Error: &apos;basePath&apos; should be an absolute path](https://github.com/prettier/prettier-eslint-cli/issues/208#issuecomment-673631308) - mathiaswillburger - 2020.08.14
- [搞懂 ESLint 和 Prettier](https://zhuanlan.zhihu.com/p/80574300) - 乃乎 - 2019.08.31
- [ESLint+Prettier 代码规范实践](https://www.jianshu.com/p/dd07cca0a48e) - Bernie 维 - 2019.06.04
</content:original-text><content:updated-at>2025-04-11T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[Webpack 读取本地 Markdown 文件并进行预处理]]></title><description><![CDATA[在开发 NetUnion 的官网页面时，有这样一个需求：读取本地目录下的新闻和博客文件，并在前端渲染，其中文件均为 Markdown 格式。 与全栈开发直接调用后端数据库不同的是，没有数据表字段来记录文件的不同属性，例如文件的题目、作者、撰写日期等，因此这些属性需要记录在 .md 文件当中。

这样的撰写方式是不是很熟悉？没错，不就是我正在写的 Hexo 博客中 .md 文件的编写格式嘛！

自动导入…]]></description><link>https://blog.towind.fun/posts/write-md-parser</link><guid isPermaLink="false">write-md-parser</guid><category><![CDATA[前端开发]]></category><pubDate>Tue, 23 Feb 2021 00:00:00 GMT</pubDate><content:original-text>
在开发 NetUnion 的官网页面时，有这样一个需求：读取本地目录下的新闻和博客文件，并在前端渲染，其中文件均为 Markdown 格式。

与全栈开发直接调用后端数据库不同的是，没有数据表字段来记录文件的不同属性，例如文件的题目、作者、撰写日期等，因此这些属性需要记录在 .md 文件当中。

这样的撰写方式是不是很熟悉？没错，不就是我正在写的 Hexo 博客中 .md 文件的编写格式嘛！

## 自动导入本地的 .md 文件

当然，首先要读取某个目录下已经撰写好的 .md 文件，才能对内容进行预处理。

但如果每撰写好一个新的新闻或博客文件，就得在代码中 `require` 出来，太过于麻烦且不现实，因此就需要**自动导入**的方法。

Webpack 提供了 `require.context()` 方法可以完美解决导入目录下所有文件的问题，该方法可以导入指定目录（也可以包括子目录）下指定格式的所有文件。关于此方法的更多细节可以在 Webpack 官方文档中[了解](https://webpack.js.org/guides/dependency-management/#requirecontext)。

撰写代码自动读取 `@/docs/blog/` 及其子目录下的所有 .md 文件如下所示，其中 `blogFiles(key)` 为文件存储的具体内容：

```js
const blogFiles = require.context(&quot;@/docs/blog/&quot;, true, /\.md$/);
blogFiles.keys().forEach((key) =&gt; {
  console.log(blogFiles(key));
});
```

## 对 .md 文件进行预处理

参考 Hexo 博客的撰写格式，可以规定 NetUnion 官网的新闻和博客撰写格式如下：

```md
---
title: ${title}
date: ${date}
author: ${author}
---

${main-text}
```

即用两个 `---` 框住**属性内容**，在第二个 `---` 下面为**正文内容**。

那么首先，可以用 `split()` 方法根据 `---` 及换行符将文章划分为长度不少于 3 （因为在正文中可能出现 `---`）的数组 arr。其中 arr[0] 为空，arr[1] 存储有属性内容，arr[2] 及之后存储正文内容。

```js
// content 为传入的 .md 文件内容
const contentArray = content.split(/---+\r?\n/g);
```

对属性内容的处理同样可以先使用 `split()` 方法按换行符拆分为数组。

```js
const contentInfo = contentArray[1];
const contentInfoArray = contentInfo.split(/\r?\n/g);
```

值得一提的是，在上面两次按换行符分割时，我都使用了 `/\r?\n/g` 正则表达式。其含义是匹配 0 个或 1 个 `\r` 及 1 个 `\n`，直到结束。因为在 `CRLF` 行尾序列的文件中，换行符由 `\r\n` 表示；而在 `LF` 行尾序列中，换行符由 `\n` 表示。这样就确保了在 Windows 和 Unix 两种不同的系统上撰写的文件，其解析不会受行尾序列所影响。

接下来就可以提取属性对象了。这里使用 `trim()` 方法来删除属性名和属性值前后可能出现的多余空格。

```js
const contentInfoItem = {};
for (let i = 0; i &lt; contentInfoArray.length - 1; i++) {
  const contentInfoParamArray = contentInfoArray[i].split(&quot;:&quot;);
  let contentInfoParamValue = &quot;&quot;;
  for (let n = 1; n &lt; contentInfoParamArray.length; n++) {
    contentInfoParamValue += contentInfoParamArray[n] + &quot;:&quot;;
  }
  contentInfoItem[contentInfoParamArray[0].trim()] = contentInfoParamValue
    .slice(0, -1)
    .trim();
}
```

对正文内容的处理就相当简单了，只需要把 arr[2] 及之后存储的内容用 `---\n` 连接起来就可以了。

```js
let contentText = contentArray[2];
if (contentArray.length &gt; 3) {
  for (let i = 3; i &lt; contentArray.length; i++) {
    contentText += &quot;---\n&quot;;
    contentText += contentArray[i];
  }
}
```

将属性对象与正文内容合并为一个新的对象，解析就完成了！

```js
const result = {
  ...contentInfoItem,
  content: contentText,
};
```

如果愿意，还可以在最后对格式进行一定规范，例如可以对 date 属性进行处理：

```js
// 格式为 YYYY-MM-DD
if (result.date != null) {
  const dateArray = result.date.split(&quot;-&quot;);
  const dateYear = dateArray[0];
  let dateMonth = dateArray[1];
  let dateDay = dateArray[2];
  if (dateMonth.length == 1) {
    dateMonth = &quot;0&quot; + dateMonth;
  }
  if (dateDay.length == 1) {
    dateDay = &quot;0&quot; + dateDay;
  }
  result.date = dateYear + &quot;-&quot; + dateMonth + &quot;-&quot; + dateDay;
}
```

## 解析 .md 为 HTML

将结果中的正文内容交给给任意 .md 解析器就可以了，例如 [markdown-it](https://github.com/markdown-it/markdown-it)。

```js
const md = require(&quot;markdown-it&quot;)({
  linkify: false, // 一些设置，并不重要，下同
  breaks: false,
  typographer: true,
});
const htmlContent = md.render(result.content);
```

完整的解析文件[在这里](https://github.com/uestclug/nu-official/blob/frontend/src/utils/mdParser.js)。
</content:original-text><content:updated-at>2021-06-26T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[一键安装并配置 MTProto Proxy 代理 Telegram]]></title><description><![CDATA[Telegram 和 MTProto 是什么 Telegram，中文名称为“电报”，或简称“tg”，是一款跨平台即时通讯软件，客户端完全开源。我觉得重要的有如下 Features：

私密性高。不再使用的账号最长保留年限为 1 年，随时销毁聊天记录和一切账号资料。 安全性。端对端加密，不受审查。 完全免费。不限制上传文件（视频会有压缩），甚至可以拿来当备用网盘使用。不过已经确认会在将来布局群组广告…]]></description><link>https://blog.towind.fun/posts/tg-mtproto-one-click</link><guid isPermaLink="false">tg-mtproto-one-click</guid><category><![CDATA[技术琐事]]></category><pubDate>Sun, 21 Feb 2021 00:00:00 GMT</pubDate><content:original-text>
## Telegram 和 MTProto 是什么

Telegram，中文名称为“电报”，或简称“tg”，是一款跨平台即时通讯软件，客户端完全开源。我觉得重要的有如下 Features：

- 私密性高。不再使用的账号最长保留年限为 1 年，随时销毁聊天记录和一切账号资料。
- 安全性。端对端加密，不受审查。
- 完全免费。不限制上传文件（视频会有压缩），甚至可以拿来当备用网盘使用。不过已经确认会在将来布局群组广告，“钞能力”还是抵不过越来越多涌入的各国用户。

当然，在被屏蔽的地域需要翻墙使用。

MTProto 是一种协议，旨在帮助移动设备上的应用程序访问服务器的 Api 接口。

## 我们要做什么

我们想要搭建一个 Proxy，提供给移动端使用，可以快捷地访问到 tg 服务器。

移动端与电脑端不同的是，电脑端通常代理软件常开，随时可以通过 web 访问到 tg 服务器，而移动端则为了省电需要，仅在使用时才手动启用代理，稍显麻烦。

实际上电脑端的 tg 客户端也需要手动设置代理全局才能访问，那么有一个 Proxy 可以省很多事儿。

## 嗨，这里是一键脚本

在 Github 上有为实现以上功能编写的基于 Python 的 Proxy: [mtprotoproxy](https://github.com/alexbers/mtprotoproxy)。号称：

&gt; The proxy performance should be enough to comfortably serve about 4000 simultaneous users on the VDS instance with 1 CPU core and 1024MB RAM.
&gt; 代理服务器的性能应该足以在使用 1 个 CPU 核心和 1024 MB 内存的 VDS 实例上同时为大约 4000 个用户提供舒适的服务。

当然也有用其它语言写的 Proxy，不过最妙的还属一键安装配置脚本：[MTProtoProxyInstaller](https://github.com/HirbodBehnam/MTProtoProxyInstaller)。不用脑子！

那么，按照作者给出的指引走就可以了。

### 购买一台自由访问 tg 的服务器

阿里云和腾讯云提供的轻量级应用服务器就很不错！选择香港地区即可，每月有 1 TB 的流量。

需要注意的是，此一键脚本仅支持如下 Linux 系统：

- Centos 7/8
- Ubuntu 16 或更新版本
- Debian 8/9

### 选择代理版本

首先明确自己要使用哪个版本的代理，官方的，基于 Python 语言的，亦或是基于 Go 语言的。

作者建议在如下情况使用基于 Python 语言的代理：

- 服务器 CPU 只有一个内核，或者只想在一个内核上运行代理。
- 服务器比较低端。
- 为一小群人提供服务，比如家庭或小公司。
- 想要限制用户的连接。
- 还将在服务器上运行其它应用程序或服务，例如 OpenVPN，Shadowsocks，Nginx 等。

否则使用官方代理。

这里我选择使用**基于 Python 语言**的代理，故下文将基于此版本阐述，若想使用其它版本可以自行查阅作者的[教程文档](https://github.com/HirbodBehnam/MTProtoProxyInstaller#official-script)。

### 部署一键脚本

执行如下命令进行安装：

```bash
curl -o MTProtoProxyInstall.sh -L https://git.io/fjo34 &amp;&amp; bash MTProtoProxyInstall.sh
```

根据提示进行配置，觉得不妥还可以重新执行上述命令重装代理服务。

假如只添加了一个代理服务器，在安装完成后会出现如下文本：

```text
Ok it must be done. I created a service to run or stop the proxy.
Use &quot;systemctl start mtprotoproxy&quot; or &quot;systemctl stop mtprotoproxy&quot; to start or stop it

Use these links to connect to your proxy:
${username}: tg://proxy?server=${ip}&amp;port=${port}&amp;secret=${secret}
```

其中 \$\{username\} 为之前输入的用户名（并不重要，只是标识），\$\{ip\} 是服务器的公网 IP 或域名，\$\{port\} 是设置的访问代理的端口，\$\{secret\} 是设置或自动生成的密钥。

这里作者提示可以用 `systemctl start mtprotoproxy` 来启动代理服务了，作为补充，`systemctl` 是 `Systemd` 进程管理命令，而 `Systemd` 是一种 Linux 的系统工具，用来启动守护进程（即一直在后台运行的进程，daemon）。这说明当前 `mtprotoproxy` 已经是可以启动的服务了，在后续的过程中只需要对此服务进行管理就可以了。

那么接下来就执行该命令来启动服务：

```bash
systemctl start mtprotoproxy # 启动代理服务
```

复制之前的链接 `tg://proxy?server=${ip}&amp;port=${port}&amp;secret=${secret}` 到剪切板，在手机端的 Telegram 上通过 `设置 - 数据和存储 - 代理设置 - 添加代理 - 从剪贴板导入` 即可完成设置。建议把这个链接记下来并保存。

### Ops，一点小麻烦

呃，似乎连接了半天也 ping 不通，这可如何是好，哪一步做错了吗？

可以先查阅一下日志信息，执行如下命令：

```bash
systemctl status mtprotoproxy -l # 查看代理服务日志信息
```

查询到如下结果：

```text
● mtprotoproxy.service - MTProto Proxy Service
     Loaded: loaded (/etc/systemd/system/mtprotoproxy.service; enabled; vendor preset: enabled)
     Active: active (running) since Sun 2021-02-21 15:12:39 CST; 43s ago
   Main PID: 39895 (python3.8)
      Tasks: 5 (limit: 1111)
     Memory: 23.7M
     CGroup: /system.slice/mtprotoproxy.service
             └─39895 /usr/bin/python3.8 /opt/mtprotoproxy/mtprotoproxy.py

Feb 21 15:13:18 ******** python3.8[39895]: Unable to connect to ****:***:****:****::a *
Feb 21 15:13:18 ******** python3.8[39895]: Unable to connect to ****:***:****:****::a *
Feb 21 15:13:19 ******** python3.8[39895]: Unable to connect to ****:***:****:****::a *
Feb 21 15:13:19 ******** python3.8[39895]: Unable to connect to ****:***:****:****::a *
Feb 21 15:13:19 ******** python3.8[39895]: Unable to connect to ****:***:****:****::a *
Feb 21 15:13:19 ******** python3.8[39895]: Unable to connect to ****:***:****:****::a *
Feb 21 15:13:20 ******** python3.8[39895]: Unable to connect to ****:***:****:****::a *
Feb 21 15:13:20 ******** python3.8[39895]: Unable to connect to ****:***:****:****::a *
Feb 21 15:13:23 ******** python3.8[39895]: Unable to connect to ****:***:****:****::a *
Feb 21 15:13:23 ******** python3.8[39895]: Unable to connect to ****:***:****:****::a *
```

幸运的是，在该仓库下的 Issues 里我找到了遇见相同问题的人（[#34](https://github.com/HirbodBehnam/MTProtoProxyInstaller/issues/34)）。

原来在最新版本的 Python 代理软件中，默认使用 IPv6，进而导致 MTProto 无法正常连接。作者提出可以在设置文件中手动设置禁用优先 IPv6 连接。操作如下：

```bash
cd /opt/mtprotoproxy # 默认安装目录
vi config.py # 编辑配置文件
```

在文件中添加一行配置信息 `PREFER_IPV6 = False` 即可。

由于服务没有热重载机制，因此在最后需要重启服务：

```bash
systemctl restart mtprotoproxy # 重启代理服务
```

### 一切搞定

现在已经可以正常访问 tg 客户端啦！假如愿意，可以将刚刚得到的链接分享给好友，enjoy tg world!

或许你会愿意再次打印一下日志信息，结果如下所示：

```text
● mtprotoproxy.service - MTProto Proxy Service
     Loaded: loaded (/etc/systemd/system/mtprotoproxy.service; enabled; vendor preset: enabled)
     Active: active (running) since Sun 2021-02-21 15:17:25 CST; 4s ago
   Main PID: 39940 (python3.8)
      Tasks: 5 (limit: 1111)
     Memory: 17.8M
     CGroup: /system.slice/mtprotoproxy.service
             └─39940 /usr/bin/python3.8 /opt/mtprotoproxy/mtprotoproxy.py

Feb 21 15:17:25 ******** systemd[1]: Started MTProto Proxy Service.
Feb 21 15:17:25 ******** python3.8[39940]: *: tg://proxy?server=*&amp;port=*&amp;secret=*&gt;
Feb 21 15:17:25 ******** python3.8[39940]: *: tg://proxy?server=*&amp;port=*&amp;secret=*&gt;
Feb 21 15:17:25 ******** python3.8[39940]: Found uvloop, using it for optimal performance
Feb 21 15:17:25 ******** python3.8[39940]: Got cert from the MASK_HOST www.cloudflare.com, its length is 1828
```

除了前文已出现过的命令，别忘了此代理服务还可以暂停，依旧是作为服务进行管理就可以了：

```bash
systemctl stop mtprotoproxy # 暂停代理服务
```
</content:original-text><content:updated-at>2021-02-23T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[使用 Github Actions 持续集成与部署 Hexo 博客]]></title><description><![CDATA[这是我撰写的第一篇与 Github Actions 有关的博客，那么就首先对 Github Actions 做一个简短的介绍吧。 Github Actions 是 Github 于 2018 年 10 月推出的持续集成服务（CI）。

大家知道，持续集成由很多操作组成，比如抓取代码、运行测试、登录远程服务器，发布到第三方服务等等。GitHub 把这些操作就称为 actions…]]></description><link>https://blog.towind.fun/posts/hexo-github-actions-ci-cd</link><guid isPermaLink="false">hexo-github-actions-ci-cd</guid><category><![CDATA[技术琐事]]></category><pubDate>Fri, 19 Feb 2021 00:00:00 GMT</pubDate><content:original-text>
这是我撰写的第一篇与 Github Actions 有关的博客，那么就首先对 Github Actions 做一个简短的介绍吧。

Github Actions 是 Github 于 2018 年 10 月推出的持续集成服务（CI）。

&gt; 大家知道，持续集成由很多操作组成，比如抓取代码、运行测试、登录远程服务器，发布到第三方服务等等。GitHub 把这些操作就称为 actions。
&gt; 很多操作在不同项目里面是类似的，完全可以共享。GitHub 注意到了这一点，想出了一个很妙的点子，允许开发者把每个操作写成独立的脚本文件，存放到代码仓库，使得其他开发者可以引用。
&gt; 如果你需要某个 action，不必自己写复杂的脚本，直接引用他人写好的 action 即可，整个持续集成过程，就变成了一个 actions 的组合。这就是 GitHub Actions 最特别的地方。
&gt; —— [GitHub Actions 入门教程](http://www.ruanyifeng.com/blog/2019/09/getting-started-with-github-actions.html)

不过在 Github Actions 的发展的过程中，它早已不局限于 CI 等功能，还可以用于各种自动化操作，例如[百度贴吧自动签到](https://github.com/srcrs/TiebaSignIn)（注：已失效。Github 官方会对此类利用服务器实现签到功能的仓库进行封禁打击，还是不要使用了吧）等。

## 持续集成与部署 Hexo 博客

在&lt;Link to=&quot;/posts/hello-hexo-world&quot;&gt;搭建自己的 Hexo 博客&lt;/Link&gt;那篇文章的最后，我们使用的是 [hexo-deployer-git 一键部署到仓库](https://hexo.io/zh-cn/docs/github-pages#%E7%A7%81%E6%9C%89-Repository)的方式，实现手动构建个人博客网页并通过脚本推送部署到自己的 Github Pages.

事实上，利用 Github Actions 就再也不用多此一举：每次提交代码到 Github 后，就可以触发 Github Actions 并自动部署新的博客内容。

### 文档是您最有用的帮手

更确切的说，英文文档是您最有用的帮手！

在此处记一个小插曲，在本博客初次撰写的时候，中文的 Hexo 文档页面仍在使用 Travis CI 实现自动化部署，而英文的 Hexo 文档已经更新到推荐使用 Github Actions 实现自动化部署操作了。

使用 Travis CI 对免费用户有 10000 分钟执行时间的限额，为了以后不再迁移，还是使用 Github Actions 吧！

接下来的内容主要参考了[英文文档](https://hexo.io/docs/github-pages)，在此基础上加上了自己的一些操作与理解。

假设您已经创建了一个 **username.github.io** 仓库，其中 username 是您在 Github 上的用户名。

### 创建存放 Hexo 源的分支

众所周知，Hexo 首先通过 `Hexo generate` 方法构建了博客所有的 HTML, JS 和 CSS 文件，只需要将这些文件上传到 **username.github.io** 仓库，并在仓库设置中修改 `GitHub Pages` 项的相应内容，就可以通过 `username.github.io` 访问到您的博客了。

因此我们可以单独将构建前的所有文件放置在 **username.github.io** 仓库中的一个分支上，每次更新此分支后，自动通过 Github Actions 将构建出的所有文件推送到展示的分支上去。

这里，假设您展示的博客文件存放在 `master` 分支，而 Hexo 源文件存放在新建的 `source` 分支。

### 编写 Github Actions

克隆此仓库到本地，切换到 `source` 分支，在根目录下新建文件夹 `.github/workflows`，在此目录下新建文件如 `main.yml`. 名字并不重要。

您也可以在 Github 的仓库页面上点击 `Actions` 并创建新的工作流 `main.yml`。

编写工作流文件 `main.yml` 如下所示：

```yml
name: Hexo Blog CI &amp; CD

on:
  push:
    branches:
      - source # 存放 Hexo 源文件的分支

jobs:
  pages:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Use Node.js 12.x
        uses: actions/setup-node@v1
        with:
          node-version: &quot;12.x&quot;
      - name: Cache NPM dependencies
        uses: actions/cache@v2
        with:
          path: node_modules
          key: ${{ runner.OS }}-npm-cache
          restore-keys: |
            ${{ runner.OS }}-npm-cache
      - name: Install Dependencies
        run: npm install
      - name: Build
        run: npm run build
      - name: Deploy
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }} # 无需修改
          publish_dir: ./public # hexo generate 生成的博客文件默认存放在 /public 目录下
          publish_branch: master # 存放展示的博客文件的分支
```

工作流采用了别人编写好的 [actions-gh-pages@v3](https://github.com/peaceiris/actions-gh-pages)，其中 `GITHUB_TOKEN` 为 Github Actions 在运行中自动生成的，用于验证身份的 Token，无需修改。关于 `GITHUB_TOKEN` 的更多介绍，可以查看[此文档](https://docs.github.com/en/actions/reference/authentication-in-a-workflow)。

提交修改或保存此工作流文件，很快 Github Actions 就会开始自动执行，并将最新的博客文件推送到仓库的 `master` 分支。

最后，等到 Github Pages 也更新完毕后，就可以访问您的博客啦！

### 假如您采用了账户两重验证

Ops, 也许您的邮箱收到了一份新的邮件，遗憾地通知您 Github Actions 执行失败。这时您可以想一想自己是否启用了 Github 账号的双重验证或其它安全访问验证。这都可能导致自动部署失败。

但是别担心，您可以通过[添加 SSH 身份验证](https://github.com/peaceiris/actions-gh-pages#%EF%B8%8F-create-ssh-deploy-key)来解决这个问题。

首先，使用 `ssh-keygen -t rsa -C &quot;YOUR USERNAME&quot;` 命令创建一个新的 SSH key 公钥密钥对。

然后在 Github 上的 **Account settings** 中的 **SSH and GPG keys** 设置中保存带有 `.pub` 后缀的公钥，并在当前项目仓库的 `secrets` 中存放不带任何后缀的密钥。

最后修改刚刚的 `main.yml` 文件，添加 `deploy_key` 设置，如下所示：

```yml
name: Hexo Blog CI &amp; CD

on:
  push:
    branches:
      - source

jobs:
  pages:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Use Node.js 12.x
        uses: actions/setup-node@v1
        with:
          node-version: &quot;12.x&quot;
      - name: Cache NPM dependencies
        uses: actions/cache@v2
        with:
          path: node_modules
          key: ${{ runner.OS }}-npm-cache
          restore-keys: |
            ${{ runner.OS }}-npm-cache
      - name: Install Dependencies
        run: npm install
      - name: Build
        run: npm run build
      - name: Deploy
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          deploy_key: ${{ secrets.ACCESS_TOKEN }} # 添加 ACCESS_TOKEN
          publish_dir: ./public
          publish_branch: master
```

其中 `ACCESS_TOKEN` 为新建的 `secret` 的名字，您应当修改为刚刚您创建 `secret` 时指定的名字。

当然，您也可以生成 Github personal access token，本文不再赘述。

最后，提交您的修改，一切都工作得如此完美。
</content:original-text><content:updated-at>2021-02-19T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[IEEE 1471（ISO/IEC/IEEE 42010）架构描述方法]]></title><description><![CDATA[关于 本文对软件体系架构的描述方法的研究基于 ISO/IEC/IEEE 42010. ISO/IEC/IEEE 42010 于 2011 年批准使用并发布，该标准是继 2006 年 ISO 快速采用 IEEE 标准后，ISO 和 IEEE 联合制定的修订 IEEE Std 1471:2000 的产物。

本文绝大多数内容通过 DeepL 翻译 ISO/IEC/IEEE 42010 原文得来。

本文系…]]></description><link>https://blog.towind.fun/posts/ISO-IEC-IEEE-42010-des</link><guid isPermaLink="false">ISO-IEC-IEEE-42010-des</guid><category><![CDATA[技术琐事]]></category><pubDate>Tue, 29 Dec 2020 00:00:00 GMT</pubDate><content:original-text>
## 关于

本文对软件体系架构的描述方法的研究基于 ISO/IEC/IEEE 42010. ISO/IEC/IEEE 42010 于 2011 年批准使用并发布，该标准是继 2006 年 ISO 快速采用 IEEE 标准后，ISO 和 IEEE 联合制定的修订 IEEE Std 1471:2000 的产物。

本文绝大多数内容通过 DeepL 翻译 ISO/IEC/IEEE 42010 原文得来。

本文系软件体系架构与设计模式课程中一项作业。

访问[我的 CSDN 博客](https://blog.csdn.net/qq_43374102/article/details/111935815)以查看此文章。
</content:original-text><content:updated-at>2021-02-19T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[Windows 远程桌面连接指南]]></title><description><![CDATA[环境 本文以电子科技大学（沙河校区）的校园网为例。家用场合可能需要拨打网络运营商开公网 IP，或通过 FRP 等技术实现。

演示的系统如下：

接受连接的主机： Windows 10 专业版 进行连接的主机： Windows 10 任意版本
配置远程桌面
配置接受连接的主机

首先需要目的主机打开允许远程协助的选项。进入高级系统设置-远程，勾选即可。

您可以选择允许进行远程登录的账号…]]></description><link>https://blog.towind.fun/posts/connect-remote-desktop</link><guid isPermaLink="false">connect-remote-desktop</guid><category><![CDATA[技术琐事]]></category><pubDate>Mon, 07 Dec 2020 00:00:00 GMT</pubDate><content:original-text>
## 环境

本文以电子科技大学（沙河校区）的校园网为例。家用场合可能需要拨打网络运营商开公网 IP，或通过 FRP 等技术实现。

演示的系统如下：

- **接受连接的主机：** Windows 10 专业版
- **进行连接的主机：** Windows 10 任意版本

## 配置远程桌面

### 配置接受连接的主机

首先需要目的主机打开允许远程协助的选项。进入高级系统设置-远程，勾选即可。

![搜索高级系统设置](./connect-remote-desktop/搜索高级系统设置.png)

![启用远程协助功能](./connect-remote-desktop/启用远程协助.png)

您可以选择允许进行远程登录的账号，通常情况下您只需要连接自己的微软账号就可以了。

![添加远程桌面可访问用户](./connect-remote-desktop/添加可访问用户.png)

此时，如果您的两台主机都在一个局域网下（如寝室里的校园网），则可以通过目的主机的计算机名称进行远程桌面控制了。

![搜索远程桌面连接](./connect-remote-desktop/搜索远程桌面连接.png)

![连接远程桌面：局域网](./connect-remote-desktop/连接远程桌面：局域网.png)

您可以在设置-关于中找到您的设备名称。当然，也可以通过设备的内网 IP 地址连接。

![查看目的主机设备名称](./connect-remote-desktop/查看设备名称.png)

### 获取接受连接的主机公网访问地址

但是，我们的目的是为了能在任何连接有互联网的位置都可以访问到目的主机，该怎么做呢？

首先，我们要拿到目的主机所连接路由器的公网 IP 地址，在地址栏键入 `192.168.1.1` 进入路由器管理页面查询。您也可以通过百度搜索 `ip` 获取公网 IP 地址。

以 TP-LINK 为例，如下图 `IP 地址` 处即为公网 IP 地址 `210.41.103.31`。

![查询公网 IP 地址](./connect-remote-desktop/查询公网%20IP.png)

一般家庭用户默认没有开放公网 IP 地址，您可以致电网络运营商，申请开放公网 IP 地址，几分钟就可以搞定。

利用路由器的虚拟服务器功能，将内网的 IP 地址上的某端口通过端口映射提供给公网，使得公网能够访问到目的主机的远程桌面服务。

在此之前，需要获取目的主机的内网 IP 地址。同样可以通过命令行中的 `ipconfig` 命令获取。

以 TP-LINK 为例，如下图 `IP` 处的 `192.168.1.105` 即为目的主机的内网 IP 地址。

![查询目的主机内网 IP 地址](./connect-remote-desktop/查询内网%20IP%20地址.png)

一般来说，路由器会按照互联网连接的顺序分配内网 IP 地址，因此端口映射时可能将错误的主机设备映射到公网。因此我们可以利用路由器的 `IP 与 MAC 绑定` 功能，将目的主机 MAC 地址和任意内网 IP 地址绑定，确保该 IP 地址对应目的主机。

以 TP-LINK 为例，如下图将内网的 `IP` 地址 `192.168.1.105` 与 `MAC` 地址为 `F6-A2-6C-58-B5-A4` 的主机绑定。

![绑定目的主机 MAC 地址和内网 IP 地址](./connect-remote-desktop/绑定%20IP%20与%20MAC%20地址.png)

记得要在目的主机连接的网络的网络设置中关闭随机硬件地址选项！

![关闭随机硬件地址](./connect-remote-desktop/关闭随机硬件地址.png)

最后一步，通过路由器的虚拟服务器功能，将目的主机的 3389 端口映射为公网端口。3389 端口为默认的远程桌面服务端口，假如您有更改过端口号，请修改为对应的映射端口。**注意，由于电子科技大学信息中心的安全考虑，默认关掉了校园网的 3389 外部端口，您需要任意设置一个其它外部端口，如 3390。**

以 TP-LINK 为例，如下图将内网的 `IP` 地址为 `192.168.1.105` 的主机的 `内部端口` 3389 与 `外部端口` 3390 绑定。

![虚拟服务器映射端口](./connect-remote-desktop/映射端口.png)

在重启路由器时，分配到的公网 IP 可能发生改变，此时需要通过新的公网 IP 连接到目的主机。电子科技大学采用哈希表的方式分配寝室网的公网 IP，因此一般情况下不会发生改变。

### 远程连接到主机

现在您可以使用任意连接互联网的设备通过远程桌面服务访问目的主机！如下图搜索 `210.41.103.31:3390` 所对应的远程桌面服务。

![连接远程桌面](./connect-remote-desktop/连接远程桌面.png)
</content:original-text><content:updated-at>2024-05-10T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[PyTorch 在 Windows 10 系统下的环境配置及安装]]></title><description><![CDATA[关于 PyTorch 是一个开源的 Python 机器学习库，基于 Torch，用于自然语言处理等应用程序。

本文基于 Windows 10 系统实现 PyTorch 的安装与配置，并且演示了如何安装启用 NVIDIA 显卡的 GPU 加速。

访问我的 CSDN 博客以查看此文章。]]></description><link>https://blog.towind.fun/posts/pytorch-install-windows-10</link><guid isPermaLink="false">pytorch-install-windows-10</guid><category><![CDATA[技术琐事]]></category><pubDate>Mon, 28 Sep 2020 00:00:00 GMT</pubDate><content:original-text>
## 关于

PyTorch 是一个开源的 Python 机器学习库，基于 Torch，用于自然语言处理等应用程序。

本文基于 Windows 10 系统实现 PyTorch 的安装与配置，并且演示了如何安装启用 NVIDIA 显卡的 GPU 加速。

访问[我的 CSDN 博客](https://blog.csdn.net/qq_43374102/article/details/108857215)以查看此文章。
</content:original-text><content:updated-at>2021-02-19T00:00:00.000Z</content:updated-at></item><item><title><![CDATA[Hello Hexo World]]></title><description><![CDATA[搭建一个自己的博客是多少投身于 IT 行业的男人女人们的梦想！撇开维护所花费的巨量时间开销不看，能够在网络上划得一片净土去传递自己的故事与思考，是一件何等快乐的事情！ 正如许多人的第一篇博客那样，在这里记录下搭建博客的流程，也许能带给你些许决意和帮助。

开始前

假设你已了解何为 Github Pages，并充分认识到它对于一个渴望搭建博客的中国人的难以替代性（是的，我不愿意备案）。在开始之前…]]></description><link>https://blog.towind.fun/posts/hello-hexo-world</link><guid isPermaLink="false">hello-hexo-world</guid><category><![CDATA[技术琐事]]></category><pubDate>Fri, 27 Dec 2019 00:00:00 GMT</pubDate><content:original-text>
搭建一个自己的博客是多少投身于 IT 行业的男人女人们的梦想！撇开维护所花费的巨量时间开销不看，能够在网络上划得一片净土去传递自己的故事与思考，是一件何等快乐的事情！

正如许多人的第一篇博客那样，在这里记录下搭建博客的流程，也许能带给你些许决意和帮助。

## 开始前

假设你已了解何为 Github Pages，并充分认识到它对于一个渴望搭建博客的中国人的难以替代性（是的，我不愿意备案）。在开始之前，我们首先要选择博客框架，并下载与之对应的依赖软件。

### Hello, Hexo

Hexo 基于 Node.js，是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。

类似的生成静态网页的框架还有 Hugo、Jekyll、Ghost 等，各有所长。权衡利弊，我最终选择了简单且高效的 Hexo 框架。

访问 [Hexo 官网](https://hexo.io/zh-cn)总是开始的不二之选。

### 依赖程序

安装 Hexo 十分简单，只需要先安装下列应用程序：

- [Node.js](https://nodejs.org/en/)（版本不低于 10.13.0）
- [Git](https://git-scm.com/)

通常选择最新版本即可。

### 安装 Hexo

安装完毕依赖程序后，打开 Git bash，使用 npm 命令一键安装 Hexo 5.x 版本以及所需依赖。

```bash
npm install -g hexo-cli
```

## 搭建博客

### 初始化

首先在 Git bash 的工作目录新建存放 Hexo 文件的文件夹，进入该文件夹并初始化。

```bash
hexo init [文件夹名]
cd [文件夹名]
npm install
```

### 修改配置文件

在 Hexo 目录下的 `_config.yml` 修改大部分的配置，包括网站标题、副标题、您的名字、网站语言和网站时区等等。

可以参见 [Hexo 配置官方文档](https://hexo.io/zh-cn/docs/configuration)，按照自己的需求进行更改。

### 部署到 Github Pages

登录你的 Github，新建一个 Repository，命名为 **你的 Github 用户名.github.io**。

现在你可以随时通过浏览器访问 `https://你的Github用户名.github.io` 的方式，进入到库中根目录下的 `index.html` 页面（如果有的话）。

Hexo 提供了快速方便的一键部署功能，配置完成以后只需要一条命令就可以将网站刷新并部署到网站上！

1.安装 hexo-deployer-git。

```bash
npm install hexo-deployer-git --save
```

2.修改 Hexo 目录下的配置文件 `_config.yml` 中 deploy 的内容如下。

```json
deploy:
  type: git
  repo: 你的 Github Pages 链接 # 例如https://bitbucket.org/JohnSmith/johnsmith.bitbucket.io
  branch: master
```

3.生成站点文件并推送至 Github 库。

```bash
hexo clean &amp;&amp; hexo deploy
```

至此，博客便已经搭建完毕了！Hexo 拥有一个 landscape 的初始主题，意味着现在你就可以访问你自己的 Github Pages 了！

更多的主题可以在 [Hexo 官方主题页面](https://hexo.io/themes)上选择。

### 维护与更新博客

当执行 `hexo deploy` 时，Hexo 会将 `public` 目录中的文件和目录推送至 `_config.yml` 中指定的远端仓库和分支中，并且**完全覆盖**该分支下的已有内容。

编写好博客推送以后，只需要用 Git bash 移动到 Hexo 目录，使用 `hexo clean &amp;&amp; hexo deploy` 命令（在 windows terminal 上请使用 `hexo clean ; hexo deploy`），即可完成博客页面的更新与同步了。

## 关于

上述步骤由 [Hexo 官方文档](https://hexo.io/docs/)简化而来。

您可以随时访问官方文档获取最新的搭建博客方法和更多重要的使用方法。
</content:original-text><content:updated-at>2021-02-19T00:00:00.000Z</content:updated-at></item></channel></rss>